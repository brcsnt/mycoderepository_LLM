{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Homework: Entity-Level Sentiment Analysis with Twitter Dataset"
      ],
      "metadata": {
        "id": "pDrvMTlFpKXB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is the Twitter Dataset?\n",
        "\n",
        "This dataset focuses on entity-level sentiment analysis of tweets. It aims to determine the sentiment expressed towards specific entities mentioned in Twitter messages."
      ],
      "metadata": {
        "id": "xEOdW76PBYS7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Motivation\n",
        "\n",
        "Twitter is a platform where users share their opinions in real-time. By analyzing these messages, we can gain insights into public perception and trends related to specific entities."
      ],
      "metadata": {
        "id": "aGEBy-Z7BeUn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem Statement\n",
        "The task is to predict the sentiment (Positive, Negative, Neutral) of a message concerning a given entity. If the message is irrelevant to the entity (Irrelevant), it is classified as Neutral."
      ],
      "metadata": {
        "id": "FmuH-z1ZBeW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What Do We Expect from You in This Assignment?\n",
        "\n",
        "We expect you to use Deep Learning and NLP techniques to analyze the messages in the dataset and correctly classify them into the sentiment categories (Positive, Negative, Neutral)."
      ],
      "metadata": {
        "id": "mjZTF7q2BeZQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "The dataset has been shared along with the homework *(twitter_training.csv)*."
      ],
      "metadata": {
        "id": "XOtxROcTpVSn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## If you have any question about the homework, you can contact us at the following e-mail adresses:\n",
        "\n",
        "\n",
        "\n",
        "*   burcusunturlu@gmail.com\n",
        "*   ozgeflzcn@gmail.com\n",
        "\n"
      ],
      "metadata": {
        "id": "OnVfKAigDq0d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 - Import Libraries"
      ],
      "metadata": {
        "id": "PjXnzMjapk0-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main Libraries for you to deploy your model (Feel free to use other libraries that you think helpful):\n",
        "\n",
        "*   Pandas\n",
        "*   Numpy\n",
        "*   Sklearn\n",
        "*   nltk\n",
        "*   keras"
      ],
      "metadata": {
        "id": "avrGn5L7pk3o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 - Importing the Data (65 points)"
      ],
      "metadata": {
        "id": "yKhDGTafpk56"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 - Loading the Data\n",
        "\n",
        "\n",
        "*   Import the dataset from the file.\n"
      ],
      "metadata": {
        "id": "Bfp1_GuFpk8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read your csv file and define column names\n",
        "columns = ['tweet_id', 'entity', 'sentiment', 'tweet_content']\n",
        "data = pd.read_csv('/content/twitter_training.csv', names = columns)\n",
        "\n",
        "# Replace 'Irrelevant' sentiment with 'Neutral'\n",
        "data['sentiment'] = data['sentiment'].replace('Irrelevant', 'Neutral')\n",
        "\n",
        "# Look at your data\n",
        "data.head()"
      ],
      "metadata": {
        "id": "dBLzkG_Lwp3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d3FLQUZyEqE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 - Exploratory Data Analysis (EDA) (20 points)\n",
        "\n",
        "Please investigate your data according to:\n",
        "* Understand the\n",
        "classes. Visualize the distribution of sentiment classes within the dataset.\n",
        "* Check distributions.\n",
        "* Check null values.\n",
        "* Drop unnecessary columns (e.g., unrelated metadata)."
      ],
      "metadata": {
        "id": "op4AIUbcpk-D"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FMTbu0D6EQi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QuEXxLegEQm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9AXj4OY3EQuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 - Data Preparation (25 points)\n",
        "\n",
        "* Clean the comments. Remove irrelevant characters (e.g., URLs, mentions). Normalize the text (lowercasing, removing punctuation, etc.).\n",
        "* Remove/unremove stopwords based on your assumption.\n",
        "* Tokenize the comments.\n",
        "* Lemmatize the comments.\n",
        "* Vectorization.\n",
        "* Word count analysis and outlier detection."
      ],
      "metadata": {
        "id": "4LRyZjvopk_-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PuRVQ1STERTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oPTI-Ic2ERV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e2jQ2ry7ERZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 - TF(Term Frequency) - IDF(Inverse Document Frequency) (15 points)\n",
        "\n",
        "* Explain TF & IDF.\n",
        "* Apply TF & IDF methods."
      ],
      "metadata": {
        "id": "-tZIOAwvvRLr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fShYEdzdESEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g4oMVPQYESHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fBFwxoDFESJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 - Train/Test Split (5 points)\n",
        "\n",
        "* Prepare the target variables and split the data into training and testing sets."
      ],
      "metadata": {
        "id": "C8f5BLLzvXih"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nJZ0Wl_wESxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YHbZX2yPESzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6FLIF2yjES27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 - Training Deep Learning Models (30 Points)\n",
        "\n",
        "* Import relevant libraries.\n",
        "* Explain the difference between Neural Networks (NN) and Convolutional Neural Networks (CNN)."
      ],
      "metadata": {
        "id": "v1ZVJq1BvXk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D, Flatten\n",
        "from keras.layers import Dense, Input, Embedding, Dropout, Activation"
      ],
      "metadata": {
        "id": "fVL7pgac-IZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PQIZXGPOETck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IoUg42amETfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0i6xthP1ETkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 - Training NN models\n",
        "\n",
        "* Construct NN models from basic one (exp. with one layer) to complex (more layer included).\n",
        "* Experiment with different optimizers, regularization methods, drop-out rates, and normalization techniques.\n",
        "* Evaluate in test data for different trials."
      ],
      "metadata": {
        "id": "dxud-Jm3vXnV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pSWthCV8EUHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_tIhIqZhEUJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-3c4_T9ZEUNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 - Testing with your Own Input (5 points)\n",
        "\n",
        "* Test the trained model with your own input sentences to predict the sentiment based on an entity."
      ],
      "metadata": {
        "id": "0nhoLxfgvXpp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try a sentence related to an entity, you can replace with your own example\n",
        "sentence = \"I love the new features of the Windows!!\"\n",
        "entity = \"Microsoft\" # specify the entity\n",
        "tmp_pred, tmp_sentiment = predict(sentence, entity)\n",
        "print(f\"The sentiment of the sentence about {entity}: \\n***\\n{sentence}\\n***\\nis {tmp_sentiment}.\")"
      ],
      "metadata": {
        "id": "lBI5zxXF-72Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dBbqrDp9EVDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 - Bonus - Training CNN Models (20 points)\n",
        "\n",
        "* Construct CNN models from basic (e.g., one layer) to complex (more layers included).\n",
        "* Use different optimizers, regularization methods, drop-out, normalization etc.\n",
        "* Evaluate in test data for different trials."
      ],
      "metadata": {
        "id": "IBosLlRAvRN4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_VVisRp9EVqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ew-Y8jssEVtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r9QK5RZAEVwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Additional Notes\n",
        "\n",
        "* Ensure all models and visualizations are well-commented.\n",
        "* Include explanations for key steps like tokenization, vectorization, and model selection.\n",
        "* Please complete your homework using this notebook."
      ],
      "metadata": {
        "id": "SHgWpcvSvRQd"
      }
    }
  ]
}