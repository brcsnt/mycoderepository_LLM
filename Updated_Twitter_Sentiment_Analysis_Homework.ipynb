{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDrvMTlFpKXB"
   },
   "source": [
    "# Homework: Entity-Level Sentiment Analysis with Twitter Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xEOdW76PBYS7"
   },
   "source": [
    "## What is the Twitter Dataset?\n",
    "\n",
    "This dataset focuses on entity-level sentiment analysis of tweets. It aims to determine the sentiment expressed towards specific entities mentioned in Twitter messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aGEBy-Z7BeUn"
   },
   "source": [
    "##  Motivation\n",
    "\n",
    "Twitter is a platform where users share their opinions in real-time. By analyzing these messages, we can gain insights into public perception and trends related to specific entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FmuH-z1ZBeW6"
   },
   "source": [
    "## Problem Statement\n",
    "The task is to predict the sentiment (Positive, Negative, Neutral) of a message concerning a given entity. If the message is irrelevant to the entity (Irrelevant), it is classified as Neutral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjZTF7q2BeZQ"
   },
   "source": [
    "## What Do We Expect from You in This Assignment?\n",
    "\n",
    "We expect you to use Deep Learning and NLP techniques to analyze the messages in the dataset and correctly classify them into the sentiment categories (Positive, Negative, Neutral)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOtxROcTpVSn"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset has been shared along with the homework *(twitter_training.csv)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OnVfKAigDq0d"
   },
   "source": [
    "## If you have any question about the homework, you can contact us at the following e-mail adresses:\n",
    "\n",
    "\n",
    "\n",
    "*   burcusunturlu@gmail.com\n",
    "*   ozgeflzcn@gmail.com\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjXnzMjapk0-"
   },
   "source": [
    "## 1 - Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avrGn5L7pk3o"
   },
   "source": [
    "Main Libraries for you to deploy your model (Feel free to use other libraries that you think helpful):\n",
    "\n",
    "*   Pandas\n",
    "*   Numpy\n",
    "*   Sklearn\n",
    "*   nltk\n",
    "*   keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKhDGTafpk56"
   },
   "source": [
    "## 2 - Importing the Data (65 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bfp1_GuFpk8Q"
   },
   "source": [
    "## 2.1 - Loading the Data\n",
    "\n",
    "\n",
    "*   Import the dataset from the file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dBLzkG_Lwp3g"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read your csv file and define column names\n",
    "columns = ['tweet_id', 'entity', 'sentiment', 'tweet_content']\n",
    "data = pd.read_csv('/content/twitter_training.csv', names = columns)\n",
    "\n",
    "# Replace 'Irrelevant' sentiment with 'Neutral'\n",
    "data['sentiment'] = data['sentiment'].replace('Irrelevant', 'Neutral')\n",
    "\n",
    "# Look at your data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d3FLQUZyEqE-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "op4AIUbcpk-D"
   },
   "source": [
    "## 2.2 - Exploratory Data Analysis (EDA) (20 points)\n",
    "\n",
    "Please investigate your data according to:\n",
    "* Understand the\n",
    "classes. Visualize the distribution of sentiment classes within the dataset.\n",
    "* Check distributions.\n",
    "* Check null values.\n",
    "* Drop unnecessary columns (e.g., unrelated metadata)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FMTbu0D6EQi4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QuEXxLegEQm2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9AXj4OY3EQuB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LRyZjvopk_-"
   },
   "source": [
    "## 2.3 - Data Preparation (25 points)\n",
    "\n",
    "* Clean the comments. Remove irrelevant characters (e.g., URLs, mentions). Normalize the text (lowercasing, removing punctuation, etc.).\n",
    "* Remove/unremove stopwords based on your assumption.\n",
    "* Tokenize the comments.\n",
    "* Lemmatize the comments.\n",
    "* Vectorization.\n",
    "* Word count analysis and outlier detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PuRVQ1STERTb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oPTI-Ic2ERV-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e2jQ2ry7ERZm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tZIOAwvvRLr"
   },
   "source": [
    "## 2.4 - TF(Term Frequency) - IDF(Inverse Document Frequency) (15 points)\n",
    "\n",
    "* Explain TF & IDF.\n",
    "* Apply TF & IDF methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fShYEdzdESEf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g4oMVPQYESHG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fBFwxoDFESJ9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8f5BLLzvXih"
   },
   "source": [
    "## 2.5 - Train/Test Split (5 points)\n",
    "\n",
    "* Prepare the target variables and split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJZ0Wl_wESxV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YHbZX2yPESzi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6FLIF2yjES27"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1ZVJq1BvXk2"
   },
   "source": [
    "# 3 - Training Deep Learning Models (30 Points)\n",
    "\n",
    "* Import relevant libraries.\n",
    "* Explain the difference between Neural Networks (NN) and Convolutional Neural Networks (CNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fVL7pgac-IZw"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, Flatten\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQIZXGPOETck"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IoUg42amETfv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0i6xthP1ETkF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxud-Jm3vXnV"
   },
   "source": [
    "## 3.1 - Training NN models\n",
    "\n",
    "* Construct NN models from basic one (exp. with one layer) to complex (more layer included).\n",
    "* Experiment with different optimizers, regularization methods, drop-out rates, and normalization techniques.\n",
    "* Evaluate in test data for different trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pSWthCV8EUHo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_tIhIqZhEUJz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-3c4_T9ZEUNX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0nhoLxfgvXpp"
   },
   "source": [
    "# 4 - Testing with your Own Input (5 points)\n",
    "\n",
    "* Test the trained model with your own input sentences to predict the sentiment based on an entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lBI5zxXF-72Q"
   },
   "outputs": [],
   "source": [
    "# Try a sentence related to an entity, you can replace with your own example\n",
    "sentence = \"I love the new features of the Windows!!\"\n",
    "entity = \"Microsoft\" # specify the entity\n",
    "tmp_pred, tmp_sentiment = predict(sentence, entity)\n",
    "print(f\"The sentiment of the sentence about {entity}: \\n***\\n{sentence}\\n***\\nis {tmp_sentiment}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dBbqrDp9EVDh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBosLlRAvRN4"
   },
   "source": [
    "# 5 - Bonus - Training CNN Models (20 points)\n",
    "\n",
    "* Construct CNN models from basic (e.g., one layer) to complex (more layers included).\n",
    "* Use different optimizers, regularization methods, drop-out, normalization etc.\n",
    "* Evaluate in test data for different trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_VVisRp9EVqn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ew-Y8jssEVtL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r9QK5RZAEVwD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHgWpcvSvRQd"
   },
   "source": [
    "## Additional Notes\n",
    "\n",
    "* Ensure all models and visualizations are well-commented.\n",
    "* Include explanations for key steps like tokenization, vectorization, and model selection.\n",
    "* Please complete your homework using this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002e74dd",
   "metadata": {},
   "source": [
    "### Additional Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf5d1b3",
   "metadata": {},
   "source": [
    "#### 1. **Data Exploration and Preprocessing**:\n",
    "- Are there any missing values in the tweets? Explain your approach to handling missing data.\n",
    "- How would you clean special characters, links, or emojis from the tweets?\n",
    "- Visualize the data: Create a chart to show the distribution of each sentiment category (Positive, Negative, Neutral)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0578010",
   "metadata": {},
   "source": [
    "#### 2. **Model Performance Analysis**:\n",
    "- Compare the accuracy of the training and validation data to analyze if the model is overfitting.\n",
    "- How would you assess if there is any bias in your model’s results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cef1c01",
   "metadata": {},
   "source": [
    "#### 3. **Hyperparameter Tuning**:\n",
    "- Explain the hyperparameters you used to train your model (e.g., learning rate, number of epochs) and show how changes in these parameters impacted the model’s performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48b8f2f",
   "metadata": {},
   "source": [
    "#### 4. **Interpretation of Results**:\n",
    "- Identify the sentiment category that has the highest misclassification rate and analyze the potential reasons behind this.\n",
    "- Are there any tweets that your model struggled to classify accurately? Describe the common characteristics of these tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd776613",
   "metadata": {},
   "source": [
    "#### 5. **Suggestions for Improvement**:\n",
    "- What additional preprocessing steps could you add to improve performance on this dataset?\n",
    "- Discuss if using a different model or approach (e.g., Transformer-based models) could yield better results."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNqDJaFnmOysMg4S0PUdbRb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
