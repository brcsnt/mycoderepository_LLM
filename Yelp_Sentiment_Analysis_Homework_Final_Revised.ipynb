{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDrvMTlFpKXB"
   },
   "source": [
    "# Homework: Sentiment Analysis with Yelp Review Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xEOdW76PBYS7"
   },
   "source": [
    "## What is the Yelp Dataset?\n",
    "\n",
    "This dataset is derived from Yelp reviews, where each review expresses a sentiment (positive, negative, or neutral) about a particular service, product, or experience. The task focuses on analyzing these reviews to extract the sentiment conveyed.\n",
    "\n",
    "## Dataset Information\n",
    "\n",
    "The Yelp dataset consists of two files:\n",
    "- `train.csv`: Training dataset containing labeled reviews for model training.\n",
    "- `val.csv`: Validation dataset for evaluating the model's performance on unseen data.\n",
    "\n",
    "Each review is associated with a `label` ranging from 0 to 4, where:\n",
    "- `label 0`: 1 star\n",
    "- `label 1`: 2 stars\n",
    "- `label 2`: 3 stars\n",
    "- `label 3`: 4 stars\n",
    "- `label 4`: 5 stars\n",
    "\n",
    "The code provided below includes a step to map these labels to their corresponding star ratings for better interpretability.\n",
    "\n",
    "## Data Exploration and Preprocessing\n",
    "\n",
    "### Missing Values\n",
    "\n",
    "- **Question**: Are there any missing values in the Yelp reviews? Explain your approach to handling missing data.\n",
    "\n",
    "Before processing, it is essential to check for any missing values in the dataset. Handling these can be crucial to avoid errors during model training.\n",
    "\n",
    "### Cleaning Text Data\n",
    "\n",
    "- **Question**: How would you clean special characters, links, or emojis from the reviews?\n",
    "\n",
    "To enhance model performance, we need to clean the text data by removing any irrelevant elements like links, special characters, or emojis. This can be achieved using regular expressions or libraries such as `re` and `emoji` in Python.\n",
    "\n",
    "### Sentiment Distribution Visualization\n",
    "\n",
    "- **Question**: Visualize the data distribution across sentiment categories (1 to 5 stars).\n",
    "\n",
    "The following code snippet helps visualize the distribution of reviews across the 1-5 star categories:\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming train_df is loaded\n",
    "train_df['star_rating'].value_counts().sort_index().plot(kind='bar', title='Sentiment Distribution')\n",
    "plt.xlabel('Star Rating')\n",
    "plt.ylabel('Number of Reviews')\n",
    "plt.show()\n",
    "```\n",
    "This will provide insights into whether the data is balanced or skewed across different ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aGEBy-Z7BeUn"
   },
   "source": [
    "##  Motivation\n",
    "\n",
    "Twitter is a platform where users share their opinions in real-time. By analyzing these messages, we can gain insights into public perception and trends related to specific entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FmuH-z1ZBeW6"
   },
   "source": [
    "## Problem Statement\n",
    "\n",
    "The task is to classify each review as expressing a Positive, Negative, or Neutral sentiment. This will help understand the general sentiment towards various businesses and services based on user feedback.\n",
    "\n",
    "## Label Adjustment Code\n",
    "\n",
    "To adjust the labels, run the following code:\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "train_df = pd.read_csv('train.csv')\n",
    "val_df = pd.read_csv('val.csv')\n",
    "\n",
    "# Map labels to star ratings\n",
    "label_to_star = {0: '1 star', 1: '2 stars', 2: '3 stars', 3: '4 stars', 4: '5 stars'}\n",
    "train_df['star_rating'] = train_df['label'].map(label_to_star)\n",
    "val_df['star_rating'] = val_df['label'].map(label_to_star)\n",
    "\n",
    "# Display the first few rows to confirm the mapping\n",
    "print(train_df.head())\n",
    "print(val_df.head())\n",
    "```\n",
    "\n",
    "This code snippet loads the `train.csv` and `val.csv` files, maps the `label` values to their respective star ratings, and displays the first few rows to verify the changes.\n",
    "\n",
    "## Hyperparameter Tuning\n",
    "\n",
    "- **Question**: Explain the hyperparameters you used to train your model (e.g., learning rate, number of epochs) and show how changes in these parameters impacted the model’s performance.\n",
    "\n",
    "The hyperparameters, such as learning rate, number of epochs, batch size, etc., play a critical role in model training. By experimenting with different values, we can observe their impact on training and validation accuracy, convergence speed, and overall model performance.\n",
    "\n",
    "For instance, a lower learning rate may result in more stable training but slower convergence, while a higher learning rate can speed up training but risk overshooting the optimal solution. Similarly, tuning the number of epochs helps control overfitting by adjusting the extent of training.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "# Example hyperparameter adjustments\n",
    "learning_rate = 0.001  # Initial setting\n",
    "num_epochs = 10        # Set initial number of epochs\n",
    "# Adjust and observe the model performance\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjZTF7q2BeZQ"
   },
   "source": [
    "## What Do We Expect from You in This Assignment?\n",
    "\n",
    "We expect you to use NLP techniques and possibly deep learning methods to analyze the text data from Yelp reviews. Your goal is to accurately classify each review into one of the sentiment categories: Positive, Negative, or Neutral.\n",
    "\n",
    "## Additional Questions for Exploration\n",
    "\n",
    "1. **Data Analysis**: What trends or patterns can you identify in the Yelp reviews' star ratings? For instance, are there more positive (4-5 stars) or negative (1-2 stars) reviews?\n",
    "2. **Data Preprocessing**: Are there any reviews that should be removed or cleaned (e.g., empty reviews, excessive punctuation)?\n",
    "3. **Sentiment Distribution**: How is the sentiment distributed across different star ratings? Is there a balanced distribution of sentiments across the dataset?\n",
    "4. **Model Improvement**: What strategies might you consider to improve model accuracy on the validation set?\n",
    "\n",
    "\n",
    "## Model Performance Analysis\n",
    "\n",
    "- **Question**: Compare the accuracy of the training and validation data to analyze if the model is overfitting.\n",
    "\n",
    "Comparing training and validation accuracy provides insights into overfitting or underfitting. A large gap with high training accuracy but low validation accuracy indicates overfitting.\n",
    "\n",
    "- **Question**: How would you assess if there is any bias in your model’s results?\n",
    "\n",
    "Evaluating bias involves checking if the model consistently misclassifies a particular sentiment class or star rating. This can be assessed by examining confusion matrices and analyzing error rates across categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOtxROcTpVSn"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset has been shared along with the homework *(twitter_training.csv)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OnVfKAigDq0d"
   },
   "source": [
    "## If you have any question about the homework, you can contact us at the following e-mail adresses:\n",
    "\n",
    "\n",
    "\n",
    "*   burcusunturlu@gmail.com\n",
    "*   ozgeflzcn@gmail.com\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjXnzMjapk0-"
   },
   "source": [
    "## 1 - Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avrGn5L7pk3o"
   },
   "source": [
    "Main Libraries for you to deploy your model (Feel free to use other libraries that you think helpful):\n",
    "\n",
    "*   Pandas\n",
    "*   Numpy\n",
    "*   Sklearn\n",
    "*   nltk\n",
    "*   keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKhDGTafpk56"
   },
   "source": [
    "## 2 - Importing the Data (65 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bfp1_GuFpk8Q"
   },
   "source": [
    "## 2.1 - Loading the Data\n",
    "\n",
    "\n",
    "*   Import the dataset from the file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dBLzkG_Lwp3g"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read your csv file and define column names\n",
    "columns = ['tweet_id', 'entity', 'sentiment', 'tweet_content']\n",
    "data = pd.read_csv('/content/twitter_training.csv', names = columns)\n",
    "\n",
    "# Replace 'Irrelevant' sentiment with 'Neutral'\n",
    "data['sentiment'] = data['sentiment'].replace('Irrelevant', 'Neutral')\n",
    "\n",
    "# Look at your data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d3FLQUZyEqE-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "op4AIUbcpk-D"
   },
   "source": [
    "## 2.2 - Exploratory Data Analysis (EDA) (20 points)\n",
    "\n",
    "Please investigate your data according to:\n",
    "* Understand the\n",
    "classes. Visualize the distribution of sentiment classes within the dataset.\n",
    "* Check distributions.\n",
    "* Check null values.\n",
    "* Drop unnecessary columns (e.g., unrelated metadata)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FMTbu0D6EQi4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QuEXxLegEQm2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9AXj4OY3EQuB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LRyZjvopk_-"
   },
   "source": [
    "## 2.3 - Data Preparation (25 points)\n",
    "\n",
    "* Clean the comments. Remove irrelevant characters (e.g., URLs, mentions). Normalize the text (lowercasing, removing punctuation, etc.).\n",
    "* Remove/unremove stopwords based on your assumption.\n",
    "* Tokenize the comments.\n",
    "* Lemmatize the comments.\n",
    "* Vectorization.\n",
    "* Word count analysis and outlier detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PuRVQ1STERTb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oPTI-Ic2ERV-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e2jQ2ry7ERZm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tZIOAwvvRLr"
   },
   "source": [
    "## 2.4 - TF(Term Frequency) - IDF(Inverse Document Frequency) (15 points)\n",
    "\n",
    "* Explain TF & IDF.\n",
    "* Apply TF & IDF methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fShYEdzdESEf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g4oMVPQYESHG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fBFwxoDFESJ9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8f5BLLzvXih"
   },
   "source": [
    "## 2.5 - Train/Test Split (5 points)\n",
    "\n",
    "* Prepare the target variables and split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJZ0Wl_wESxV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YHbZX2yPESzi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6FLIF2yjES27"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1ZVJq1BvXk2"
   },
   "source": [
    "# 3 - Training Deep Learning Models (30 Points)\n",
    "\n",
    "* Import relevant libraries.\n",
    "* Explain the difference between Neural Networks (NN) and Convolutional Neural Networks (CNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fVL7pgac-IZw"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, Flatten\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQIZXGPOETck"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IoUg42amETfv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0i6xthP1ETkF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxud-Jm3vXnV"
   },
   "source": [
    "## 3.1 - Training NN models\n",
    "\n",
    "* Construct NN models from basic one (exp. with one layer) to complex (more layer included).\n",
    "* Experiment with different optimizers, regularization methods, drop-out rates, and normalization techniques.\n",
    "* Evaluate in test data for different trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pSWthCV8EUHo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_tIhIqZhEUJz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-3c4_T9ZEUNX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0nhoLxfgvXpp"
   },
   "source": [
    "# 4 - Testing with your Own Input (5 points)\n",
    "\n",
    "* Test the trained model with your own input sentences to predict the sentiment based on an entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lBI5zxXF-72Q"
   },
   "outputs": [],
   "source": [
    "# Try a sentence related to an entity, you can replace with your own example\n",
    "sentence = \"I love the new features of the Windows!!\"\n",
    "entity = \"Microsoft\" # specify the entity\n",
    "tmp_pred, tmp_sentiment = predict(sentence, entity)\n",
    "print(f\"The sentiment of the sentence about {entity}: \\n***\\n{sentence}\\n***\\nis {tmp_sentiment}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dBbqrDp9EVDh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBosLlRAvRN4"
   },
   "source": [
    "# 5 - Bonus - Training CNN Models (20 points)\n",
    "\n",
    "* Construct CNN models from basic (e.g., one layer) to complex (more layers included).\n",
    "* Use different optimizers, regularization methods, drop-out, normalization etc.\n",
    "* Evaluate in test data for different trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_VVisRp9EVqn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ew-Y8jssEVtL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r9QK5RZAEVwD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHgWpcvSvRQd"
   },
   "source": [
    "## Additional Notes\n",
    "\n",
    "* Ensure all models and visualizations are well-commented.\n",
    "* Include explanations for key steps like tokenization, vectorization, and model selection.\n",
    "* Please complete your homework using this notebook."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNqDJaFnmOysMg4S0PUdbRb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
