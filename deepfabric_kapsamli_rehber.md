# ğŸ¨ DeepFabric - KapsamlÄ± KullanÄ±m Rehberi

## ğŸ“‘ Ä°Ã§indekiler

1. [GiriÅŸ ve Genel BakÄ±ÅŸ](#1-giriÅŸ-ve-genel-bakÄ±ÅŸ)
2. [Kurulum](#2-kurulum)
3. [Temel Kavramlar](#3-temel-kavramlar)
4. [CLI KullanÄ±mÄ±](#4-cli-kullanÄ±mÄ±)
5. [YAML KonfigÃ¼rasyon](#5-yaml-konfigÃ¼rasyon)
6. [SDK KullanÄ±mÄ±](#6-sdk-kullanÄ±mÄ±)
7. [Provider'lar ve Modeller](#7-providerlar-ve-modeller)
8. [Pratik Ã–rnekler](#8-pratik-Ã¶rnekler)
9. [Ä°leri Seviye KullanÄ±m](#9-ileri-seviye-kullanÄ±m)
10. [Best Practices](#10-best-practices)
11. [Troubleshooting](#11-troubleshooting)

---

## 1. GiriÅŸ ve Genel BakÄ±ÅŸ

### ğŸ¯ DeepFabric Nedir?

**DeepFabric** (eski adÄ±: promptwright), bÃ¼yÃ¼k dil modellerini (LLM) kullanarak **yÃ¼ksek kaliteli sentetik veri setleri** oluÅŸturmak iÃ§in geliÅŸtirilmiÅŸ bir **CLI aracÄ±** ve **Python SDK**'dÄ±r.

### ğŸ“Š KullanÄ±m AlanlarÄ±

| Alan | AÃ§Ä±klama | Ã–rnek |
|------|----------|-------|
| **Model Distillation** | BÃ¼yÃ¼k Ã¶ÄŸretmen modellerden kÃ¼Ã§Ã¼k Ã¶ÄŸrenci modellere bilgi aktarÄ±mÄ± | GPT-4 â†’ Llama-7B |
| **Evaluation Benchmarks** | Model ve agent'larÄ±n performansÄ±nÄ± Ã¶lÃ§mek iÃ§in test setleri | MMLU, HellaSwag gibi |
| **Fine-tuning Data** | Ã–zel domain'ler iÃ§in eÄŸitim verisi | Medikal, legal, finans |
| **Synthetic Training** | GerÃ§ek veri olmadan model eÄŸitimi | Gizlilik gerektiren projeler |

### ğŸŒŸ Ana Ã–zellikler

#### âœ… Graf ve AÄŸaÃ§ TabanlÄ± Mimari
- **Topic Tree**: HiyerarÅŸik konu yapÄ±larÄ±
- **Topic Graph**: KarmaÅŸÄ±k iliÅŸkilerle birbirine baÄŸlÄ± konular
- Minimal duplikasyon, maksimum Ã§eÅŸitlilik

#### âœ… Ã‡oklu LLM DesteÄŸi
- OpenAI (GPT-4, GPT-3.5)
- Anthropic (Claude)
- Google (Gemini)
- Ollama (Yerel modeller)
- OpenRouter (Ã‡oklu model eriÅŸimi)

#### âœ… Esnek Veri FormatlarÄ±
- OpenAI Chat Format (varsayÄ±lan)
- Ã–zelleÅŸtirilebilir format dÃ¶nÃ¼ÅŸtÃ¼rÃ¼cÃ¼leri (yakÄ±nda)
- Multi-turn konuÅŸmalar
- Chain of Thought
- Tool calling

#### âœ… Hibrit Model KullanÄ±mÄ±
```yaml
# Topic generation iÃ§in pahalÄ± model
topic_tree:
  provider: "openai"
  model: "gpt-4"

# Data generation iÃ§in ucuz model  
data_engine:
  provider: "ollama"
  model: "llama3:70b"
```

#### âœ… Hugging Face Entegrasyonu
- Otomatik dataset upload
- Dataset card oluÅŸturma
- Tag yÃ¶netimi

---

## 2. Kurulum

### ğŸ“¦ Temel Kurulum

```bash
# PyPI'dan kurulum
pip install deepfabric

# GitHub'dan en son versiyon
pip install git+https://github.com/lukehinds/deepfabric.git

# GeliÅŸtirme modu
git clone https://github.com/lukehinds/deepfabric.git
cd deepfabric
pip install -e .
```

### ğŸ”‘ API AnahtarlarÄ±

DeepFabric, environment variable'lar Ã¼zerinden API anahtarlarÄ±nÄ± alÄ±r:

```bash
# OpenAI
export OPENAI_API_KEY="sk-..."

# Anthropic (Claude)
export ANTHROPIC_API_KEY="sk-ant-..."

# Google (Gemini)
export GEMINI_API_KEY="..."

# OpenRouter
export OPENROUTER_API_KEY="sk-or-..."

# Hugging Face (opsiyonel)
export HF_TOKEN="hf_..."
```

### âœ… Kurulum DoÄŸrulama

```bash
# DeepFabric versiyonunu kontrol et
deepfabric --version

# YardÄ±m menÃ¼sÃ¼nÃ¼ gÃ¶ster
deepfabric --help

# Basit test
deepfabric generate --help
```

---

## 3. Temel Kavramlar

### ğŸŒ³ Topic Tree (Konu AÄŸacÄ±)

Topic Tree, veri Ã¼retiminin **birinci aÅŸamasÄ±dÄ±r** ve organize bir konu hiyerarÅŸisi oluÅŸturur.

#### Neden Topic Tree?

**Olmadan:**
```
âŒ "Python nedir?"
âŒ "Java syntax"
âŒ "Matematik problemi"
âŒ "Rastgele sorular..."
```

**Ä°le:**
```
âœ… Python â†’ Veri Tipleri â†’ Liste Ä°ÅŸlemleri
âœ… Python â†’ Fonksiyonlar â†’ Lambda Expressions
âœ… Python â†’ OOP â†’ Inheritance
âœ… Organize, iliÅŸkili, Ã§eÅŸitli konular
```

#### Topic Tree Parametreleri

| Parametre | AÃ§Ä±klama | Ã–rnek | Etki |
|-----------|----------|-------|------|
| `depth` | AÄŸaÃ§ derinliÄŸi | `3` | 3 seviye hiyerarÅŸi |
| `degree` | Her dÃ¼ÄŸÃ¼mden kaÃ§ dal | `4` | Her konudan 4 alt konu |
| `topic_prompt` | Ana konu | `"ML Fundamentals"` | Temel konu baÅŸlÄ±ÄŸÄ± |
| `temperature` | YaratÄ±cÄ±lÄ±k | `0.7` | Ã‡eÅŸitli konular |

#### Matematiksel Ä°liÅŸki

```
Toplam konu sayÄ±sÄ± â‰ˆ Î£(degree^i) for i in range(1, depth+1)

Ã–rnekler:
depth=2, degree=3 â†’ 3Â¹ + 3Â² = 3 + 9 = 12 konu
depth=3, degree=4 â†’ 4Â¹ + 4Â² + 4Â³ = 4 + 16 + 64 = 84 konu
depth=4, degree=5 â†’ 5Â¹ + 5Â² + 5Â³ + 5â´ = 780 konu
```

### âš™ï¸ Data Engine (Veri Motoru)

Data Engine, topic tree'den gelen konularÄ± kullanarak **gerÃ§ek eÄŸitim verisi** Ã¼retir.

#### Data Engine Ä°ÅŸleyiÅŸi

```
Topic Tree â†’ "Liste Comprehension in Python"
            â†“
Data Engine iÅŸler
            â†“
Output: {
  "messages": [
    {
      "role": "user",
      "content": "List comprehension nedir ve nasÄ±l kullanÄ±lÄ±r?"
    },
    {
      "role": "assistant", 
      "content": "List comprehension, Python'da liste oluÅŸturmanÄ±n..."
    }
  ]
}
```

#### Data Engine Parametreleri

| Parametre | AÃ§Ä±klama | DeÄŸer AralÄ±ÄŸÄ± | Ã–nerilen |
|-----------|----------|---------------|----------|
| `num_steps` | KaÃ§ Ã¶rnek | 1 - âˆ | 100-1000 |
| `batch_size` | Paralel iÅŸlem | 1 - 10 | 3-5 |
| `temperature` | TutarlÄ±lÄ±k | 0.0 - 1.0 | 0.3-0.5 |
| `conversation_type` | Veri tipi | cot, tool, multi-turn, qa, code | - |

### ğŸ†š Topic Tree vs Data Engine

```mermaid
graph LR
    A[KullanÄ±cÄ±] --> B[Topic Tree]
    B -->|Konu HiyerarÅŸisi| C[Data Engine]
    C -->|Q&A Ã‡iftleri| D[Dataset JSONL]
    
    B -.->|YÃ¼ksek temp 0.7| B
    B -.->|GÃ¼Ã§lÃ¼ model GPT-4| B
    
    C -.->|DÃ¼ÅŸÃ¼k temp 0.5| C
    C -.->|Ucuz model Llama| C
```

**Fark:**
- **Topic Tree**: "Ne" konuÅŸulacaÄŸÄ±nÄ± belirler (konu Ã¼retimi)
- **Data Engine**: "NasÄ±l" konuÅŸulacaÄŸÄ±nÄ± belirler (veri Ã¼retimi)

---

## 4. CLI KullanÄ±mÄ±

### ğŸš€ HÄ±zlÄ± BaÅŸlangÄ±Ã§

#### En Basit KullanÄ±m

```bash
deepfabric generate \
  --mode tree \
  --provider openai \
  --model gpt-4o \
  --depth 2 \
  --degree 3 \
  --num-steps 10 \
  --topic-prompt "Python Programming Basics" \
  --dataset-save-as my_dataset.jsonl
```

**Ne yapar?**
1. 2 seviyeli, 3'er dallÄ± topic tree oluÅŸturur (9 konu)
2. Her konu iÃ§in 10 Q&A Ã§ifti Ã¼retir
3. `my_dataset.jsonl` dosyasÄ±na kaydeder

### ğŸ“ TÃ¼m CLI Parametreleri

```bash
deepfabric generate \
  # Temel parametreler
  --mode tree \                           # veya 'graph'
  --provider openai \                     # LLM saÄŸlayÄ±cÄ±
  --model gpt-4-turbo \                   # Model adÄ±
  
  # Topic Tree parametreleri
  --depth 3 \                             # AÄŸaÃ§ derinliÄŸi
  --degree 4 \                            # Dallanma sayÄ±sÄ±
  --topic-prompt "Machine Learning" \     # Ana konu
  --save-tree topics.jsonl \              # Topic tree kaydet
  --temperature 0.7 \                     # YaratÄ±cÄ±lÄ±k
  
  # Data Engine parametreleri
  --num-steps 100 \                       # Ã–rnek sayÄ±sÄ±
  --batch-size 5 \                        # Batch boyutu
  --generation-system-prompt "You are..." \ # Ã–zel prompt
  
  # Output parametreleri
  --dataset-save-as dataset.jsonl \       # Dataset kaydet
  --sys-msg true \                        # System mesajÄ± dahil
  
  # Hugging Face parametreleri
  --hf-repo username/dataset-name \       # HF repo
  --hf-token $HF_TOKEN \                  # HF token
  --hf-tags tag1 --hf-tags tag2           # Etiketler
```

### ğŸ¯ YaygÄ±n KullanÄ±m SenaryolarÄ±

#### Senaryo 1: HÄ±zlÄ± Test

```bash
deepfabric generate \
  --provider ollama \
  --model llama3:8b \
  --depth 1 \
  --degree 2 \
  --num-steps 5 \
  --topic-prompt "Python Basics" \
  --dataset-save-as test.jsonl
```

**SÃ¼re:** ~1 dakika  
**Ã‡Ä±ktÄ±:** 5 Ã¶rnek, 2 konu

#### Senaryo 2: Orta Ã–lÃ§ekli Dataset

```bash
deepfabric generate \
  --provider openai \
  --model gpt-4o \
  --depth 2 \
  --degree 3 \
  --num-steps 100 \
  --batch-size 5 \
  --topic-prompt "Data Science with Python" \
  --dataset-save-as data_science_100.jsonl
```

**SÃ¼re:** ~30 dakika  
**Ã‡Ä±ktÄ±:** 100 Ã¶rnek, 12 konu

#### Senaryo 3: BÃ¼yÃ¼k Dataset + HF Upload

```bash
deepfabric generate \
  --provider openrouter \
  --model meta-llama/llama-3.1-70b-instruct \
  --depth 3 \
  --degree 4 \
  --num-steps 1000 \
  --batch-size 5 \
  --topic-prompt "Advanced Machine Learning Algorithms" \
  --dataset-save-as ml_1000.jsonl \
  --hf-repo myusername/ml-qa-dataset \
  --hf-token $HF_TOKEN
```

**SÃ¼re:** ~5-8 saat  
**Ã‡Ä±ktÄ±:** 1000 Ã¶rnek, 84 konu, HF'e yÃ¼klendi

#### Senaryo 4: Hibrit Model KullanÄ±mÄ±

```bash
# Bu senaryo iÃ§in YAML kullanmak daha iyi (aÅŸaÄŸÄ±da)
```

---

## 5. YAML KonfigÃ¼rasyon

### ğŸ“„ Neden YAML?

YAML konfigÃ¼rasyonu ÅŸunlarÄ± saÄŸlar:
- âœ… KarmaÅŸÄ±k ayarlarÄ± organize etme
- âœ… Hibrit model kullanÄ±mÄ±
- âœ… Tekrar kullanÄ±labilirlik
- âœ… Versiyon kontrolÃ¼ (Git)
- âœ… DokÃ¼mantasyon

### ğŸ¨ Tam Ã–zellikli YAML Ã–rneÄŸi

```yaml
# ============================================
# DeepFabric KonfigÃ¼rasyon DosyasÄ±
# ============================================

# Ana sistem prompt'u (fallback olarak kullanÄ±lÄ±r)
dataset_system_prompt: |
  Sen bir uzman AI asistanÄ±sÄ±n. KullanÄ±cÄ±lara 
  net, eÄŸitici ve doÄŸru bilgiler saÄŸlarsÄ±n.

# ============================================
# TOPIC TREE - Konu HiyerarÅŸisi OluÅŸturma
# ============================================
topic_tree:
  # Ana konu tanÄ±mÄ±
  topic_prompt: |
    Python Programlama: Temel kavramlardan ileri seviye 
    tekniklere kadar Python ile yazÄ±lÄ±m geliÅŸtirme
  
  # LLM AyarlarÄ±
  provider: "openai"              # openai, anthropic, gemini, ollama
  model: "gpt-4-turbo"            # Model adÄ±
  temperature: 0.7                # 0.0 = deterministik, 1.0 = yaratÄ±cÄ±
  
  # AÄŸaÃ§ YapÄ±sÄ±
  degree: 3                       # Her dÃ¼ÄŸÃ¼mden 3 alt konu
  depth: 3                        # 3 seviye derinlik
  
  # Ã–zel topic generation prompt (opsiyonel)
  topic_system_prompt: |
    Sen bir mÃ¼fredat tasarÄ±m uzmanÄ±sÄ±n. 
    KonularÄ± mantÄ±klÄ± bir sÄ±rayla organize et:
    1. Kolay kavramlar Ã¶nce
    2. Ã–nkoÅŸul bilgiler sÄ±ralÄ±
    3. Pratik uygulamalar dahil
  
  # Output
  save_as: "python_topics_tree.jsonl"

# ============================================
# DATA ENGINE - Veri Ãœretimi
# ============================================
data_engine:
  # Genel talimatlar
  instructions: |
    Her konu iÃ§in:
    - AÃ§Ä±k ve anlaÅŸÄ±lÄ±r sorular sor
    - DetaylÄ± ve Ã¶ÄŸretici cevaplar ver
    - Kod Ã¶rnekleri ekle
    - Best practices belirt
  
  # LLM AyarlarÄ± (Topic'ten farklÄ± olabilir!)
  provider: "ollama"              # Ucuz/hÄ±zlÄ± model
  model: "llama3:70b"
  temperature: 0.4                # Daha tutarlÄ± cevaplar
  max_retries: 3                  # Hata durumunda tekrar
  
  # Ã–zel generation prompt
  generation_system_prompt: |
    Sen bir Python eÄŸitmenisin. Her soruya:
    1. KavramÄ± aÃ§Ä±kla
    2. Ã‡alÄ±ÅŸan kod Ã¶rneÄŸi ver
    3. Edge case'leri belirt
    4. Common mistake'leri aÃ§Ä±kla
  
  # Conversation type (opsiyonel)
  conversation_type: "qa"         # qa, cot, tool_calling, multi_turn, code

# ============================================
# DATASET - Final Veri Seti OluÅŸturma
# ============================================
dataset:
  creation:
    num_steps: 100                # 100 eÄŸitim Ã¶rneÄŸi
    batch_size: 5                 # 5'er 5'er iÅŸle
    sys_msg: true                 # System mesajÄ± dahil et
  
  # Output
  save_as: "python_programming_dataset.jsonl"

# ============================================
# HUGGING FACE (Opsiyonel)
# ============================================
huggingface:
  # Repository bilgileri
  repository: "myusername/python-qa-dataset"
  
  # Token (veya environment variable: HF_TOKEN)
  token: "hf_..."
  
  # Ek etiketler (deepfabric ve synthetic otomatik eklenir)
  tags:
    - "python"
    - "programming"
    - "educational"
    - "synthetic-data"
```

### ğŸš€ YAML ile KullanÄ±m

```bash
# YAML dosyasÄ±nÄ± kullan
deepfabric generate config.yaml

# YAML + CLI override
deepfabric generate config.yaml \
  --num-steps 200 \
  --temperature 0.6
```

### ğŸ“‹ Minimal YAML Ã–rneÄŸi

```yaml
# Minimum gereksinimler
dataset_system_prompt: "Sen bir yardÄ±mcÄ± asistansÄ±n."

topic_tree:
  topic_prompt: "Python Basics"
  provider: "ollama"
  model: "llama3:8b"
  degree: 2
  depth: 2

data_engine:
  provider: "ollama"
  model: "llama3:8b"

dataset:
  creation:
    num_steps: 10
  save_as: "dataset.jsonl"
```

### ğŸ¯ FarklÄ± AmaÃ§lar iÃ§in YAML ÅablonlarÄ±

#### Åablon 1: Chain of Thought (Matematik/Problem Solving)

```yaml
dataset_system_prompt: "Sen bir matematik Ã¶ÄŸretmenisin."

topic_tree:
  topic_prompt: "Matematik Problemleri: Algebra, Geometri, Calculus"
  provider: "openai"
  model: "gpt-4-turbo"
  temperature: 0.7
  degree: 3
  depth: 2

data_engine:
  generation_system_prompt: |
    Her problemi adÄ±m adÄ±m Ã§Ã¶z:
    1. Problemi anla
    2. YaklaÅŸÄ±mÄ± belirle
    3. AdÄ±m adÄ±m Ã§Ã¶z
    4. Sonucu doÄŸrula
  
  conversation_type: "chain_of_thought"
  provider: "openai"
  model: "gpt-4-turbo"
  temperature: 0.5

dataset:
  creation:
    num_steps: 50
    batch_size: 3
  save_as: "math_cot.jsonl"
```

#### Åablon 2: Code Generation

```yaml
dataset_system_prompt: "Sen bir yazÄ±lÄ±m mÃ¼hendisisin."

topic_tree:
  topic_prompt: "Python AlgoritmalarÄ± ve Veri YapÄ±larÄ±"
  provider: "anthropic"
  model: "claude-3-sonnet-20240229"
  temperature: 0.6
  degree: 4
  depth: 2

data_engine:
  generation_system_prompt: |
    Her problem iÃ§in:
    - Ã‡alÄ±ÅŸan Python kodu yaz
    - Yorum satÄ±rlarÄ± ekle
    - Zaman karmaÅŸÄ±klÄ±ÄŸÄ±nÄ± belirt
    - Test case'leri ver
  
  conversation_type: "code"
  provider: "anthropic"
  model: "claude-3-sonnet-20240229"
  temperature: 0.3  # DÃ¼ÅŸÃ¼k = Ã§alÄ±ÅŸan kod

dataset:
  creation:
    num_steps: 100
    batch_size: 5
  save_as: "python_code.jsonl"
```

#### Åablon 3: Multi-Turn Conversations

```yaml
dataset_system_prompt: "Sen bir dost canlÄ±sÄ± chatbot'sun."

topic_tree:
  topic_prompt: "GÃ¼nlÃ¼k KonuÅŸma KonularÄ±: Hobiler, Seyahat, Teknoloji"
  provider: "openai"
  model: "gpt-4o"
  temperature: 0.8  # YÃ¼ksek = Ã§eÅŸitli konuÅŸmalar
  degree: 3
  depth: 3

data_engine:
  generation_system_prompt: |
    DoÄŸal bir konuÅŸma yarat:
    - 3-5 tur konuÅŸma
    - Context'i koru
    - ArkadaÅŸÃ§a ton
    - Ä°lgi gÃ¶ster
  
  conversation_type: "multi_turn"
  provider: "openai"
  model: "gpt-4o"
  temperature: 0.7  # YÃ¼ksek = doÄŸal konuÅŸma

dataset:
  creation:
    num_steps: 200
    batch_size: 5
  save_as: "chatbot_conversations.jsonl"
```

---

## 6. SDK KullanÄ±mÄ±

### ğŸ Python SDK ile Programatik KullanÄ±m

DeepFabric'i Python kodundan kullanmak iÃ§in:

```python
from deepfabric import TopicTree, DataEngine, Dataset

# 1. Topic Tree oluÅŸtur
topic_tree = TopicTree(
    topic_prompt="Machine Learning Fundamentals",
    provider="openai",
    model="gpt-4-turbo",
    temperature=0.7,
    degree=3,
    depth=2
)

# Topic'leri Ã¼ret
topics = topic_tree.generate()
print(f"OluÅŸturulan konu sayÄ±sÄ±: {len(topics)}")

# Topic'leri kaydet
topic_tree.save("topics.jsonl")

# 2. Data Engine ile veri Ã¼ret
data_engine = DataEngine(
    topics=topics,
    provider="ollama",
    model="llama3:70b",
    temperature=0.5,
    conversation_type="qa"
)

# Veri Ã¼ret
dataset = data_engine.generate(
    num_steps=100,
    batch_size=5
)

# 3. Dataset'i kaydet
dataset.save("ml_dataset.jsonl")

# HuggingFace'e yÃ¼kle (opsiyonel)
dataset.push_to_hub(
    repo_id="username/ml-qa-dataset",
    token="hf_..."
)
```

### ğŸ“Š Daha GeliÅŸmiÅŸ SDK KullanÄ±mÄ±

```python
from deepfabric import DeepFabric

# Tam pipeline
fabric = DeepFabric(
    # Topic Tree config
    topic_config={
        "topic_prompt": "Python Web Development",
        "provider": "openai",
        "model": "gpt-4-turbo",
        "temperature": 0.7,
        "degree": 4,
        "depth": 3
    },
    
    # Data Engine config
    data_config={
        "provider": "ollama",
        "model": "llama3:70b",
        "temperature": 0.4,
        "conversation_type": "code",
        "generation_system_prompt": "Sen bir web development uzmanÄ±sÄ±n."
    },
    
    # Dataset config
    dataset_config={
        "num_steps": 200,
        "batch_size": 5,
        "sys_msg": True
    }
)

# TÃ¼m pipeline'Ä± Ã§alÄ±ÅŸtÄ±r
fabric.generate()

# SonuÃ§larÄ± kaydet
fabric.save("web_dev_dataset.jsonl")

# Ä°statistikleri gÃ¶rÃ¼ntÃ¼le
stats = fabric.get_statistics()
print(f"Toplam konu: {stats['total_topics']}")
print(f"Toplam Ã¶rnek: {stats['total_examples']}")
print(f"Ortalama cevap uzunluÄŸu: {stats['avg_response_length']}")
```

### ğŸ”§ Custom Handlers

```python
from deepfabric import TopicTree, DataEngine

# Custom topic handler
def my_topic_handler(topic):
    """Her topic iÃ§in Ã¶zel iÅŸlem"""
    print(f"Ä°ÅŸleniyor: {topic['name']}")
    # Filtreleme
    if "advanced" in topic['name'].lower():
        topic['difficulty'] = "hard"
    return topic

# Custom data handler
def my_data_handler(data_point):
    """Her veri noktasÄ± iÃ§in Ã¶zel iÅŸlem"""
    # Format kontrolÃ¼
    if len(data_point['assistant']) < 50:
        return None  # Ã‡ok kÄ±sa cevaplarÄ± atla
    return data_point

# Handler'larÄ± kullan
topic_tree = TopicTree(
    topic_prompt="Python Advanced",
    handler=my_topic_handler
)

data_engine = DataEngine(
    topics=topic_tree.generate(),
    handler=my_data_handler
)

dataset = data_engine.generate(num_steps=100)
```

### ğŸ“ Batch Ä°ÅŸleme

```python
from deepfabric import DataEngine
import time

# BÃ¼yÃ¼k dataset'i batch'ler halinde iÅŸle
def process_large_dataset():
    topics = load_topics("topics.jsonl")
    
    total_examples = 10000
    batch_size = 100
    
    data_engine = DataEngine(topics=topics)
    
    for i in range(0, total_examples, batch_size):
        print(f"Ä°ÅŸleniyor: {i}-{i+batch_size}")
        
        batch = data_engine.generate(
            num_steps=batch_size,
            batch_size=10
        )
        
        # Batch'i kaydet
        batch.save(f"batch_{i}.jsonl")
        
        # Rate limit iÃ§in bekle
        time.sleep(5)
    
    # TÃ¼m batch'leri birleÅŸtir
    merge_batches("batch_*.jsonl", "final_dataset.jsonl")

process_large_dataset()
```

---

## 7. Provider'lar ve Modeller

### ğŸ¤– Desteklenen Provider'lar

#### 1. OpenAI

```bash
# Environment variable
export OPENAI_API_KEY="sk-..."

# CLI
deepfabric generate \
  --provider openai \
  --model gpt-4-turbo \
  ...
```

```yaml
# YAML
topic_tree:
  provider: "openai"
  model: "gpt-4-turbo"  # veya gpt-4o, gpt-3.5-turbo
```

**Ã–nerilen Modeller:**
- `gpt-4-turbo`: En gÃ¼Ã§lÃ¼, pahalÄ±
- `gpt-4o`: Dengeli performans/maliyet
- `gpt-3.5-turbo`: HÄ±zlÄ±, ucuz, basit tasklar iÃ§in

#### 2. Anthropic (Claude)

```bash
export ANTHROPIC_API_KEY="sk-ant-..."

deepfabric generate \
  --provider anthropic \
  --model claude-3-opus-20240229 \
  ...
```

```yaml
topic_tree:
  provider: "anthropic"
  model: "claude-3-opus-20240229"
  # veya: claude-3-sonnet-20240229, claude-3-haiku-20240307
```

**Ã–nerilen Modeller:**
- `claude-3-opus`: En gÃ¼Ã§lÃ¼, karmaÅŸÄ±k tasklar
- `claude-3-sonnet`: Dengeli, kod iÃ§in mÃ¼kemmel
- `claude-3-haiku`: En hÄ±zlÄ±, basit tasklar

#### 3. Google (Gemini)

```bash
export GEMINI_API_KEY="..."

deepfabric generate \
  --provider gemini \
  --model gemini-pro \
  ...
```

```yaml
topic_tree:
  provider: "gemini"
  model: "gemini-pro"
```

#### 4. Ollama (Yerel)

```bash
# API key gerekmez, yerel Ã§alÄ±ÅŸÄ±r

# Ã–nce Ollama kurulumu
curl -fsSL https://ollama.com/install.sh | sh

# Model indir
ollama pull llama3:70b

# DeepFabric ile kullan
deepfabric generate \
  --provider ollama \
  --model llama3:70b \
  ...
```

```yaml
topic_tree:
  provider: "ollama"
  model: "llama3:70b"
  # veya: llama3:8b, mistral:7b, qwen3:32b
```

**Avantajlar:**
- ğŸ†“ Tamamen Ã¼cretsiz
- ğŸ”’ Veri gizliliÄŸi (yerel)
- âš¡ Rate limit yok

**Dezavantajlar:**
- ğŸ’» GÃ¼Ã§lÃ¼ GPU gerekir
- ğŸŒ Cloud API'lerden yavaÅŸ

#### 5. OpenRouter

```bash
export OPENROUTER_API_KEY="sk-or-..."

deepfabric generate \
  --provider openrouter \
  --model meta-llama/llama-3.1-70b-instruct \
  ...
```

```yaml
topic_tree:
  provider: "openrouter"
  model: "meta-llama/llama-3.1-70b-instruct"
```

**Avantajlar:**
- ğŸŒ Tek API'den Ã§oklu model
- ğŸ’° Uygun fiyatlandÄ±rma
- ğŸš€ Ã‡ok sayÄ±da model seÃ§eneÄŸi

**PopÃ¼ler OpenRouter Modelleri:**
```yaml
# Llama modelleri
model: "meta-llama/llama-3.1-70b-instruct"
model: "meta-llama/llama-3.1-8b-instruct"

# Mistral modelleri
model: "mistralai/mixtral-8x7b-instruct"
model: "mistralai/mistral-7b-instruct"

# Anthropic (OpenRouter Ã¼zerinden)
model: "anthropic/claude-3-sonnet"

# OpenAI (OpenRouter Ã¼zerinden)
model: "openai/gpt-4-turbo"
```

### ğŸ¯ Model SeÃ§im Rehberi

#### Maliyet OdaklÄ±

```yaml
# EN UCUZ: Ãœcretsiz
topic_tree:
  provider: "ollama"
  model: "llama3:8b"
data_engine:
  provider: "ollama"
  model: "llama3:8b"

# UCUZ: OpenRouter
topic_tree:
  provider: "openrouter"
  model: "meta-llama/llama-3.1-70b-instruct"
data_engine:
  provider: "openrouter"
  model: "meta-llama/llama-3.1-8b-instruct"

# PAHALI: Premium modeller
topic_tree:
  provider: "openai"
  model: "gpt-4-turbo"
data_engine:
  provider: "anthropic"
  model: "claude-3-opus-20240229"
```

#### Kalite OdaklÄ±

```yaml
# YÃœK SEK KALÄ°TE
topic_tree:
  provider: "openai"
  model: "gpt-4-turbo"          # En iyi topic generation
data_engine:
  provider: "anthropic"
  model: "claude-3-opus-20240229"  # En iyi data generation

# DENGELI KALÄ°TE
topic_tree:
  provider: "openai"
  model: "gpt-4o"
data_engine:
  provider: "anthropic"
  model: "claude-3-sonnet-20240229"
```

#### HÄ±z OdaklÄ±

```yaml
# EN HIZLI (yerel)
topic_tree:
  provider: "ollama"
  model: "llama3:8b"
data_engine:
  provider: "ollama"
  model: "llama3:8b"

# HIZLI (cloud)
topic_tree:
  provider: "openai"
  model: "gpt-3.5-turbo"
data_engine:
  provider: "anthropic"
  model: "claude-3-haiku-20240307"
```

---

## 8. Pratik Ã–rnekler

### ğŸ“ Ã–rnek 1: EÄŸitim Materyali Ãœretimi

**Hedef:** Lise Ã¶ÄŸrencileri iÃ§in matematik Q&A

```yaml
# math_education.yaml
dataset_system_prompt: |
  Sen bir lise matematik Ã¶ÄŸretmenisin. 
  Ã–ÄŸrencilere sabÄ±rlÄ± ve aÃ§Ä±klayÄ±cÄ± ÅŸekilde matematik Ã¶ÄŸretiyorsun.

topic_tree:
  topic_prompt: |
    Lise Matematik MÃ¼fredatÄ±:
    - Algebra (denklemler, eÅŸitsizlikler)
    - Geometri (ÅŸekiller, alan, hacim)
    - Trigonometri (sin, cos, tan)
    - Fonksiyonlar (grafik, tÃ¼rev)
  
  provider: "openai"
  model: "gpt-4o"
  temperature: 0.7
  degree: 4
  depth: 2

data_engine:
  generation_system_prompt: |
    Her soru iÃ§in:
    1. AdÄ±m adÄ±m Ã§Ã¶zÃ¼m gÃ¶ster
    2. GÃ¶rsel aÃ§Ä±klamalar kullan (ASCII art)
    3. YaygÄ±n hatalarÄ± belirt
    4. Pratik Ã¶rnekler ver
  
  conversation_type: "chain_of_thought"
  provider: "openai"
  model: "gpt-4o"
  temperature: 0.5

dataset:
  creation:
    num_steps: 200
    batch_size: 5
  save_as: "high_school_math.jsonl"

huggingface:
  repository: "edutech/high-school-math-qa"
  tags:
    - "education"
    - "mathematics"
    - "high-school"
```

```bash
# Ã‡alÄ±ÅŸtÄ±r
deepfabric generate math_education.yaml
```

**Beklenen Output:**
```jsonl
{"messages": [
  {"role": "system", "content": "Sen bir lise matematik Ã¶ÄŸretmenisin..."},
  {"role": "user", "content": "2x + 5 = 15 denklemini nasÄ±l Ã§Ã¶zerim?"},
  {"role": "assistant", "content": "Bu denklemi adÄ±m adÄ±m Ã§Ã¶zelim:\n\nAdÄ±m 1: Her iki taraftan 5 Ã§Ä±kar\n2x + 5 - 5 = 15 - 5\n2x = 10\n\nAdÄ±m 2: Her iki tarafÄ± 2'ye bÃ¶l\n2x / 2 = 10 / 2\nx = 5\n\nDoÄŸrulama: 2(5) + 5 = 10 + 5 = 15 âœ“"}
]}
```

### ğŸ’» Ã–rnek 2: Kod Ãœretimi iÃ§in Dataset

**Hedef:** Python algoritma problemleri

```yaml
# python_coding.yaml
dataset_system_prompt: "Sen bir yazÄ±lÄ±m mÃ¼hendisi ve algoritma uzmanÄ±sÄ±n."

topic_tree:
  topic_prompt: |
    Python AlgoritmalarÄ±:
    - Sorting (QuickSort, MergeSort, HeapSort)
    - Searching (Binary Search, DFS, BFS)
    - Dynamic Programming (Fibonacci, Knapsack)
    - Data Structures (LinkedList, Tree, Graph)
  
  provider: "anthropic"
  model: "claude-3-sonnet-20240229"
  temperature: 0.6
  degree: 4
  depth: 2

data_engine:
  generation_system_prompt: |
    Her algoritma iÃ§in:
    - Temiz, Ã§alÄ±ÅŸan Python kodu
    - Docstring ile aÃ§Ä±klama
    - Zaman karmaÅŸÄ±klÄ±ÄŸÄ± (Big O)
    - Test case'leri
    - Edge case'leri ele al
  
  conversation_type: "code"
  provider: "anthropic"
  model: "claude-3-sonnet-20240229"
  temperature: 0.3  # DÃ¼ÅŸÃ¼k = Ã§alÄ±ÅŸan kod

dataset:
  creation:
    num_steps: 150
    batch_size: 5
  save_as: "python_algorithms.jsonl"
```

**Beklenen Output:**
```jsonl
{"messages": [
  {"role": "user", "content": "Binary search algoritmasÄ±nÄ± Python'da implement et"},
  {"role": "assistant", "content": "```python\ndef binary_search(arr: list, target: int) -> int:\n    \"\"\"\n    Binary search algoritmasÄ±.\n    \n    Args:\n        arr: SÄ±ralÄ± liste\n        target: Aranan deÄŸer\n    \n    Returns:\n        Index (bulunursa), -1 (bulunamazsa)\n    \n    Time Complexity: O(log n)\n    Space Complexity: O(1)\n    \"\"\"\n    left, right = 0, len(arr) - 1\n    \n    while left <= right:\n        mid = (left + right) // 2\n        \n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    \n    return -1\n\n# Test cases\nassert binary_search([1, 3, 5, 7, 9], 5) == 2\nassert binary_search([1, 3, 5, 7, 9], 10) == -1\nassert binary_search([], 5) == -1\n```"}
]}
```

### ğŸ¤– Ã–rnek 3: Chatbot KonuÅŸmalarÄ±

**Hedef:** DoÄŸal dil konuÅŸmalarÄ±

```yaml
# chatbot_conversations.yaml
dataset_system_prompt: "Sen arkadaÅŸÃ§a ve yardÄ±msever bir chatbot'sun."

topic_tree:
  topic_prompt: |
    GÃ¼nlÃ¼k KonuÅŸma KonularÄ±:
    - Hobiler ve ilgi alanlarÄ±
    - Seyahat deneyimleri
    - Teknoloji ve gadget'lar
    - Yemek ve tarifler
    - Spor ve fitness
  
  provider: "openai"
  model: "gpt-4o"
  temperature: 0.8  # YÃ¼ksek = Ã§eÅŸitli konuÅŸmalar
  degree: 3
  depth: 3

data_engine:
  generation_system_prompt: |
    3-5 turlu doÄŸal bir konuÅŸma yarat:
    - Samimi ve arkadaÅŸÃ§a ton
    - Context'i koru
    - Sorular sor, ilgi gÃ¶ster
    - Empati kur
    - Bilgi paylaÅŸ
  
  conversation_type: "multi_turn"
  provider: "openai"
  model: "gpt-4o"
  temperature: 0.7

dataset:
  creation:
    num_steps: 300
    batch_size: 5
  save_as: "chatbot_multi_turn.jsonl"
```

**Beklenen Output:**
```jsonl
{"messages": [
  {"role": "user", "content": "FotoÄŸrafÃ§Ä±lÄ±k hobiyle mi ilgileniyorsun?"},
  {"role": "assistant", "content": "Evet, fotoÄŸrafÃ§Ä±lÄ±ÄŸa ilgi duyuyorum! Ã–zellikle doÄŸa ve manzara fotoÄŸrafÃ§Ä±lÄ±ÄŸÄ± hoÅŸuma gidiyor. Sen hiÃ§ fotoÄŸrafÃ§Ä±lÄ±kla uÄŸraÅŸtÄ±n mÄ±?"},
  {"role": "user", "content": "Biraz, ama genelde telefon kamerasÄ±yla Ã§ekiyorum"},
  {"role": "assistant", "content": "Telefon kameralarÄ± artÄ±k Ã§ok geliÅŸti! AslÄ±nda Ã¶nemli olan ekipman deÄŸil, kompozisyon ve Ä±ÅŸÄ±k. Hangi tÃ¼r fotoÄŸraflar Ã§ekmeyi seviyorsun?"},
  {"role": "user", "content": "Genelde sokak fotoÄŸraflarÄ± ve insanlar ilgimi Ã§ekiyor"},
  {"role": "assistant", "content": "Sokak fotoÄŸrafÃ§Ä±lÄ±ÄŸÄ± harika bir alan! Ä°nsanlarÄ±n doÄŸal anlarÄ±nÄ± yakalamak gerÃ§ekten Ã¶zel. BirkaÃ§ ipucu: golden hour'da (gÃ¼n batÄ±mÄ±) Ã§ek, kalabalÄ±k yerlere git, ve Ã§ekmeden Ã¶nce izin almayÄ± unutma. Hangi ÅŸehirde Ã§ekiyorsun?"}
]}
```

### ğŸ¥ Ã–rnek 4: Domain-Specific (Medikal)

**Hedef:** TÄ±bbi Q&A (dikkat: bu sadece Ã¶rnek, gerÃ§ek medikal veriler iÃ§in uzman danÄ±ÅŸÄ±lmalÄ±)

```yaml
# medical_qa.yaml
dataset_system_prompt: |
  Sen bir tÄ±bbi bilgi asistanÄ±sÄ±n. UYARI: TeÅŸhis koymuyorsun, 
  sadece genel bilgi veriyorsun.

topic_tree:
  topic_prompt: |
    Genel SaÄŸlÄ±k Bilgileri:
    - YaygÄ±n hastalÄ±klar ve belirtileri
    - SaÄŸlÄ±klÄ± yaÅŸam Ã¶nerileri
    - Ä°lk yardÄ±m bilgileri
    - Ã–nleyici saÄŸlÄ±k
  
  provider: "anthropic"
  model: "claude-3-opus-20240229"
  temperature: 0.6
  degree: 4
  depth: 2

data_engine:
  generation_system_prompt: |
    Her soru iÃ§in:
    - TÄ±bbi aÃ§Ä±dan doÄŸru bilgi ver
    - TeÅŸhis koyma, sadece bilgilendir
    - "Doktora danÄ±ÅŸÄ±n" uyarÄ±sÄ± ekle
    - KaynaklarÄ± belirt (genel tÄ±bbi literatÃ¼r)
  
  provider: "anthropic"
  model: "claude-3-opus-20240229"
  temperature: 0.4  # DÃ¼ÅŸÃ¼k = doÄŸru bilgi

dataset:
  creation:
    num_steps: 100
    batch_size: 3
  save_as: "medical_general_qa.jsonl"
```

### ğŸ› ï¸ Ã–rnek 5: Tool Calling iÃ§in Dataset

**Hedef:** API fonksiyon Ã§aÄŸÄ±rma Ã¶ÄŸretimi

```yaml
# tool_calling.yaml
dataset_system_prompt: "Sen bir API asistanÄ±sÄ±n."

topic_tree:
  topic_prompt: |
    API KullanÄ±m SenaryolarÄ±:
    - Hava durumu sorgulama
    - Ã‡eviri iÅŸlemleri
    - Takvim yÃ¶netimi
    - Dosya iÅŸlemleri
  
  provider: "openai"
  model: "gpt-4o"
  temperature: 0.7
  degree: 3
  depth: 2

data_engine:
  generation_system_prompt: |
    Her senaryo iÃ§in:
    - KullanÄ±cÄ± isteÄŸini anla
    - DoÄŸru fonksiyonu seÃ§
    - Parametreleri Ã§Ä±kar
    - Fonksiyonu Ã§aÄŸÄ±r
    - Sonucu yorumla
  
  conversation_type: "tool_calling"
  provider: "openai"
  model: "gpt-4o"
  temperature: 0.4

dataset:
  creation:
    num_steps: 80
    batch_size: 4
  save_as: "tool_calling_examples.jsonl"
```

**Beklenen Output:**
```jsonl
{"messages": [
  {"role": "user", "content": "Ä°stanbul'da hava nasÄ±l?"},
  {"role": "assistant", "content": null, "tool_calls": [
    {
      "id": "call_123",
      "type": "function",
      "function": {
        "name": "get_weather",
        "arguments": "{\"city\": \"Istanbul\", \"country\": \"Turkey\"}"
      }
    }
  ]},
  {"role": "tool", "tool_call_id": "call_123", "content": "{\"temperature\": 18, \"condition\": \"sunny\", \"humidity\": 65}"},
  {"role": "assistant", "content": "Ä°stanbul'da ÅŸu anda hava 18Â°C ve gÃ¼neÅŸli. Nem oranÄ± %65."}
]}
```

---

## 9. Ä°leri Seviye KullanÄ±m

### ğŸ”„ Pipeline Optimizasyonu

#### Strateji 1: Hibrit Model KullanÄ±mÄ±

```yaml
# PahalÄ± model sadece topic generation iÃ§in
topic_tree:
  provider: "openai"
  model: "gpt-4-turbo"      # $$$
  temperature: 0.7

# Ucuz model data generation iÃ§in
data_engine:
  provider: "ollama"
  model: "llama3:70b"        # Ãœcretsiz
  temperature: 0.5

# SonuÃ§: %70 maliyet tasarrufu, minimal kalite kaybÄ±
```

#### Strateji 2: Kademeli BÃ¼yÃ¼tme

```python
# 1. KÃ¼Ã§Ã¼k baÅŸla, test et
small_config = {
    "depth": 1,
    "degree": 2,
    "num_steps": 10
}

# 2. Kaliteyi deÄŸerlendir
evaluate_quality(small_dataset)

# 3. Parametreleri bÃ¼yÃ¼t
if quality_good:
    large_config = {
        "depth": 3,
        "degree": 4,
        "num_steps": 1000
    }
```

#### Strateji 3: Paralel Ä°ÅŸleme

```bash
# FarklÄ± topic'ler iÃ§in paralel pipeline'lar

# Terminal 1
deepfabric generate config_math.yaml &

# Terminal 2
deepfabric generate config_physics.yaml &

# Terminal 3
deepfabric generate config_chemistry.yaml &

# Sonra birleÅŸtir
cat math.jsonl physics.jsonl chemistry.jsonl > combined.jsonl
```

### ğŸ“Š Kalite KontrolÃ¼

```python
# quality_check.py
import json

def check_quality(dataset_path):
    """Dataset kalitesini kontrol et"""
    with open(dataset_path) as f:
        data = [json.loads(line) for line in f]
    
    issues = []
    
    for i, item in enumerate(data):
        messages = item['messages']
        
        # Kontrol 1: Ã‡ok kÄ±sa cevaplar
        assistant_msg = [m for m in messages if m['role'] == 'assistant'][0]
        if len(assistant_msg['content']) < 50:
            issues.append(f"Line {i}: Cevap Ã§ok kÄ±sa")
        
        # Kontrol 2: BoÅŸ cevaplar
        if not assistant_msg['content'].strip():
            issues.append(f"Line {i}: BoÅŸ cevap")
        
        # Kontrol 3: Tekrar eden ifadeler
        if "I apologize" in assistant_msg['content']:
            issues.append(f"Line {i}: Hata mesajÄ± iÃ§eriyor")
    
    print(f"Toplam: {len(data)} Ã¶rnek")
    print(f"Sorunlu: {len(issues)} Ã¶rnek")
    
    return issues

# KullanÄ±m
issues = check_quality("dataset.jsonl")
for issue in issues:
    print(issue)
```

### ğŸ” Veri Analizi ve GÃ¶rselleÅŸtirme

```python
# analyze_dataset.py
import json
import matplotlib.pyplot as plt
from collections import Counter

def analyze_dataset(dataset_path):
    """Dataset istatistikleri"""
    with open(dataset_path) as f:
        data = [json.loads(line) for line in f]
    
    # Uzunluk analizi
    lengths = []
    for item in data:
        assistant = [m for m in item['messages'] if m['role'] == 'assistant'][0]
        lengths.append(len(assistant['content']))
    
    # Ä°statistikler
    print(f"Toplam Ã¶rnek: {len(data)}")
    print(f"Ortalama uzunluk: {sum(lengths)/len(lengths):.0f} karakter")
    print(f"Min uzunluk: {min(lengths)}")
    print(f"Max uzunluk: {max(lengths)}")
    
    # Histogram
    plt.figure(figsize=(10, 6))
    plt.hist(lengths, bins=50)
    plt.xlabel('Cevap UzunluÄŸu (karakter)')
    plt.ylabel('Frekans')
    plt.title('Cevap UzunluÄŸu DaÄŸÄ±lÄ±mÄ±')
    plt.savefig('length_distribution.png')
    
    # Kelime analizi
    all_words = []
    for item in data:
        assistant = [m for m in item['messages'] if m['role'] == 'assistant'][0]
        all_words.extend(assistant['content'].lower().split())
    
    most_common = Counter(all_words).most_common(20)
    print("\nEn yaygÄ±n kelimeler:")
    for word, count in most_common:
        print(f"  {word}: {count}")

# KullanÄ±m
analyze_dataset("dataset.jsonl")
```

### ğŸ§¹ Veri Temizleme

```python
# clean_dataset.py
import json

def clean_dataset(input_path, output_path):
    """Dataset'i temizle ve filtrele"""
    with open(input_path) as f:
        data = [json.loads(line) for line in f]
    
    cleaned = []
    stats = {
        "total": len(data),
        "too_short": 0,
        "has_errors": 0,
        "duplicates": 0,
        "kept": 0
    }
    
    seen_questions = set()
    
    for item in data:
        messages = item['messages']
        user = [m for m in messages if m['role'] == 'user'][0]
        assistant = [m for m in messages if m['role'] == 'assistant'][0]
        
        # Filtre 1: Ã‡ok kÄ±sa
        if len(assistant['content']) < 50:
            stats["too_short"] += 1
            continue
        
        # Filtre 2: Hata mesajlarÄ±
        if "I apologize" in assistant['content'] or "I cannot" in assistant['content']:
            stats["has_errors"] += 1
            continue
        
        # Filtre 3: Duplikasyonlar
        if user['content'] in seen_questions:
            stats["duplicates"] += 1
            continue
        
        seen_questions.add(user['content'])
        cleaned.append(item)
        stats["kept"] += 1
    
    # TemizlenmiÅŸ veriyi kaydet
    with open(output_path, 'w') as f:
        for item in cleaned:
            f.write(json.dumps(item) + '\n')
    
    # Rapor
    print("Temizleme Raporu:")
    print(f"  Toplam: {stats['total']}")
    print(f"  Ã‡ok kÄ±sa: {stats['too_short']}")
    print(f"  HatalÄ±: {stats['has_errors']}")
    print(f"  Duplikat: {stats['duplicates']}")
    print(f"  Tutuldu: {stats['kept']} ({100*stats['kept']/stats['total']:.1f}%)")

# KullanÄ±m
clean_dataset("raw_dataset.jsonl", "cleaned_dataset.jsonl")
```

---

## 10. Best Practices

### âœ… YapÄ±lmasÄ± Gerekenler

#### 1. KÃ¼Ã§Ã¼k BaÅŸlayÄ±n, Test Edin

```yaml
# âŒ Ä°lk denemede bÃ¼yÃ¼k
depth: 5
num_steps: 10000

# âœ… Ä°lk denemede kÃ¼Ã§Ã¼k
depth: 1
num_steps: 10
```

#### 2. Topic Tree'yi Kaydedin ve Ä°nceleyin

```yaml
topic_tree:
  save_as: "topics_debug.jsonl"  # Mutlaka kaydedin!
```

```python
# Topic'leri inceleyin
with open("topics_debug.jsonl") as f:
    topics = [json.loads(line) for line in f]
    for topic in topics[:10]:  # Ä°lk 10'u gÃ¶ster
        print(f"- {topic['topic']}")
```

#### 3. Temperature'Ä± DoÄŸru AyarlayÄ±n

```yaml
# Topic Generation: YÃ¼ksek (Ã§eÅŸitlilik)
topic_tree:
  temperature: 0.7

# Data Generation: DÃ¼ÅŸÃ¼k (tutarlÄ±lÄ±k)
data_engine:
  temperature: 0.4
```

#### 4. Hibrit Model Stratejisi

```yaml
# PahalÄ± model â†’ Topic
topic_tree:
  provider: "openai"
  model: "gpt-4-turbo"

# Ucuz model â†’ Data
data_engine:
  provider: "ollama"
  model: "llama3:70b"
```

#### 5. Batch Size'Ä± Optimize Edin

```python
# Rate limit varsa
batch_size: 1-2

# Rate limit yoksa
batch_size: 5-10
```

#### 6. Kalite KontrolÃ¼ YapÄ±n

```bash
# Her 100 Ã¶rnekte bir kontrol edin
python quality_check.py dataset.jsonl
```

#### 7. Versionlama KullanÄ±n

```bash
# Git ile versiyonlayÄ±n
git add config_v1.yaml
git commit -m "Initial config"

# Dataset'leri version'layÄ±n
dataset_v1_2024-11-09.jsonl
dataset_v2_2024-11-10.jsonl
```

### âŒ YapÄ±lmamasÄ± Gerekenler

#### 1. Ä°lk Denemede BÃ¼yÃ¼k Ãœretim

```yaml
# âŒ YAPMAYIN
depth: 5
degree: 10
num_steps: 50000
# â†’ GÃ¼nler sÃ¼rer, maliyetli, muhtemelen hatalÄ±
```

#### 2. Temperature'Ä± Ã‡ok YÃ¼ksek/DÃ¼ÅŸÃ¼k

```yaml
# âŒ YAPMAYIN
topic_tree:
  temperature: 0.0  # Ã‡ok monoton
  
data_engine:
  temperature: 1.5  # Ã‡ok kaotik
```

#### 3. Topic Tree'yi Atlama

```bash
# âŒ YAPMAYIN - Rastgele veri Ã¼retimi
# Topic tree olmadan data engine kullanma
```

#### 4. Kalite KontrolsÃ¼z Ãœretim

```python
# âŒ YAPMAYIN
generate_10000_examples()  # HiÃ§ kontrol etmeden
```

#### 5. API Key'leri Kodda Saklama

```yaml
# âŒ YAPMAYIN
huggingface:
  token: "hf_actual_token_here"  # Git'e gider!

# âœ… YAPIN
huggingface:
  token: "${HF_TOKEN}"  # Environment variable
```

### ğŸ¯ Ã–nerilen Workflow

```bash
# 1. KÃ¼Ã§Ã¼k test
deepfabric generate config.yaml  # num_steps: 10

# 2. Kalite kontrolÃ¼
python quality_check.py test_output.jsonl

# 3. Parametreleri ayarla
# config.yaml'Ä± dÃ¼zenle

# 4. Orta Ã¶lÃ§ekli Ã¼retim
deepfabric generate config.yaml  # num_steps: 100

# 5. Analiz
python analyze_dataset.py output.jsonl

# 6. Temizleme
python clean_dataset.py output.jsonl cleaned.jsonl

# 7. BÃ¼yÃ¼k Ã¼retim
deepfabric generate final_config.yaml  # num_steps: 1000+

# 8. HuggingFace'e yÃ¼kle
deepfabric generate final_config.yaml --hf-repo user/dataset
```

---

## 11. Troubleshooting

### ğŸ”§ YaygÄ±n Sorunlar ve Ã‡Ã¶zÃ¼mler

#### Sorun 1: Rate Limit HatasÄ±

```
Error: Rate limit exceeded (429)
```

**Ã‡Ã¶zÃ¼m:**
```yaml
# Batch size'Ä± azalt
dataset:
  creation:
    batch_size: 1  # veya 2

# max_retries'Ä± artÄ±r
data_engine:
  max_retries: 5

# FarklÄ± provider dene
provider: "ollama"  # Rate limit yok
```

#### Sorun 2: API AnahtarÄ± HatasÄ±

```
Error: Invalid API key
```

**Ã‡Ã¶zÃ¼m:**
```bash
# DoÄŸru environment variable'Ä± kullandÄ±ÄŸÄ±nÄ±zdan emin olun
echo $OPENAI_API_KEY

# Tekrar ayarlayÄ±n
export OPENAI_API_KEY="sk-..."

# Kontrol edin
deepfabric generate config.yaml
```

#### Sorun 3: BoÅŸ/KÄ±sa Cevaplar

```json
{"role": "assistant", "content": "I apologize, I cannot..."}
```

**Ã‡Ã¶zÃ¼m:**
```yaml
# 1. Temperature'Ä± ayarla
data_engine:
  temperature: 0.6  # Ã‡ok dÃ¼ÅŸÃ¼kse artÄ±r

# 2. System prompt'u iyileÅŸtir
data_engine:
  generation_system_prompt: |
    ZORUNLU: Her soruya en az 100 kelime ile detaylÄ± cevap ver.
    Asla Ã¶zÃ¼r dileme veya "yapamam" deme.

# 3. Model deÄŸiÅŸtir
data_engine:
  model: "gpt-4-turbo"  # Daha gÃ¼Ã§lÃ¼ model
```

#### Sorun 4: AynÄ± Konular Tekrar Ediyor

```
Konu 1: "Python Lists"
Konu 2: "Lists in Python"
Konu 3: "Python List Operations"
```

**Ã‡Ã¶zÃ¼m:**
```yaml
# Temperature'Ä± artÄ±r
topic_tree:
  temperature: 0.8  # Daha Ã§eÅŸitli

# Topic prompt'u daha spesifik yap
topic_tree:
  topic_prompt: |
    Python Veri YapÄ±larÄ± (her biri benzersiz):
    - List (temel iÅŸlemler)
    - Dictionary (key-value pairs)
    - Set (unique elements)
    - Tuple (immutable sequences)
```

#### Sorun 5: Memory HatasÄ±

```
Error: Out of memory
```

**Ã‡Ã¶zÃ¼m:**
```yaml
# Batch size'Ä± azalt
dataset:
  creation:
    batch_size: 1

# Daha kÃ¼Ã§Ã¼k model kullan
provider: "ollama"
model: "llama3:8b"  # 70b yerine
```

#### Sorun 6: Ã‡ok YavaÅŸ Ãœretim

**Ã‡Ã¶zÃ¼m:**
```yaml
# 1. Yerel model kullan
provider: "ollama"

# 2. Batch size'Ä± artÄ±r
batch_size: 10  # (rate limit yoksa)

# 3. Daha hÄ±zlÄ± model
model: "gpt-3.5-turbo"  # gpt-4 yerine

# 4. Paralel iÅŸleme
# Birden fazla terminal'de Ã§alÄ±ÅŸtÄ±r
```

#### Sorun 7: JSON Format HatasÄ±

```
Error: Invalid JSON in output
```

**Ã‡Ã¶zÃ¼m:**
```yaml
# System prompt'a format talimatÄ± ekle
generation_system_prompt: |
  Her zaman geÃ§erli, dÃ¼zgÃ¼n formatlanmÄ±ÅŸ cevap ver.
  Ã–zel karakterleri escape et.
  JSON formatÄ±na uygun ol.
```

### ğŸ“ YardÄ±m Alma

```bash
# Resmi dokÃ¼mantasyon
# https://github.com/lukehinds/deepfabric

# Issue aÃ§ma
# https://github.com/lukehinds/deepfabric/issues

# Community Discord (varsa)
```

---

## ğŸ“š Ek Kaynaklar

### ğŸ”— Resmi Linkler

- **GitHub**: https://github.com/lukehinds/deepfabric
- **DokÃ¼mantasyon**: https://lukehinds.github.io/DeepFabric/
- **PyPI**: https://pypi.org/project/deepfabric/

### ğŸ“– Ä°lgili Projeler

- **Hugging Face Datasets**: https://huggingface.co/docs/datasets/
- **OpenAI Cookbook**: https://github.com/openai/openai-cookbook
- **Anthropic Claude**: https://docs.anthropic.com/
- **Ollama**: https://ollama.ai/

### ğŸ“ Ã–ÄŸrenme KaynaklarÄ±

- Sentetik veri Ã¼retimi best practices
- Model distillation teknikleri
- Fine-tuning metodlarÄ±
- Prompt engineering rehberleri

---

## ğŸ‰ SonuÃ§

DeepFabric, yÃ¼ksek kaliteli sentetik veri Ã¼retimi iÃ§in gÃ¼Ã§lÃ¼ bir araÃ§tÄ±r. Bu rehber ile:

âœ… DeepFabric'in temel kavramlarÄ±nÄ± Ã¶ÄŸrendiniz
âœ… CLI ve SDK kullanÄ±mÄ±nÄ± gÃ¶rdÃ¼nÃ¼z
âœ… FarklÄ± provider'larÄ± keÅŸfettiniz
âœ… Pratik Ã¶rneklerle deneyim kazandÄ±nÄ±z
âœ… Best practices'leri Ã¶ÄŸrendiniz
âœ… Troubleshooting yÃ¶ntemlerini biliyorsunuz

**BaÅŸarÄ±lar! ğŸš€**

---

*Son gÃ¼ncelleme: 2025-11*
*Versiyon: 1.0*
