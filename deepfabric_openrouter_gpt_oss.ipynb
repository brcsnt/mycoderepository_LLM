{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# DeepFabric ile Sentetik Veri Ãœretimi - OpenRouter + GPT-OSS-20B\n",
    "\n",
    "Bu notebook, DeepFabric kÃ¼tÃ¼phanesini kullanarak **OpenRouter API** Ã¼zerinden **GPT-OSS-20B** modeli ile sentetik veri Ã¼retimini gÃ¶stermektedir.\n",
    "\n",
    "## Ä°Ã§indekiler\n",
    "1. Kurulum ve BaÄŸÄ±mlÄ±lÄ±klar\n",
    "2. OpenRouter API YapÄ±landÄ±rmasÄ±\n",
    "3. FarklÄ± TÃ¼rde Sentetik Veri Ãœretimi:\n",
    "   - Chain of Thought (AdÄ±m AdÄ±m AkÄ±l YÃ¼rÃ¼tme)\n",
    "   - Tool Calling (Fonksiyon Ã‡aÄŸÄ±rma)\n",
    "   - Multi-Turn Conversations (Ã‡ok Turlu KonuÅŸmalar)\n",
    "   - Question-Answer Pairs (Soru-Cevap Ã‡iftleri)\n",
    "   - Code Generation (Kod Ãœretimi)\n",
    "4. Veri Analizi ve GÃ¶rselleÅŸtirme\n",
    "5. Fine-tuning iÃ§in HazÄ±rlÄ±k\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. Kurulum ve BaÄŸÄ±mlÄ±lÄ±klar\n",
    "\n",
    "### DeepFabric ve Gerekli KÃ¼tÃ¼phaneleri Kurma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "# DeepFabric kÃ¼tÃ¼phanesini GitHub'dan kuruyoruz\n",
    "!pip install git+https://github.com/lukehinds/deepfabric.git -q\n",
    "\n",
    "# Gerekli yardÄ±mcÄ± kÃ¼tÃ¼phaneleri kuruyoruz\n",
    "!pip install datasets pandas matplotlib seaborn pyyaml -q\n",
    "\n",
    "print(\"âœ… TÃ¼m kÃ¼tÃ¼phaneler baÅŸarÄ±yla kuruldu!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imports"
   },
   "source": [
    "### KÃ¼tÃ¼phaneleri Ä°Ã§e Aktarma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import-libs"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "import subprocess\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# GÃ¶rselleÅŸtirme ayarlarÄ±\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"âœ… KÃ¼tÃ¼phaneler baÅŸarÄ±yla yÃ¼klendi!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "api-setup"
   },
   "source": [
    "## 2. OpenRouter API YapÄ±landÄ±rmasÄ±\n",
    "\n",
    "### API AnahtarÄ±nÄ± Ayarlama\n",
    "\n",
    "OpenRouter API kullanmak iÃ§in bir API anahtarÄ±na ihtiyacÄ±nÄ±z var:\n",
    "1. [OpenRouter](https://openrouter.ai/) sitesine gidin\n",
    "2. Hesap oluÅŸturun veya giriÅŸ yapÄ±n\n",
    "3. API anahtarÄ±nÄ±zÄ± alÄ±n\n",
    "4. AÅŸaÄŸÄ±daki hÃ¼crede API anahtarÄ±nÄ±zÄ± girin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "api-key"
   },
   "outputs": [],
   "source": [
    "# OpenRouter API anahtarÄ±nÄ± ayarla\n",
    "OPENROUTER_API_KEY = getpass(\"OpenRouter API AnahtarÄ±nÄ±zÄ± girin: \")\n",
    "\n",
    "# Environment variable olarak ayarla\n",
    "os.environ['OPENROUTER_API_KEY'] = OPENROUTER_API_KEY\n",
    "\n",
    "print(\"âœ… API anahtarÄ± ayarlandÄ±!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dirs"
   },
   "source": [
    "### Ã‡alÄ±ÅŸma Dizinlerini OluÅŸturma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create-dirs"
   },
   "outputs": [],
   "source": [
    "# YapÄ±landÄ±rma dizini oluÅŸtur\n",
    "config_dir = Path('deepfabric_configs')\n",
    "config_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Ã‡Ä±ktÄ± dizini oluÅŸtur\n",
    "output_dir = Path('synthetic_datasets')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"âœ… Dizinler oluÅŸturuldu!\")\n",
    "print(f\"   ğŸ“ YapÄ±landÄ±rma dizini: {config_dir}\")\n",
    "print(f\"   ğŸ“ Ã‡Ä±ktÄ± dizini: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-cot"
   },
   "source": [
    "## 3. Sentetik Veri Ãœretimi\n",
    "\n",
    "---\n",
    "\n",
    "## 3.1 Chain of Thought (AdÄ±m AdÄ±m AkÄ±l YÃ¼rÃ¼tme)\n",
    "\n",
    "Bu tip veri, modellere adÄ±m adÄ±m dÃ¼ÅŸÃ¼nmeyi ve problem Ã§Ã¶zmeyi Ã¶ÄŸretmek iÃ§in kullanÄ±lÄ±r."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config-cot"
   },
   "outputs": [],
   "source": "# Chain of Thought yapÄ±landÄ±rmasÄ± - OpenRouter + GPT-OSS-20B\ncot_config = {\n    'dataset_system_prompt': 'Sen bir matematik ve programlama uzmanÄ±sÄ±n. Ã–ÄŸrencilere adÄ±m adÄ±m problem Ã§Ã¶zmeyi Ã¶ÄŸretiyorsun. Her problem iÃ§in dÃ¼ÅŸÃ¼nce sÃ¼recini detaylÄ± aÃ§Ä±klÄ±yorsun.',\n    \n    'topic_graph': {\n        'topic_prompt': 'Matematik ve Programlama Problemleri: Algoritmalar, veri yapÄ±larÄ±, ve matematiksel Ã§Ã¶zÃ¼mlemeler',\n        'provider': 'openrouter',\n        'model': 'neuralmagic/llama-3.1-nemotron-70b-instruct-hf',\n        'api_key': OPENROUTER_API_KEY,\n        'base_url': 'https://openrouter.ai/api/v1',\n        'temperature': 0.7,\n        'degree': 3,  # Her dÃ¼ÄŸÃ¼mden 3 alt konu Ã¼retilecek\n        'depth': 2,   # 2 seviye derinlik\n        'save_as': str(output_dir / 'topics_cot.jsonl')\n    },\n    \n    'data_engine': {\n        'generation_system_prompt': 'Her problemi adÄ±m adÄ±m Ã§Ã¶z ve dÃ¼ÅŸÃ¼nce sÃ¼recini aÃ§Ä±kÃ§a gÃ¶ster.',\n        'instructions': 'AdÄ±m adÄ±m akÄ±l yÃ¼rÃ¼tme ile matematik ve programlama problemleri Ã§Ã¶z.',\n        'conversation_type': 'chain_of_thought',\n        'provider': 'openrouter',\n        'model': 'neuralmagic/llama-3.1-nemotron-70b-instruct-hf',\n        'api_key': OPENROUTER_API_KEY,\n        'base_url': 'https://openrouter.ai/api/v1',\n        'temperature': 0.5,\n        'max_retries': 3\n    },\n    \n    'dataset': {\n        'save_as': str(output_dir / 'cot_math_programming.jsonl'),\n        'creation': {\n            'num_steps': 15,  # 15 Ã¶rnek Ã¼ret\n            'batch_size': 3,\n            'sys_msg': True\n        }\n    }\n}\n\n# YAML dosyasÄ± olarak kaydet\ncot_config_path = config_dir / 'cot_openrouter_config.yaml'\nwith open(cot_config_path, 'w', encoding='utf-8') as f:\n    yaml.dump(cot_config, f, allow_unicode=True)\n\nprint(f\"âœ… Chain of Thought yapÄ±landÄ±rmasÄ± kaydedildi: {cot_config_path}\")\nprint(\"\\nğŸ“‹ YapÄ±landÄ±rma Ã¶zeti:\")\nprint(f\"  â€¢ Konu derinliÄŸi: {cot_config['topic_graph']['depth']}\")\nprint(f\"  â€¢ Konu geniÅŸliÄŸi: {cot_config['topic_graph']['degree']}\")\nprint(f\"  â€¢ Ã–rnek sayÄ±sÄ±: {cot_config['dataset']['creation']['num_steps']}\")\nprint(f\"  â€¢ Batch boyutu: {cot_config['dataset']['creation']['batch_size']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate-cot"
   },
   "outputs": [],
   "source": "# Chain of Thought verisi Ã¼ret\ncmd = [\n    'deepfabric', 'generate',\n    str(cot_config_path)\n]\n\nprint(\"ğŸš€ Chain of Thought verisi Ã¼retiliyor...\")\nprint(f\"ğŸ“ Komut: {' '.join(cmd)}\")\nprint(f\"ğŸ“ Config dosyasÄ±: {cot_config_path}\")\nprint(\"\\n\" + \"=\"*80 + \"\\n\")\n\nresult = subprocess.run(cmd, capture_output=True, text=True)\nprint(result.stdout)\nif result.stderr:\n    print(\"âš ï¸ UyarÄ±lar:\")\n    print(result.stderr)\n\n# Ã‡Ä±ktÄ± dosyalarÄ±nÄ± kontrol et\noutput_file_cot = output_dir / 'cot_math_programming.jsonl'\nif output_file_cot.exists():\n    print(\"\\n\" + \"=\"*80)\n    print(f\"âœ… Chain of Thought verisi oluÅŸturuldu: {output_file_cot}\")\n    print(f\"ğŸ“¦ Dosya boyutu: {output_file_cot.stat().st_size / 1024:.2f} KB\")\nelse:\n    print(\"\\n\" + \"=\"*80)\n    print(f\"âš ï¸ Ã‡Ä±ktÄ± dosyasÄ± bulunamadÄ±: {output_file_cot}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-tool"
   },
   "source": [
    "---\n",
    "\n",
    "## 3.2 Tool Calling (Fonksiyon Ã‡aÄŸÄ±rma)\n",
    "\n",
    "Bu tip veri, modellere API fonksiyonlarÄ±nÄ± doÄŸru seÃ§ip Ã§aÄŸÄ±rmayÄ± Ã¶ÄŸretir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config-tool"
   },
   "outputs": [],
   "source": "# Tool Calling yapÄ±landÄ±rmasÄ±\ntool_config = {\n    'dataset_system_prompt': 'Sen bir API yardÄ±mcÄ±sÄ±sÄ±n. KullanÄ±cÄ±lara doÄŸru fonksiyonlarÄ± seÃ§ip Ã§aÄŸÄ±rmayÄ±, parametreleri doÄŸru belirlemeyi Ã¶ÄŸretiyorsun.',\n    \n    'topic_graph': {\n        'topic_prompt': 'API ve Fonksiyon Ã‡aÄŸÄ±rma: Web servisleri, veritabanÄ± iÅŸlemleri, dosya yÃ¶netimi ve veri dÃ¶nÃ¼ÅŸÃ¼mleri',\n        'provider': 'openrouter',\n        'model': 'neuralmagic/llama-3.1-nemotron-70b-instruct-hf',\n        'api_key': OPENROUTER_API_KEY,\n        'base_url': 'https://openrouter.ai/api/v1',\n        'temperature': 0.8,\n        'degree': 4,\n        'depth': 2,\n        'save_as': str(output_dir / 'topics_tool_calling.jsonl')\n    },\n    \n    'data_engine': {\n        'generation_system_prompt': 'Her iÅŸlem iÃ§in hangi tool kullanÄ±lacaÄŸÄ±nÄ± aÃ§Ä±kla ve doÄŸru parametreleri belirle.',\n        'instructions': 'API fonksiyon Ã§aÄŸrÄ±larÄ±nÄ± doÄŸru bir ÅŸekilde gÃ¶ster.',\n        'conversation_type': 'basic',  # Tool calling iÃ§in basic type kullanÄ±yoruz\n        'provider': 'openrouter',\n        'model': 'neuralmagic/llama-3.1-nemotron-70b-instruct-hf',\n        'api_key': OPENROUTER_API_KEY,\n        'base_url': 'https://openrouter.ai/api/v1',\n        'temperature': 0.3,\n        'max_retries': 3\n    },\n    \n    'dataset': {\n        'save_as': str(output_dir / 'tool_calling_api.jsonl'),\n        'creation': {\n            'num_steps': 12,\n            'batch_size': 3,\n            'sys_msg': True\n        }\n    }\n}\n\ntool_config_path = config_dir / 'tool_calling_config.yaml'\nwith open(tool_config_path, 'w', encoding='utf-8') as f:\n    yaml.dump(tool_config, f, allow_unicode=True)\n\nprint(f\"âœ… Tool Calling yapÄ±landÄ±rmasÄ± kaydedildi: {tool_config_path}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate-tool"
   },
   "outputs": [],
   "source": "# Tool Calling verisi Ã¼ret\ncmd = [\n    'deepfabric', 'generate',\n    str(tool_config_path)\n]\n\nprint(\"ğŸš€ Tool Calling verisi Ã¼retiliyor...\")\nprint(f\"ğŸ“ Config dosyasÄ±: {tool_config_path}\")\nprint(\"\\n\" + \"=\"*80 + \"\\n\")\n\nresult = subprocess.run(cmd, capture_output=True, text=True)\nprint(result.stdout)\nif result.stderr:\n    print(\"âš ï¸ UyarÄ±lar:\")\n    print(result.stderr)\n\noutput_file_tool = output_dir / 'tool_calling_api.jsonl'\nif output_file_tool.exists():\n    print(\"\\n\" + \"=\"*80)\n    print(f\"âœ… Tool Calling verisi oluÅŸturuldu: {output_file_tool}\")\n    print(f\"ğŸ“¦ Dosya boyutu: {output_file_tool.stat().st_size / 1024:.2f} KB\")\nelse:\n    print(\"\\n\" + \"=\"*80)\n    print(f\"âš ï¸ Ã‡Ä±ktÄ± dosyasÄ± bulunamadÄ±: {output_file_tool}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-multiturn"
   },
   "source": [
    "---\n",
    "\n",
    "## 3.3 Multi-Turn Conversations (Ã‡ok Turlu KonuÅŸmalar)\n",
    "\n",
    "Bu tip veri, modellere uzun sÃ¼reli konuÅŸma yeteneÄŸi kazandÄ±rÄ±r."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config-multiturn"
   },
   "outputs": [],
   "source": "# Multi-turn conversation yapÄ±landÄ±rmasÄ±\nmultiturn_config = {\n    'dataset_system_prompt': 'Sen yardÄ±mcÄ± bir teknoloji danÄ±ÅŸmanÄ±sÄ±n. KullanÄ±cÄ±larla doÄŸal, Ã§ok turlu konuÅŸmalar yaparak sorunlarÄ±nÄ± adÄ±m adÄ±m Ã§Ã¶zÃ¼yorsun.',\n    \n    'topic_graph': {\n        'topic_prompt': 'YazÄ±lÄ±m GeliÅŸtirme Destek: Debugging, kod optimizasyonu, mimari kararlar ve best practices',\n        'provider': 'openrouter',\n        'model': 'neuralmagic/llama-3.1-nemotron-70b-instruct-hf',\n        'api_key': OPENROUTER_API_KEY,\n        'base_url': 'https://openrouter.ai/api/v1',\n        'temperature': 0.7,\n        'degree': 3,\n        'depth': 2,\n        'save_as': str(output_dir / 'topics_multiturn.jsonl')\n    },\n    \n    'data_engine': {\n        'generation_system_prompt': 'Her turda baÄŸlamÄ± hatÄ±rlayarak yardÄ±mcÄ± ol ve problemi adÄ±m adÄ±m Ã§Ã¶z.',\n        'instructions': 'KullanÄ±cÄ±yla Ã§ok turlu, doÄŸal konuÅŸmalar yap.',\n        'conversation_type': 'basic',  # Multi-turn konuÅŸmalar iÃ§in\n        'provider': 'openrouter',\n        'model': 'neuralmagic/llama-3.1-nemotron-70b-instruct-hf',\n        'api_key': OPENROUTER_API_KEY,\n        'base_url': 'https://openrouter.ai/api/v1',\n        'temperature': 0.6,\n        'max_retries': 3\n    },\n    \n    'dataset': {\n        'save_as': str(output_dir / 'multiturn_tech_support.jsonl'),\n        'creation': {\n            'num_steps': 10,\n            'batch_size': 2,\n            'sys_msg': True\n        }\n    }\n}\n\nmultiturn_config_path = config_dir / 'multiturn_config.yaml'\nwith open(multiturn_config_path, 'w', encoding='utf-8') as f:\n    yaml.dump(multiturn_config, f, allow_unicode=True)\n\nprint(f\"âœ… Multi-turn yapÄ±landÄ±rmasÄ± kaydedildi: {multiturn_config_path}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate-multiturn"
   },
   "outputs": [],
   "source": "# Multi-turn verisi Ã¼ret\ncmd = [\n    'deepfabric', 'generate',\n    str(multiturn_config_path)\n]\n\nprint(\"ğŸš€ Multi-turn conversation verisi Ã¼retiliyor...\")\nprint(f\"ğŸ“ Config dosyasÄ±: {multiturn_config_path}\")\nprint(\"\\n\" + \"=\"*80 + \"\\n\")\n\nresult = subprocess.run(cmd, capture_output=True, text=True)\nprint(result.stdout)\nif result.stderr:\n    print(\"âš ï¸ UyarÄ±lar:\")\n    print(result.stderr)\n\noutput_file_multiturn = output_dir / 'multiturn_tech_support.jsonl'\nif output_file_multiturn.exists():\n    print(\"\\n\" + \"=\"*80)\n    print(f\"âœ… Multi-turn verisi oluÅŸturuldu: {output_file_multiturn}\")\n    print(f\"ğŸ“¦ Dosya boyutu: {output_file_multiturn.stat().st_size / 1024:.2f} KB\")\nelse:\n    print(\"\\n\" + \"=\"*80)\n    print(f\"âš ï¸ Ã‡Ä±ktÄ± dosyasÄ± bulunamadÄ±: {output_file_multiturn}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-qa"
   },
   "source": [
    "---\n",
    "\n",
    "## 3.4 Question-Answer Pairs (Soru-Cevap Ã‡iftleri)\n",
    "\n",
    "Basit soru-cevap formatÄ±nda eÄŸitim verisi Ã¼retimi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config-qa"
   },
   "outputs": [],
   "source": "# Q&A yapÄ±landÄ±rmasÄ±\nqa_config = {\n    'dataset_system_prompt': 'Sen bir eÄŸitim asistanÄ±sÄ±n. KullanÄ±cÄ± sorularÄ±na net, aÃ§Ä±klayÄ±cÄ± ve doÄŸru cevaplar veriyorsun.',\n    \n    'topic_graph': {\n        'topic_prompt': 'Python ve Makine Ã–ÄŸrenmesi: Temel kavramlar, kÃ¼tÃ¼phaneler, algoritmalar ve uygulamalar',\n        'provider': 'openrouter',\n        'model': 'neuralmagic/llama-3.1-nemotron-70b-instruct-hf',\n        'api_key': OPENROUTER_API_KEY,\n        'base_url': 'https://openrouter.ai/api/v1',\n        'temperature': 0.7,\n        'degree': 4,\n        'depth': 2,\n        'save_as': str(output_dir / 'topics_qa.jsonl')\n    },\n    \n    'data_engine': {\n        'generation_system_prompt': 'Net ve aÃ§Ä±klayÄ±cÄ± cevaplar ver.',\n        'instructions': 'Python ve makine Ã¶ÄŸrenmesi konularÄ±nda eÄŸitim amaÃ§lÄ± Q&A Ã§iftleri oluÅŸtur.',\n        'conversation_type': 'basic',\n        'provider': 'openrouter',\n        'model': 'neuralmagic/llama-3.1-nemotron-70b-instruct-hf',\n        'api_key': OPENROUTER_API_KEY,\n        'base_url': 'https://openrouter.ai/api/v1',\n        'temperature': 0.4,\n        'max_retries': 3\n    },\n    \n    'dataset': {\n        'save_as': str(output_dir / 'qa_python_ml.jsonl'),\n        'creation': {\n            'num_steps': 20,\n            'batch_size': 4,\n            'sys_msg': True\n        }\n    }\n}\n\nqa_config_path = config_dir / 'qa_config.yaml'\nwith open(qa_config_path, 'w', encoding='utf-8') as f:\n    yaml.dump(qa_config, f, allow_unicode=True)\n\nprint(f\"âœ… Q&A yapÄ±landÄ±rmasÄ± kaydedildi: {qa_config_path}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate-qa"
   },
   "outputs": [],
   "source": "# Q&A verisi Ã¼ret\ncmd = [\n    'deepfabric', 'generate',\n    str(qa_config_path)\n]\n\nprint(\"ğŸš€ Q&A verisi Ã¼retiliyor...\")\nprint(f\"ğŸ“ Config dosyasÄ±: {qa_config_path}\")\nprint(\"\\n\" + \"=\"*80 + \"\\n\")\n\nresult = subprocess.run(cmd, capture_output=True, text=True)\nprint(result.stdout)\nif result.stderr:\n    print(\"âš ï¸ UyarÄ±lar:\")\n    print(result.stderr)\n\noutput_file_qa = output_dir / 'qa_python_ml.jsonl'\nif output_file_qa.exists():\n    print(\"\\n\" + \"=\"*80)\n    print(f\"âœ… Q&A verisi oluÅŸturuldu: {output_file_qa}\")\n    print(f\"ğŸ“¦ Dosya boyutu: {output_file_qa.stat().st_size / 1024:.2f} KB\")\nelse:\n    print(\"\\n\" + \"=\"*80)\n    print(f\"âš ï¸ Ã‡Ä±ktÄ± dosyasÄ± bulunamadÄ±: {output_file_qa}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-code"
   },
   "source": [
    "---\n",
    "\n",
    "## 3.5 Code Generation (Kod Ãœretimi)\n",
    "\n",
    "Kod yazma ve aÃ§Ä±klama iÃ§in Ã¶zelleÅŸtirilmiÅŸ veri Ã¼retimi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config-code"
   },
   "outputs": [],
   "source": "# Code Generation yapÄ±landÄ±rmasÄ±\ncode_config = {\n    'dataset_system_prompt': 'Sen uzman bir yazÄ±lÄ±m geliÅŸtiricisisin. KullanÄ±cÄ±larÄ±n isteklerine gÃ¶re temiz, iyi dokÃ¼mante edilmiÅŸ ve best practice kod Ã¶rnekleri yazÄ±yorsun.',\n    \n    'topic_graph': {\n        'topic_prompt': 'Python Kod Ã–rnekleri: Web scraping, veri analizi, API geliÅŸtirme, automation ve utilities',\n        'provider': 'openrouter',\n        'model': 'neuralmagic/llama-3.1-nemotron-70b-instruct-hf',\n        'api_key': OPENROUTER_API_KEY,\n        'base_url': 'https://openrouter.ai/api/v1',\n        'temperature': 0.6,\n        'degree': 3,\n        'depth': 2,\n        'save_as': str(output_dir / 'topics_code.jsonl')\n    },\n    \n    'data_engine': {\n        'generation_system_prompt': 'Her kod bloÄŸunu detaylÄ± aÃ§Ä±kla ve best practice uygula.',\n        'instructions': 'Python kod Ã¶rnekleri ile aÃ§Ä±klamalar yaz.',\n        'conversation_type': 'basic',\n        'provider': 'openrouter',\n        'model': 'neuralmagic/llama-3.1-nemotron-70b-instruct-hf',\n        'api_key': OPENROUTER_API_KEY,\n        'base_url': 'https://openrouter.ai/api/v1',\n        'temperature': 0.3,\n        'max_retries': 3\n    },\n    \n    'dataset': {\n        'save_as': str(output_dir / 'code_generation_python.jsonl'),\n        'creation': {\n            'num_steps': 15,\n            'batch_size': 3,\n            'sys_msg': True\n        }\n    }\n}\n\ncode_config_path = config_dir / 'code_generation_config.yaml'\nwith open(code_config_path, 'w', encoding='utf-8') as f:\n    yaml.dump(code_config, f, allow_unicode=True)\n\nprint(f\"âœ… Code Generation yapÄ±landÄ±rmasÄ± kaydedildi: {code_config_path}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate-code"
   },
   "outputs": [],
   "source": "# Code Generation verisi Ã¼ret\ncmd = [\n    'deepfabric', 'generate',\n    str(code_config_path)\n]\n\nprint(\"ğŸš€ Code Generation verisi Ã¼retiliyor...\")\nprint(f\"ğŸ“ Config dosyasÄ±: {code_config_path}\")\nprint(\"\\n\" + \"=\"*80 + \"\\n\")\n\nresult = subprocess.run(cmd, capture_output=True, text=True)\nprint(result.stdout)\nif result.stderr:\n    print(\"âš ï¸ UyarÄ±lar:\")\n    print(result.stderr)\n\noutput_file_code = output_dir / 'code_generation_python.jsonl'\nif output_file_code.exists():\n    print(\"\\n\" + \"=\"*80)\n    print(f\"âœ… Code Generation verisi oluÅŸturuldu: {output_file_code}\")\n    print(f\"ğŸ“¦ Dosya boyutu: {output_file_code.stat().st_size / 1024:.2f} KB\")\nelse:\n    print(\"\\n\" + \"=\"*80)\n    print(f\"âš ï¸ Ã‡Ä±ktÄ± dosyasÄ± bulunamadÄ±: {output_file_code}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-analysis"
   },
   "source": [
    "---\n",
    "\n",
    "## 4. Veri Analizi ve GÃ¶rselleÅŸtirme\n",
    "\n",
    "Ãœretilen tÃ¼m veri setlerini analiz edelim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "analysis-functions"
   },
   "outputs": [],
   "source": [
    "def load_jsonl(file_path):\n",
    "    \"\"\"JSONL dosyasÄ±nÄ± yÃ¼kle\"\"\"\n",
    "    data = []\n",
    "    if not Path(file_path).exists():\n",
    "        print(f\"âš ï¸ Dosya bulunamadÄ±: {file_path}\")\n",
    "        return data\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():  # BoÅŸ satÄ±rlarÄ± atla\n",
    "                data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "def analyze_dataset(data, dataset_name):\n",
    "    \"\"\"Veri setini detaylÄ± analiz et\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ğŸ“Š Dataset: {dataset_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Toplam Ã¶rnek sayÄ±sÄ±: {len(data)}\")\n",
    "    \n",
    "    if len(data) > 0:\n",
    "        # Ä°lk Ã¶rneÄŸi gÃ¶ster\n",
    "        print(\"\\nğŸ“ Ä°lk Ã–rnek (ilk 500 karakter):\")\n",
    "        print(\"-\" * 80)\n",
    "        print(json.dumps(data[0], indent=2, ensure_ascii=False)[:500] + \"...\")\n",
    "        \n",
    "        # Mesaj istatistikleri\n",
    "        message_lengths = []\n",
    "        total_messages = 0\n",
    "        \n",
    "        for item in data:\n",
    "            if 'messages' in item:\n",
    "                total_messages += len(item['messages'])\n",
    "                for msg in item['messages']:\n",
    "                    if 'content' in msg:\n",
    "                        message_lengths.append(len(msg['content']))\n",
    "        \n",
    "        if message_lengths:\n",
    "            print(f\"\\nğŸ“ˆ Ä°statistikler:\")\n",
    "            print(f\"  â€¢ Toplam mesaj sayÄ±sÄ±: {total_messages}\")\n",
    "            print(f\"  â€¢ Ortalama konuÅŸma uzunluÄŸu: {total_messages/len(data):.1f} mesaj/Ã¶rnek\")\n",
    "            print(f\"  â€¢ Ortalama mesaj uzunluÄŸu: {sum(message_lengths)/len(message_lengths):.0f} karakter\")\n",
    "            print(f\"  â€¢ En kÄ±sa mesaj: {min(message_lengths)} karakter\")\n",
    "            print(f\"  â€¢ En uzun mesaj: {max(message_lengths)} karakter\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "print(\"âœ… Analiz fonksiyonlarÄ± hazÄ±r!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-analyze"
   },
   "outputs": [],
   "source": [
    "# TÃ¼m veri setlerini yÃ¼kle ve analiz et\n",
    "datasets = {\n",
    "    'Chain of Thought': output_file_cot,\n",
    "    'Tool Calling': output_file_tool,\n",
    "    'Multi-Turn': output_file_multiturn,\n",
    "    'Q&A Pairs': output_file_qa,\n",
    "    'Code Generation': output_file_code,\n",
    "}\n",
    "\n",
    "loaded_datasets = {}\n",
    "for name, path in datasets.items():\n",
    "    data = load_jsonl(path)\n",
    "    loaded_datasets[name] = analyze_dataset(data, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize"
   },
   "outputs": [],
   "source": [
    "# Veri setlerini gÃ¶rselleÅŸtir\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Veri seti boyutlarÄ±\n",
    "dataset_sizes = {name: len(data) for name, data in loaded_datasets.items() if len(data) > 0}\n",
    "if dataset_sizes:\n",
    "    colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12', '#9b59b6']\n",
    "    bars = axes[0, 0].bar(range(len(dataset_sizes)), list(dataset_sizes.values()), color=colors[:len(dataset_sizes)])\n",
    "    axes[0, 0].set_xticks(range(len(dataset_sizes)))\n",
    "    axes[0, 0].set_xticklabels(list(dataset_sizes.keys()), rotation=45, ha='right')\n",
    "    axes[0, 0].set_xlabel('Veri Seti', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].set_ylabel('Ã–rnek SayÄ±sÄ±', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].set_title('ğŸ“Š Veri Seti BoyutlarÄ±', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # DeÄŸerleri barlarÄ±n Ã¼zerine ekle\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        axes[0, 0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{int(height)}',\n",
    "                       ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 2. Mesaj uzunluk daÄŸÄ±lÄ±mÄ± (box plot)\n",
    "all_lengths = []\n",
    "labels = []\n",
    "for name, data in loaded_datasets.items():\n",
    "    if len(data) > 0:\n",
    "        lengths = []\n",
    "        for item in data:\n",
    "            if 'messages' in item:\n",
    "                for msg in item['messages']:\n",
    "                    if 'content' in msg:\n",
    "                        lengths.append(len(msg['content']))\n",
    "        if lengths:\n",
    "            all_lengths.append(lengths)\n",
    "            labels.append(name)\n",
    "\n",
    "if all_lengths:\n",
    "    bp = axes[0, 1].boxplot(all_lengths, labels=labels, patch_artist=True)\n",
    "    for patch, color in zip(bp['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "    axes[0, 1].set_xlabel('Veri Seti', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].set_ylabel('Mesaj UzunluÄŸu (karakter)', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].set_title('ğŸ“ Mesaj Uzunluk DaÄŸÄ±lÄ±mÄ±', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. Toplam karakter sayÄ±sÄ±\n",
    "total_chars = {}\n",
    "for name, data in loaded_datasets.items():\n",
    "    if len(data) > 0:\n",
    "        char_count = 0\n",
    "        for item in data:\n",
    "            if 'messages' in item:\n",
    "                for msg in item['messages']:\n",
    "                    if 'content' in msg:\n",
    "                        char_count += len(msg['content'])\n",
    "        total_chars[name] = char_count / 1000  # KB cinsinden\n",
    "\n",
    "if total_chars:\n",
    "    bars = axes[1, 0].bar(range(len(total_chars)), list(total_chars.values()), color=colors[:len(total_chars)])\n",
    "    axes[1, 0].set_xticks(range(len(total_chars)))\n",
    "    axes[1, 0].set_xticklabels(list(total_chars.keys()), rotation=45, ha='right')\n",
    "    axes[1, 0].set_xlabel('Veri Seti', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].set_ylabel('Toplam Boyut (K karakter)', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].set_title('ğŸ’¾ Toplam Veri Boyutu', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        axes[1, 0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{height:.0f}K',\n",
    "                       ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 4. Ortalama konuÅŸma uzunluÄŸu (mesaj sayÄ±sÄ±)\n",
    "avg_conv_length = {}\n",
    "for name, data in loaded_datasets.items():\n",
    "    if len(data) > 0:\n",
    "        total_msgs = 0\n",
    "        for item in data:\n",
    "            if 'messages' in item:\n",
    "                total_msgs += len(item['messages'])\n",
    "        avg_conv_length[name] = total_msgs / len(data)\n",
    "\n",
    "if avg_conv_length:\n",
    "    bars = axes[1, 1].bar(range(len(avg_conv_length)), list(avg_conv_length.values()), color=colors[:len(avg_conv_length)])\n",
    "    axes[1, 1].set_xticks(range(len(avg_conv_length)))\n",
    "    axes[1, 1].set_xticklabels(list(avg_conv_length.keys()), rotation=45, ha='right')\n",
    "    axes[1, 1].set_xlabel('Veri Seti', fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].set_ylabel('Ortalama Mesaj SayÄ±sÄ±', fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].set_title('ğŸ’¬ Ortalama KonuÅŸma UzunluÄŸu', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        axes[1, 1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{height:.1f}',\n",
    "                       ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'dataset_analysis_comprehensive.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ… GÃ¶rselleÅŸtirme kaydedildi: {output_dir / 'dataset_analysis_comprehensive.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-examples"
   },
   "source": [
    "### Ã–rnek KonuÅŸmalarÄ± Ä°nceleme\n",
    "\n",
    "Her veri setinden birer Ã¶rnek gÃ¶relim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "display-examples"
   },
   "outputs": [],
   "source": [
    "def display_example(data, dataset_name, index=0):\n",
    "    \"\"\"Belirli bir Ã¶rneÄŸi gÃ¼zel formatta gÃ¶ster\"\"\"\n",
    "    if len(data) == 0:\n",
    "        print(f\"âš ï¸ {dataset_name} veri seti boÅŸ!\")\n",
    "        return\n",
    "    \n",
    "    if index >= len(data):\n",
    "        index = 0\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ğŸ“– Dataset: {dataset_name} - Ã–rnek #{index + 1}/{len(data)}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    item = data[index]\n",
    "    \n",
    "    if 'messages' in item:\n",
    "        for i, msg in enumerate(item['messages'], 1):\n",
    "            role = msg.get('role', 'unknown').upper()\n",
    "            content = msg.get('content', '')\n",
    "            \n",
    "            # Rol bazlÄ± emoji\n",
    "            emoji = {'SYSTEM': 'âš™ï¸', 'USER': 'ğŸ‘¤', 'ASSISTANT': 'ğŸ¤–'}.get(role, 'ğŸ’¬')\n",
    "            \n",
    "            print(f\"{emoji} [{role}] - Mesaj {i}/{len(item['messages'])}\")\n",
    "            print(f\"{'-'*80}\")\n",
    "            print(content[:500] + (\"...\" if len(content) > 500 else \"\"))  # Ä°lk 500 karakter\n",
    "            print(f\"\\n\")\n",
    "    else:\n",
    "        print(json.dumps(item, indent=2, ensure_ascii=False)[:1000])\n",
    "\n",
    "# Her veri setinden birer Ã¶rnek gÃ¶ster\n",
    "for name, data in loaded_datasets.items():\n",
    "    if len(data) > 0:\n",
    "        display_example(data, name, index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-combine"
   },
   "source": [
    "---\n",
    "\n",
    "## 5. Veri Setlerini BirleÅŸtirme ve Fine-tuning HazÄ±rlÄ±ÄŸÄ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "combine-datasets"
   },
   "outputs": [],
   "source": [
    "# Veri setlerini birleÅŸtir\n",
    "combined_data = []\n",
    "dataset_distribution = {}\n",
    "\n",
    "for name, data in loaded_datasets.items():\n",
    "    if len(data) > 0:\n",
    "        combined_data.extend(data)\n",
    "        dataset_distribution[name] = len(data)\n",
    "\n",
    "# BirleÅŸtirilmiÅŸ veri setini kaydet\n",
    "combined_output = output_dir / 'combined_openrouter_dataset.jsonl'\n",
    "with open(combined_output, 'w', encoding='utf-8') as f:\n",
    "    for item in combined_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"âœ… BirleÅŸtirilmiÅŸ veri seti kaydedildi: {combined_output}\")\n",
    "print(f\"\\nğŸ“Š Toplam Ä°statistikler:\")\n",
    "print(f\"  â€¢ Toplam Ã¶rnek sayÄ±sÄ±: {len(combined_data)}\")\n",
    "print(f\"\\nğŸ“ˆ Veri Seti DaÄŸÄ±lÄ±mÄ±:\")\n",
    "for name, count in dataset_distribution.items():\n",
    "    percentage = (count / len(combined_data)) * 100\n",
    "    print(f\"  â€¢ {name:20s}: {count:3d} Ã¶rnek ({percentage:5.1f}%)\")\n",
    "\n",
    "# Dosya boyutu bilgisi\n",
    "file_size = combined_output.stat().st_size / (1024 * 1024)  # MB\n",
    "print(f\"\\nğŸ’¾ Dosya boyutu: {file_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-with-hf"
   },
   "outputs": [],
   "source": [
    "# HuggingFace datasets kÃ¼tÃ¼phanesi ile yÃ¼kle\n",
    "try:\n",
    "    dataset = load_dataset('json', data_files=str(combined_output))\n",
    "    print(\"âœ… Veri seti HuggingFace Datasets ile baÅŸarÄ±yla yÃ¼klendi!\")\n",
    "    print(f\"\\nğŸ“‹ Veri seti bilgileri:\")\n",
    "    print(dataset)\n",
    "    \n",
    "    # Ã–rnek bir satÄ±r gÃ¶ster\n",
    "    if len(dataset['train']) > 0:\n",
    "        print(\"\\nğŸ“ HuggingFace formatÄ±nda Ã¶rnek bir satÄ±r:\")\n",
    "        print(json.dumps(dataset['train'][0], indent=2, ensure_ascii=False)[:500] + \"...\")\nexcept Exception as e:\n",
    "    print(f\"âš ï¸ Hata: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-finetuning"
   },
   "source": [
    "---\n",
    "\n",
    "## 6. Fine-tuning Ã–rnek Kodu\n",
    "\n",
    "Ãœretilen veri ile model fine-tuning iÃ§in Ã¶rnek kod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "finetuning-code"
   },
   "outputs": [],
   "source": [
    "# Fine-tuning iÃ§in Ã¶rnek kod\n",
    "finetuning_code = '''\n",
    "# ============================================================================\n",
    "# DeepFabric ile Ãœretilen Veri ile Model Fine-tuning\n",
    "# ============================================================================\n",
    "\n",
    "# Gerekli kÃ¼tÃ¼phaneleri kur:\n",
    "# !pip install transformers trl peft accelerate bitsandbytes\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training\n",
    "import torch\n",
    "\n",
    "# ============================================================================\n",
    "# 1. Model ve Tokenizer YÃ¼kleme\n",
    "# ============================================================================\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"  # veya istediÄŸiniz model\n",
    "\n",
    "# 4-bit quantization iÃ§in yapÄ±landÄ±rma (GPU belleÄŸi tasarrufu)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Model yÃ¼kle\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Tokenizer yÃ¼kle\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Modeli QLoRA iÃ§in hazÄ±rla\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# ============================================================================\n",
    "# 2. Veri Setini YÃ¼kle\n",
    "# ============================================================================\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=\"synthetic_datasets/combined_openrouter_dataset.jsonl\"\n",
    ")\n",
    "\n",
    "# Veri setini train/test olarak bÃ¶l (opsiyonel)\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "print(f\"Train set: {len(dataset['train'])} Ã¶rnekler\")\n",
    "print(f\"Test set: {len(dataset['test'])} Ã¶rnekler\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. LoRA YapÄ±landÄ±rmasÄ±\n",
    "# ============================================================================\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16,                    # LoRA rank\n",
    "    lora_alpha=32,          # LoRA alpha\n",
    "    lora_dropout=0.05,      # Dropout\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[         # Hangi modÃ¼llere LoRA uygulanacak\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. Training ArgÃ¼manlarÄ±\n",
    "# ============================================================================\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finetuned_model\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",  # \"wandb\" iÃ§in WandB logging\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. SFTTrainer OluÅŸtur ve EÄŸit\n",
    "# ============================================================================\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=2048,\n",
    "    dataset_text_field=\"messages\",  # DeepFabric formatÄ± iÃ§in\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "# EÄŸitimi baÅŸlat\n",
    "print(\"ğŸš€ Fine-tuning baÅŸlÄ±yor...\")\n",
    "trainer.train()\n",
    "\n",
    "# ============================================================================\n",
    "# 6. Modeli Kaydet\n",
    "# ============================================================================\n",
    "\n",
    "# LoRA adapterlerini kaydet\n",
    "trainer.model.save_pretrained(\"./finetuned_model_lora\")\n",
    "tokenizer.save_pretrained(\"./finetuned_model_lora\")\n",
    "\n",
    "print(\"âœ… Model kaydedildi: ./finetuned_model_lora\")\n",
    "\n",
    "# ============================================================================\n",
    "# 7. Test ve Inference\n",
    "# ============================================================================\n",
    "\n",
    "# Modeli test et\n",
    "def test_model(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Ã–rnek test\n",
    "test_prompt = \"Bir binary search tree'yi Python ile nasÄ±l implement ederim?\"\n",
    "print(f\"\\nTest Sorusu: {test_prompt}\")\n",
    "print(f\"\\nModel CevabÄ±:\\n{test_model(test_prompt)}\")\n",
    "'''\n",
    "\n",
    "# Kodu dosyaya kaydet\n",
    "finetuning_file = output_dir / 'example_finetuning_openrouter.py'\n",
    "with open(finetuning_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(finetuning_code)\n",
    "\n",
    "print(\"ğŸ“ Fine-tuning Ã–rnek Kodu\")\n",
    "print(\"=\"*80)\n",
    "print(finetuning_code[:1000] + \"\\n... (devam ediyor) ...\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"âœ… Tam kod kaydedildi: {finetuning_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-summary"
   },
   "source": [
    "---\n",
    "\n",
    "## 7. Ã–zet ve Sonraki AdÄ±mlar\n",
    "\n",
    "### âœ… Tamamlananlar:\n",
    "\n",
    "1. **DeepFabric Kurulumu**: KÃ¼tÃ¼phane ve baÄŸÄ±mlÄ±lÄ±klar kuruldu\n",
    "2. **OpenRouter Entegrasyonu**: GPT-OSS-20B modeli ile baÄŸlantÄ± saÄŸlandÄ±\n",
    "3. **5 FarklÄ± Sentetik Veri Ãœretimi**:\n",
    "   - âš¡ Chain of Thought (AdÄ±m adÄ±m akÄ±l yÃ¼rÃ¼tme)\n",
    "   - ğŸ› ï¸ Tool Calling (Fonksiyon Ã§aÄŸÄ±rma)\n",
    "   - ğŸ’¬ Multi-Turn Conversations (Ã‡ok turlu konuÅŸmalar)\n",
    "   - â“ Question-Answer Pairs (Soru-cevap Ã§iftleri)\n",
    "   - ğŸ’» Code Generation (Kod Ã¼retimi)\n",
    "4. **Veri Analizi**: DetaylÄ± istatistikler ve gÃ¶rselleÅŸtirmeler\n",
    "5. **Fine-tuning HazÄ±rlÄ±ÄŸÄ±**: HuggingFace uyumlu format ve Ã¶rnek kod\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸš€ Sonraki AdÄ±mlar:\n",
    "\n",
    "#### 1. Daha Fazla Veri Ãœretimi\n",
    "- `num_steps` parametresini artÄ±rarak daha fazla Ã¶rnek Ã¼retin\n",
    "- FarklÄ± `topic_prompt`'lar ile Ã§eÅŸitli konularda veri Ã¼retin\n",
    "- `temperature` deÄŸerlerini ayarlayarak Ã§eÅŸitliliÄŸi kontrol edin\n",
    "\n",
    "#### 2. Veri Kalitesi Ä°yileÅŸtirme\n",
    "- System prompt'larÄ± Ã¶zelleÅŸtirerek format kontrolÃ¼ saÄŸlayÄ±n\n",
    "- FarklÄ± reasoning template'leri deneyin\n",
    "- Manuel inceleme ile kalitesiz Ã¶rnekleri filtreleyin\n",
    "\n",
    "#### 3. Model Fine-tuning\n",
    "- YukarÄ±daki Ã¶rnek kodu kullanarak model eÄŸitin\n",
    "- QLoRA ile hafÄ±za verimliliÄŸi saÄŸlayÄ±n\n",
    "- FarklÄ± hyperparameter'lar deneyin\n",
    "\n",
    "#### 4. DeÄŸerlendirme\n",
    "- Fine-tune edilmiÅŸ modeli test veri seti ile deÄŸerlendirin\n",
    "- Benchmark'larda performans Ã¶lÃ§Ã¼mÃ¼ yapÄ±n\n",
    "- Ä°yileÅŸtirmeler iÃ§in feedback loop oluÅŸturun\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“š FaydalÄ± Kaynaklar:\n",
    "\n",
    "- ğŸ“– [DeepFabric DokÃ¼mantasyonu](https://github.com/lukehinds/deepfabric)\n",
    "- ğŸŒ [OpenRouter API](https://openrouter.ai/docs)\n",
    "- ğŸ¤— [HuggingFace TRL](https://huggingface.co/docs/trl/)\n",
    "- ğŸ”§ [PEFT/LoRA](https://huggingface.co/docs/peft/)\n",
    "- ğŸ’¡ [Fine-tuning Best Practices](https://huggingface.co/blog/llama2)\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’¡ Ä°puÃ§larÄ±:\n",
    "\n",
    "1. **Maliyet YÃ¶netimi**: OpenRouter kullanÄ±rken token kullanÄ±mÄ±nÄ± takip edin\n",
    "2. **Batch Ä°ÅŸleme**: BÃ¼yÃ¼k veri setleri iÃ§in batch_size'Ä± optimize edin\n",
    "3. **Hata YÃ¶netimi**: max_retries parametresini ayarlayÄ±n\n",
    "4. **Veri Ã‡eÅŸitliliÄŸi**: FarklÄ± temperature deÄŸerleri ile deneyler yapÄ±n\n",
    "5. **Kalite KontrolÃ¼**: Ãœretilen verileri manuel olarak inceleyin\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-cleanup"
   },
   "source": [
    "## 8. Dosya Ã–zeti ve Temizlik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "file-summary"
   },
   "outputs": [],
   "source": [
    "# TÃ¼m Ã¼retilen dosyalarÄ± listele ve Ã¶zetle\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“‚ Ãœretilen Dosyalar\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "total_size = 0\n",
    "file_list = []\n",
    "\n",
    "for file_type in ['*.jsonl', '*.yaml', '*.py', '*.png']:\n",
    "    files = sorted(output_dir.glob(file_type)) + sorted(config_dir.glob(file_type))\n",
    "    for file in files:\n",
    "        if file.exists():\n",
    "            size = file.stat().st_size\n",
    "            size_mb = size / (1024 * 1024)\n",
    "            total_size += size\n",
    "            \n",
    "            # Dosya tÃ¼rÃ¼ne gÃ¶re emoji\n",
    "            emoji = {\n",
    "                '.jsonl': 'ğŸ“Š',\n",
    "                '.yaml': 'âš™ï¸',\n",
    "                '.py': 'ğŸ',\n",
    "                '.png': 'ğŸ–¼ï¸'\n",
    "            }.get(file.suffix, 'ğŸ“„')\n",
    "            \n",
    "            file_list.append((file, size_mb, emoji))\n",
    "\n",
    "# Boyuta gÃ¶re sÄ±rala (bÃ¼yÃ¼kten kÃ¼Ã§Ã¼ÄŸe)\n",
    "file_list.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for file, size_mb, emoji in file_list:\n",
    "    print(f\"{emoji} {file.name:50s} - {size_mb:8.2f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"ğŸ“¦ Toplam {len(file_list)} dosya - {total_size / (1024 * 1024):.2f} MB\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Google Drive'a kaydetme talimatlarÄ±\n",
    "print(\"\\nğŸ’¾ Google Drive'a Kaydetme:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"\"\"\n",
    "# Sol panelde 'Files' ikonuna tÄ±klayÄ±n\n",
    "# 'synthetic_datasets' klasÃ¶rÃ¼ne saÄŸ tÄ±klayÄ±n\n",
    "# 'Download' seÃ§eneÄŸini seÃ§in\n",
    "#\n",
    "# Veya Google Drive mount edin:\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# DosyalarÄ± kopyalayÄ±n:\n",
    "!cp -r synthetic_datasets /content/drive/MyDrive/\n",
    "!cp -r deepfabric_configs /content/drive/MyDrive/\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "footer"
   },
   "source": [
    "---\n",
    "\n",
    "## ğŸ‰ Tebrikler!\n",
    "\n",
    "DeepFabric ile OpenRouter GPT-OSS-20B modeli kullanarak baÅŸarÄ±yla sentetik veri Ã¼rettiniz!\n",
    "\n",
    "Bu notebook'u geliÅŸtirmek iÃ§in Ã¶nerileriniz varsa lÃ¼tfen paylaÅŸÄ±n.\n",
    "\n",
    "---\n",
    "\n",
    "**HazÄ±rlayan**: DeepFabric + OpenRouter Integration  \n",
    "**Tarih**: 2025  \n",
    "**Versiyon**: 1.0  \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}