{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepFabric ile Sentetik Veri Ãœretimi - KapsamlÄ± Rehber\n",
    "\n",
    "Bu notebook, DeepFabric kÃ¼tÃ¼phanesini kullanarak lokal LLM modelleri ile sentetik veri Ã¼retimini adÄ±m adÄ±m gÃ¶stermektedir.\n",
    "\n",
    "## DeepFabric Nedir?\n",
    "\n",
    "DeepFabric, yapay zeka modelleri iÃ§in yÃ¼ksek kaliteli eÄŸitim verileri oluÅŸturmak Ã¼zere tasarlanmÄ±ÅŸ bir sentetik veri Ã¼retim Ã§erÃ§evesidir. NVIDIA, Google ve GitHub mÃ¼hendisleri tarafÄ±ndan geliÅŸtirilmiÅŸtir.\n",
    "\n",
    "### Temel Ã–zellikler:\n",
    "\n",
    "- **HiyerarÅŸik Konu AÄŸacÄ±**: Bir ana konudan alt konular oluÅŸturur ve bunlardan sentetik veriler Ã¼retir\n",
    "- **Ã‡oklu LLM DesteÄŸi**: OpenAI, Anthropic, Google, Ollama ve OpenAI-uyumlu API'ler\n",
    "- **FarklÄ± Veri Tipleri**: Soru-cevap, konuÅŸmalar, araÃ§ Ã§aÄŸrÄ±larÄ±, akÄ±l yÃ¼rÃ¼tme zincirleri\n",
    "- **Esnek Formatlar**: TRL, ChatML, Alpaca, Ã¶zel formatlar\n",
    "- **Lokal ve Bulut**: Hem lokal hem de bulut tabanlÄ± modeller\n",
    "\n",
    "### NasÄ±l Ã‡alÄ±ÅŸÄ±r?\n",
    "\n",
    "1. **Konu AÄŸacÄ± OluÅŸturma**: VerdiÄŸiniz ana konudan hiyerarÅŸik alt konular Ã¼retir\n",
    "2. **Veri Ãœretimi**: Her konu iÃ§in soru-cevap Ã§iftleri, konuÅŸmalar veya Ã¶rnekler oluÅŸturur\n",
    "3. **Formatlama**: Ãœretilen veriyi model eÄŸitimi iÃ§in uygun formata dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Kurulum\n",
    "\n",
    "DeepFabric'i pip ile kuruyoruz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepFabric kÃ¼tÃ¼phanesini kur\n",
    "!pip install deepfabric -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerekli kÃ¼tÃ¼phaneleri import et\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Lokal LLM API KonfigÃ¼rasyonu\n",
    "\n",
    "### SeÃ§enek 1: OpenAI-Uyumlu API (vLLM, text-generation-webui, LM Studio vb.)\n",
    "\n",
    "EÄŸer lokal Llama modelinizi OpenAI-uyumlu bir API ile Ã§alÄ±ÅŸtÄ±rÄ±yorsanÄ±z (Ã¶rneÄŸin vLLM, text-generation-webui, LM Studio):\n",
    "\n",
    "```python\n",
    "# API bilgilerinizi buraya girin\n",
    "os.environ['OPENAI_API_KEY'] = 'dummy-key'  # Lokal API iÃ§in genelde gerekli deÄŸil ama zorunlu\n",
    "os.environ['OPENAI_BASE_URL'] = 'http://localhost:8000/v1'  # API URL'iniz\n",
    "```\n",
    "\n",
    "### SeÃ§enek 2: Ollama ile\n",
    "\n",
    "EÄŸer Ollama kullanÄ±yorsanÄ±z:\n",
    "```bash\n",
    "# Terminal'de Ollama'yÄ± baÅŸlatÄ±n\n",
    "ollama serve\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API KONFÄ°GÃœRASYONU - KENDÄ° BÄ°LGÄ°LERÄ°NÄ°ZÄ° GÄ°RÄ°N\n",
    "\n",
    "# SeÃ§enek 1: OpenAI-uyumlu lokal API\n",
    "os.environ['OPENAI_API_KEY'] = 'sk-dummy-key'  # Lokal iÃ§in dummy key\n",
    "os.environ['OPENAI_BASE_URL'] = 'http://localhost:8000/v1'  # Kendi API URL'iniz\n",
    "\n",
    "# Model adÄ± - API'nizde kullandÄ±ÄŸÄ±nÄ±z model adÄ±\n",
    "LOCAL_MODEL_NAME = 'llama-3.1-8b'  # Kendi model adÄ±nÄ±zÄ± girin\n",
    "\n",
    "# EÄŸer Ollama kullanÄ±yorsanÄ±z:\n",
    "# PROVIDER = 'ollama'\n",
    "# LOCAL_MODEL_NAME = 'llama3.1:8b'\n",
    "\n",
    "PROVIDER = 'openai'  # OpenAI-uyumlu API iÃ§in 'openai', Ollama iÃ§in 'ollama'\n",
    "\n",
    "print(f\"âœ“ API KonfigÃ¼rasyonu tamamlandÄ±\")\n",
    "print(f\"  Provider: {PROVIDER}\")\n",
    "print(f\"  Model: {LOCAL_MODEL_NAME}\")\n",
    "print(f\"  Base URL: {os.environ.get('OPENAI_BASE_URL', 'default')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basit Ã–rnek: CLI ile Sentetik Veri Ãœretimi\n",
    "\n",
    "DeepFabric'in en basit kullanÄ±mÄ± CLI (komut satÄ±rÄ±) Ã¼zerinden yapÄ±lÄ±r:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basit bir sentetik veri seti oluÅŸtur\n",
    "# Bu komut:\n",
    "# 1. \"Python Programlama\" konusundan konu aÄŸacÄ± oluÅŸturur (depth=2, degree=2)\n",
    "# 2. Her konu iÃ§in 8 veri Ã¶rneÄŸi Ã¼retir (num-steps=8)\n",
    "# 3. SonuÃ§larÄ± 'dataset_basit.jsonl' dosyasÄ±na kaydeder\n",
    "\n",
    "!deepfabric generate \\\n",
    "  --mode tree \\\n",
    "  --provider {PROVIDER} \\\n",
    "  --model {LOCAL_MODEL_NAME} \\\n",
    "  --depth 2 \\\n",
    "  --degree 2 \\\n",
    "  --num-steps 8 \\\n",
    "  --topic-prompt \"Python Programlama Temelleri\" \\\n",
    "  --dataset-save-as dataset_basit.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OluÅŸturulan veriyi inceleyelim\n",
    "if os.path.exists('dataset_basit.jsonl'):\n",
    "    with open('dataset_basit.jsonl', 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i < 2:  # Ä°lk 2 Ã¶rneÄŸi gÃ¶ster\n",
    "                data = json.loads(line)\n",
    "                print(f\"\\n--- Ã–rnek {i+1} ---\")\n",
    "                print(json.dumps(data, indent=2, ensure_ascii=False))\n",
    "else:\n",
    "    print(\"âŒ Veri seti henÃ¼z oluÅŸturulmadÄ±\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. YAML KonfigÃ¼rasyonu ile Ä°leri Seviye KullanÄ±m\n",
    "\n",
    "Daha fazla kontrol iÃ§in YAML konfigÃ¼rasyon dosyasÄ± kullanabilirsiniz. Bu yÃ¶ntemle:\n",
    "- Prompt'larÄ± Ã¶zelleÅŸtirebilirsiniz\n",
    "- FarklÄ± parametreler ayarlayabilirsiniz\n",
    "- Ã‡Ä±ktÄ± formatÄ±nÄ± belirleyebilirsiniz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YAML konfigÃ¼rasyon dosyasÄ± oluÅŸtur\n",
    "config_yaml = f\"\"\"\n",
    "# Sistem prompt'u - veri setindeki genel davranÄ±ÅŸÄ± belirler\n",
    "dataset_system_prompt: |\n",
    "  Sen yardÄ±mcÄ± bir TÃ¼rkÃ§e AI asistanÄ±sÄ±n. KullanÄ±cÄ±lara detaylÄ± ve \n",
    "  anlaÅŸÄ±lÄ±r ÅŸekilde yanÄ±t veriyorsun.\n",
    "\n",
    "# Konu aÄŸacÄ± konfigÃ¼rasyonu\n",
    "topic_tree:\n",
    "  # Ana konu - buradan alt konular tÃ¼retilecek\n",
    "  topic_prompt: \"Makine Ã–ÄŸrenmesi ve Yapay Zeka\"\n",
    "  \n",
    "  # LLM provider ayarlarÄ±\n",
    "  provider: \"{PROVIDER}\"\n",
    "  model: \"{LOCAL_MODEL_NAME}\"\n",
    "  temperature: 0.7  # YaratÄ±cÄ±lÄ±k seviyesi (0.0-1.0)\n",
    "  \n",
    "  # AÄŸaÃ§ yapÄ±sÄ±\n",
    "  degree: 3  # Her konudan kaÃ§ alt konu tÃ¼retilebilir\n",
    "  depth: 2   # Konu aÄŸacÄ±nÄ±n derinliÄŸi\n",
    "\n",
    "# Veri Ã¼retimi ayarlarÄ±\n",
    "data_engine:\n",
    "  # Veri Ã¼retim prompt'u - Ã¼retilecek verilerin stilini belirler\n",
    "  generation_system_prompt: |\n",
    "    Sen bir yapay zeka eÄŸitmenisin. Ã–ÄŸrencilere kavramlarÄ± aÃ§Ä±k ve \n",
    "    anlaÅŸÄ±lÄ±r ÅŸekilde Ã¶ÄŸretiyorsun. Her soruya detaylÄ±, Ã¶rnekli ve \n",
    "    adÄ±m adÄ±m yanÄ±tlar veriyorsun.\n",
    "  \n",
    "  temperature: 0.6\n",
    "  provider: \"{PROVIDER}\"\n",
    "  model: \"{LOCAL_MODEL_NAME}\"\n",
    "\n",
    "# Veri seti oluÅŸturma parametreleri\n",
    "dataset:\n",
    "  creation:\n",
    "    num_steps: 12      # Toplam kaÃ§ veri Ã¶rneÄŸi Ã¼retilecek\n",
    "    batch_size: 2      # AynÄ± anda kaÃ§ Ã¶rnek Ã¼retilecek\n",
    "    sys_msg: true      # Sistem mesajÄ± dahil edilsin mi\n",
    "  \n",
    "  # Ã‡Ä±ktÄ± formatÄ± (opsiyonel)\n",
    "  # builtin://trl_sft_tools  - HuggingFace TRL iÃ§in\n",
    "  # builtin://chatml.py      - ChatML formatÄ±\n",
    "  # builtin://alpaca.py      - Alpaca formatÄ±\n",
    "  template: \"builtin://chatml.py\"\n",
    "\"\"\"\n",
    "\n",
    "# Config dosyasÄ±nÄ± kaydet\n",
    "with open('deepfabric_config.yaml', 'w', encoding='utf-8') as f:\n",
    "    f.write(config_yaml)\n",
    "\n",
    "print(\"âœ“ KonfigÃ¼rasyon dosyasÄ± oluÅŸturuldu: deepfabric_config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YAML konfigÃ¼rasyonu ile veri Ã¼ret\n",
    "!deepfabric generate \\\n",
    "  --config deepfabric_config.yaml \\\n",
    "  --dataset-save-as dataset_yaml.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. TXT DosyasÄ±ndan Soru-Cevap Ãœretimi\n",
    "\n",
    "DeepFabric doÄŸrudan txt dosyasÄ±ndan veri Ã¼retmez, ancak txt iÃ§eriÄŸini **konu (topic)** olarak kullanabilirsiniz.\n",
    "\n",
    "### YaklaÅŸÄ±m:\n",
    "1. Txt dosyanÄ±zÄ± okuyun\n",
    "2. Ä°Ã§eriÄŸi topic prompt olarak kullanÄ±n\n",
    "3. DeepFabric bu konudan soru-cevaplar Ã¼retir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ã–rnek bir txt dosyasÄ± oluÅŸtur\n",
    "ornek_metin = \"\"\"\n",
    "Python Veri YapÄ±larÄ±\n",
    "\n",
    "Python'da temel veri yapÄ±larÄ± ÅŸunlardÄ±r:\n",
    "\n",
    "1. Liste (List): SÄ±ralÄ±, deÄŸiÅŸtirilebilir koleksiyon. Ã–rnek: [1, 2, 3, 4]\n",
    "2. Demet (Tuple): SÄ±ralÄ±, deÄŸiÅŸtirilemez koleksiyon. Ã–rnek: (1, 2, 3)\n",
    "3. SÃ¶zlÃ¼k (Dictionary): Anahtar-deÄŸer Ã§iftleri. Ã–rnek: {'ad': 'Ali', 'yaÅŸ': 25}\n",
    "4. KÃ¼me (Set): SÄ±rasÄ±z, benzersiz elemanlar. Ã–rnek: {1, 2, 3, 4}\n",
    "\n",
    "Listeler en Ã§ok kullanÄ±lan veri yapÄ±sÄ±dÄ±r Ã§Ã¼nkÃ¼ esnektir ve birÃ§ok metoda sahiptir.\n",
    "\"\"\"\n",
    "\n",
    "with open('ornek_konu.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(ornek_metin)\n",
    "\n",
    "print(\"âœ“ Ã–rnek txt dosyasÄ± oluÅŸturuldu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Txt dosyasÄ±nÄ± oku ve iÃ§eriÄŸi topic olarak kullan\n",
    "with open('ornek_konu.txt', 'r', encoding='utf-8') as f:\n",
    "    txt_icerik = f.read()\n",
    "\n",
    "# Ã–zel prompt ile YAML oluÅŸtur\n",
    "txt_config = f\"\"\"\n",
    "dataset_system_prompt: |\n",
    "  Sen bir Python programlama eÄŸitmenisin. Ã–ÄŸrencilere veri yapÄ±larÄ±nÄ± \n",
    "  aÃ§Ä±k ve Ã¶rneklerle Ã¶ÄŸretiyorsun.\n",
    "\n",
    "topic_tree:\n",
    "  # Txt dosyasÄ±nÄ±n iÃ§eriÄŸini konu olarak kullanÄ±yoruz\n",
    "  topic_prompt: |\n",
    "    {txt_icerik}\n",
    "    \n",
    "    YukarÄ±daki konular hakkÄ±nda detaylÄ± alt baÅŸlÄ±klar oluÅŸtur.\n",
    "  \n",
    "  provider: \"{PROVIDER}\"\n",
    "  model: \"{LOCAL_MODEL_NAME}\"\n",
    "  temperature: 0.7\n",
    "  degree: 4\n",
    "  depth: 2\n",
    "\n",
    "data_engine:\n",
    "  generation_system_prompt: |\n",
    "    Python veri yapÄ±larÄ± hakkÄ±nda soru-cevap Ã§iftleri oluÅŸtur.\n",
    "    Her cevap:\n",
    "    - AÃ§Ä±k ve anlaÅŸÄ±lÄ±r olmalÄ±\n",
    "    - Kod Ã¶rnekleri iÃ§ermeli\n",
    "    - Pratik kullanÄ±m senaryolarÄ± gÃ¶stermeli\n",
    "  \n",
    "  temperature: 0.5\n",
    "  provider: \"{PROVIDER}\"\n",
    "  model: \"{LOCAL_MODEL_NAME}\"\n",
    "\n",
    "dataset:\n",
    "  creation:\n",
    "    num_steps: 16\n",
    "    batch_size: 2\n",
    "    sys_msg: true\n",
    "  template: \"builtin://chatml.py\"\n",
    "\"\"\"\n",
    "\n",
    "with open('txt_based_config.yaml', 'w', encoding='utf-8') as f:\n",
    "    f.write(txt_config)\n",
    "\n",
    "print(\"âœ“ Txt tabanlÄ± konfigÃ¼rasyon oluÅŸturuldu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Txt iÃ§eriÄŸinden soru-cevap Ã¼ret\n",
    "!deepfabric generate \\\n",
    "  --config txt_based_config.yaml \\\n",
    "  --dataset-save-as dataset_from_txt.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ãœretilen soru-cevaplarÄ± incele\n",
    "if os.path.exists('dataset_from_txt.jsonl'):\n",
    "    print(\"\\nğŸ“Š TXT DOSYASINDAN ÃœRETÄ°LEN SORU-CEVAPLAR:\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    with open('dataset_from_txt.jsonl', 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i < 3:  # Ä°lk 3 Ã¶rneÄŸi gÃ¶ster\n",
    "                data = json.loads(line)\n",
    "                print(f\"\\nğŸ”¹ Ã–rnek {i+1}:\")\n",
    "                print(\"-\" * 80)\n",
    "                \n",
    "                # Messages varsa gÃ¼zel formatlayalÄ±m\n",
    "                if 'messages' in data:\n",
    "                    for msg in data['messages']:\n",
    "                        role = msg.get('role', 'unknown').upper()\n",
    "                        content = msg.get('content', '')\n",
    "                        print(f\"\\n[{role}]\")\n",
    "                        print(content[:500])  # Ä°lk 500 karakter\n",
    "                        if len(content) > 500:\n",
    "                            print(\"... (devamÄ± var)\")\n",
    "                else:\n",
    "                    print(json.dumps(data, indent=2, ensure_ascii=False)[:1000])\n",
    "                \n",
    "                print(\"\\n\" + \"=\" * 80)\n",
    "else:\n",
    "    print(\"âŒ Veri seti henÃ¼z oluÅŸturulmadÄ±\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. FarklÄ± Veri FormatlarÄ±\n",
    "\n",
    "DeepFabric, Ã¼retilen veriyi farklÄ± formatlara dÃ¶nÃ¼ÅŸtÃ¼rebilir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mevcut bir veri setini farklÄ± formata Ã§evir\n",
    "# ChatML formatÄ±na Ã§evir\n",
    "!deepfabric format \\\n",
    "  --dataset dataset_from_txt.jsonl \\\n",
    "  --format builtin://chatml.py \\\n",
    "  --output dataset_chatml.jsonl\n",
    "\n",
    "print(\"\\nâœ“ ChatML formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼ldÃ¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpaca formatÄ±na Ã§evir\n",
    "!deepfabric format \\\n",
    "  --dataset dataset_from_txt.jsonl \\\n",
    "  --format builtin://alpaca.py \\\n",
    "  --output dataset_alpaca.jsonl\n",
    "\n",
    "print(\"\\nâœ“ Alpaca formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼ldÃ¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Parametrelerin AnlamÄ± ve Ã–zelleÅŸtirme\n",
    "\n",
    "### Konu AÄŸacÄ± Parametreleri:\n",
    "\n",
    "- **`depth`**: Konu aÄŸacÄ±nÄ±n derinliÄŸi. Ã–rnek: `depth=3` â†’ Ana konu â†’ Alt konu â†’ Alt-alt konu\n",
    "- **`degree`**: Her konudan kaÃ§ alt konu Ã¼retilir. Ã–rnek: `degree=4` â†’ Her konudan 4 alt konu\n",
    "- **`topic_prompt`**: Ana konu. Buradan tÃ¼m alt konular tÃ¼retilir\n",
    "\n",
    "**Toplam konu sayÄ±sÄ±**: `degree^depth` (Ã¶rn: 3^2 = 9 konu)\n",
    "\n",
    "### Veri Ãœretim Parametreleri:\n",
    "\n",
    "- **`num_steps`**: Toplam kaÃ§ veri Ã¶rneÄŸi Ã¼retilecek\n",
    "- **`batch_size`**: AynÄ± anda kaÃ§ Ã¶rnek Ã¼retilecek (paralel iÅŸlem)\n",
    "- **`temperature`**: YaratÄ±cÄ±lÄ±k seviyesi (0.0 = deterministik, 1.0 = yaratÄ±cÄ±)\n",
    "- **`sys_msg`**: Sistem mesajÄ± dahil edilsin mi?\n",
    "\n",
    "### Prompt'lar:\n",
    "\n",
    "- **`dataset_system_prompt`**: Genel sistem davranÄ±ÅŸÄ±\n",
    "- **`generation_system_prompt`**: Veri Ã¼retim stili\n",
    "- **`topic_prompt`**: Ana konu (txt dosyanÄ±zÄ±n iÃ§eriÄŸi de olabilir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ã–zel Senaryolar\n",
    "\n",
    "### Senaryo 1: Kod Ã–rnekleri Ãœretimi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kod_config = f\"\"\"\n",
    "dataset_system_prompt: |\n",
    "  Sen bir programlama eÄŸitmenisin. Her yanÄ±tta Ã§alÄ±ÅŸan kod Ã¶rnekleri veriyorsun.\n",
    "\n",
    "topic_tree:\n",
    "  topic_prompt: |\n",
    "    Python ile veri analizi: Pandas, NumPy ve Matplotlib kullanÄ±mÄ±\n",
    "  \n",
    "  provider: \"{PROVIDER}\"\n",
    "  model: \"{LOCAL_MODEL_NAME}\"\n",
    "  temperature: 0.6\n",
    "  degree: 3\n",
    "  depth: 2\n",
    "\n",
    "data_engine:\n",
    "  generation_system_prompt: |\n",
    "    Veri analizi hakkÄ±nda soru-cevap Ã¼ret. Her cevap:\n",
    "    1. Konsepti aÃ§Ä±kla\n",
    "    2. Kod Ã¶rneÄŸi ver (Ã§alÄ±ÅŸÄ±r kod)\n",
    "    3. Ã‡Ä±ktÄ±yÄ± gÃ¶ster\n",
    "    4. KullanÄ±m alanlarÄ±nÄ± aÃ§Ä±kla\n",
    "  \n",
    "  temperature: 0.4\n",
    "  provider: \"{PROVIDER}\"\n",
    "  model: \"{LOCAL_MODEL_NAME}\"\n",
    "\n",
    "dataset:\n",
    "  creation:\n",
    "    num_steps: 20\n",
    "    batch_size: 2\n",
    "    sys_msg: true\n",
    "\"\"\"\n",
    "\n",
    "with open('kod_ornekleri_config.yaml', 'w', encoding='utf-8') as f:\n",
    "    f.write(kod_config)\n",
    "\n",
    "print(\"âœ“ Kod Ã¶rnekleri konfigÃ¼rasyonu hazÄ±r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Senaryo 2: Ã‡ok Turlu KonuÅŸmalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "konusma_config = f\"\"\"\n",
    "dataset_system_prompt: |\n",
    "  Sen yardÄ±mcÄ± bir mÃ¼ÅŸteri hizmetleri temsilcisisin. Nazik ve Ã§Ã¶zÃ¼m odaklÄ±sÄ±n.\n",
    "\n",
    "topic_tree:\n",
    "  topic_prompt: |\n",
    "    E-ticaret mÃ¼ÅŸteri hizmetleri: SipariÅŸ sorunlarÄ±, iade iÅŸlemleri, \n",
    "    Ã¼rÃ¼n bilgileri, teslimat takibi\n",
    "  \n",
    "  provider: \"{PROVIDER}\"\n",
    "  model: \"{LOCAL_MODEL_NAME}\"\n",
    "  temperature: 0.7\n",
    "  degree: 4\n",
    "  depth: 2\n",
    "\n",
    "data_engine:\n",
    "  generation_system_prompt: |\n",
    "    MÃ¼ÅŸteri-temsilci konuÅŸmalarÄ± oluÅŸtur. Her konuÅŸma:\n",
    "    - GerÃ§ekÃ§i olmalÄ±\n",
    "    - 3-5 mesaj alÄ±ÅŸveriÅŸi iÃ§ermeli\n",
    "    - Sorunu Ã§Ã¶zmeli\n",
    "    - Profesyonel ama samimi olmalÄ±\n",
    "  \n",
    "  temperature: 0.7\n",
    "  provider: \"{PROVIDER}\"\n",
    "  model: \"{LOCAL_MODEL_NAME}\"\n",
    "\n",
    "dataset:\n",
    "  creation:\n",
    "    num_steps: 15\n",
    "    batch_size: 1\n",
    "    sys_msg: true\n",
    "  template: \"builtin://chatml.py\"\n",
    "\"\"\"\n",
    "\n",
    "with open('konusma_config.yaml', 'w', encoding='utf-8') as f:\n",
    "    f.write(konusma_config)\n",
    "\n",
    "print(\"âœ“ KonuÅŸma konfigÃ¼rasyonu hazÄ±r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Veri Setini Ä°nceleme ve Ä°statistikler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def veri_seti_analizi(dosya_yolu):\n",
    "    \"\"\"\n",
    "    JSONL formatÄ±ndaki veri setini analiz eder\n",
    "    \"\"\"\n",
    "    if not os.path.exists(dosya_yolu):\n",
    "        print(f\"âŒ Dosya bulunamadÄ±: {dosya_yolu}\")\n",
    "        return\n",
    "    \n",
    "    veriler = []\n",
    "    with open(dosya_yolu, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            veriler.append(json.loads(line))\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Veri Seti Analizi: {dosya_yolu}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Toplam Ã¶rnek sayÄ±sÄ±: {len(veriler)}\")\n",
    "    \n",
    "    # Mesaj sayÄ±larÄ±\n",
    "    if veriler and 'messages' in veriler[0]:\n",
    "        mesaj_sayilari = [len(v['messages']) for v in veriler if 'messages' in v]\n",
    "        print(f\"Ortalama mesaj sayÄ±sÄ±: {sum(mesaj_sayilari) / len(mesaj_sayilari):.1f}\")\n",
    "        print(f\"Min mesaj sayÄ±sÄ±: {min(mesaj_sayilari)}\")\n",
    "        print(f\"Max mesaj sayÄ±sÄ±: {max(mesaj_sayilari)}\")\n",
    "    \n",
    "    # Alan isimleri\n",
    "    if veriler:\n",
    "        print(f\"\\nVeri alanlarÄ±: {list(veriler[0].keys())}\")\n",
    "    \n",
    "    # Ä°lk Ã¶rneÄŸi gÃ¶ster\n",
    "    if veriler:\n",
    "        print(\"\\nğŸ“ Ã–rnek veri:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(json.dumps(veriler[0], indent=2, ensure_ascii=False)[:1500])\n",
    "        print(\"...\")\n",
    "    \n",
    "    return veriler\n",
    "\n",
    "# Ã–rnek kullanÄ±m\n",
    "# veri_seti_analizi('dataset_from_txt.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model EÄŸitimi iÃ§in Veri HazÄ±rlÄ±ÄŸÄ±\n",
    "\n",
    "Ãœretilen veriyi model eÄŸitimi iÃ§in kullanmak:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace format iÃ§in\n",
    "!deepfabric format \\\n",
    "  --dataset dataset_from_txt.jsonl \\\n",
    "  --format builtin://trl_sft_tools \\\n",
    "  --output dataset_hf_ready.jsonl\n",
    "\n",
    "print(\"\\nâœ“ HuggingFace TRL iÃ§in hazÄ±r veri seti oluÅŸturuldu\")\n",
    "print(\"\\nBu veri setini ÅŸu ÅŸekilde kullanabilirsiniz:\")\n",
    "print(\"\"\"\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "\n",
    "dataset = load_dataset('json', data_files='dataset_hf_ready.jsonl')\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset['train'],\n",
    "    ...\n",
    ")\n",
    "trainer.train()\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Ä°puÃ§larÄ± ve En Ä°yi Pratikler\n",
    "\n",
    "### ğŸ¯ Kaliteli Veri Ãœretimi Ä°Ã§in:\n",
    "\n",
    "1. **Spesifik Konular SeÃ§in**: \n",
    "   - âŒ \"Programlama\" yerine\n",
    "   - âœ… \"Python'da list comprehension kullanÄ±mÄ± ve performans optimizasyonu\"\n",
    "\n",
    "2. **Prompt'larÄ± Ã–zelleÅŸtirin**:\n",
    "   - `generation_system_prompt` ile veri stilini belirleyin\n",
    "   - Ã–rnekler, aÃ§Ä±klamalar, kod bloklarÄ± gibi isteklerinizi ekleyin\n",
    "\n",
    "3. **Temperature AyarlarÄ±**:\n",
    "   - Konu oluÅŸturma: 0.7-0.8 (daha yaratÄ±cÄ±)\n",
    "   - Veri Ã¼retimi: 0.4-0.6 (daha tutarlÄ±)\n",
    "   - Teknik iÃ§erik: 0.3-0.5 (daha deterministik)\n",
    "\n",
    "4. **KÃ¼Ã§Ã¼k BaÅŸlayÄ±n**:\n",
    "   - Ä°lk denemede `num_steps=5-10` kullanÄ±n\n",
    "   - Kaliteyi kontrol edin\n",
    "   - Sonra artÄ±rÄ±n\n",
    "\n",
    "5. **Batch Size**:\n",
    "   - Lokal API: 1-2 (kaynak sÄ±nÄ±rlÄ±)\n",
    "   - GÃ¼Ã§lÃ¼ API: 4-8 (daha hÄ±zlÄ±)\n",
    "\n",
    "### ğŸ”§ Sorun Giderme:\n",
    "\n",
    "**API BaÄŸlantÄ± HatasÄ±:**\n",
    "```python\n",
    "# API'nizin Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olun\n",
    "import requests\n",
    "response = requests.get(os.environ['OPENAI_BASE_URL'].replace('/v1', '/health'))\n",
    "print(response.status_code)  # 200 olmalÄ±\n",
    "```\n",
    "\n",
    "**DÃ¼ÅŸÃ¼k Kaliteli Veri:**\n",
    "- `generation_system_prompt` daha detaylÄ± yapÄ±n\n",
    "- Temperature'Ä± dÃ¼ÅŸÃ¼rÃ¼n\n",
    "- Daha gÃ¼Ã§lÃ¼ bir model deneyin\n",
    "\n",
    "**YavaÅŸ Ãœretim:**\n",
    "- `batch_size` artÄ±rÄ±n\n",
    "- Daha kÃ¼Ã§Ã¼k bir model kullanÄ±n\n",
    "- `depth` ve `degree` azaltÄ±n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Ã–zet ve Sonraki AdÄ±mlar\n",
    "\n",
    "### ğŸ“ Ã–ÄŸrendikleriniz:\n",
    "\n",
    "âœ… DeepFabric'i kurdunuz ve yapÄ±landÄ±rdÄ±nÄ±z  \n",
    "âœ… Lokal LLM API ile baÄŸlandÄ±nÄ±z  \n",
    "âœ… Basit ve geliÅŸmiÅŸ veri setleri oluÅŸturdunuz  \n",
    "âœ… Txt dosyasÄ±ndan soru-cevap Ã¼rettiniz  \n",
    "âœ… FarklÄ± formatlarÄ± Ã¶ÄŸrendiniz  \n",
    "âœ… Prompt'larÄ± Ã¶zelleÅŸtirdiniz  \n",
    "\n",
    "### ğŸš€ Sonraki AdÄ±mlar:\n",
    "\n",
    "1. **Kendi Verilerinizi OluÅŸturun**: \n",
    "   - Kendi txt dosyalarÄ±nÄ±zÄ± kullanÄ±n\n",
    "   - Domain-spesifik konular seÃ§in\n",
    "\n",
    "2. **Kaliteyi ArtÄ±rÄ±n**:\n",
    "   - FarklÄ± prompt'lar deneyin\n",
    "   - Temperature ayarlarÄ±nÄ± optimize edin\n",
    "   - Ãœretilen veriyi gÃ¶zden geÃ§irin\n",
    "\n",
    "3. **Model EÄŸitimi**:\n",
    "   - ÃœrettiÄŸiniz veri ile modelinizi fine-tune edin\n",
    "   - QLoRA veya LoRA kullanÄ±n\n",
    "   - SonuÃ§larÄ± deÄŸerlendirin\n",
    "\n",
    "4. **Ä°leri Seviye**:\n",
    "   - Ã–zel formatlar yazÄ±n\n",
    "   - AraÃ§ Ã§aÄŸrÄ±larÄ± (tool calling) ekleyin\n",
    "   - Multi-turn konuÅŸmalar oluÅŸturun\n",
    "\n",
    "### ğŸ“š Kaynaklar:\n",
    "\n",
    "- GitHub: https://github.com/lukehinds/deepfabric\n",
    "- DokÃ¼mantasyon: Repository README\n",
    "- Ã–rnekler: `examples/` klasÃ¶rÃ¼\n",
    "\n",
    "---\n",
    "\n",
    "**Ä°yi Ã§alÄ±ÅŸmalar! ğŸ‰**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
