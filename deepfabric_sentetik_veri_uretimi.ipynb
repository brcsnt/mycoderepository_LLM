{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepFabric ile Sentetik Veri Ãœretimi - KapsamlÄ± Rehber\n",
    "\n",
    "Bu notebook, DeepFabric kÃ¼tÃ¼phanesini kullanarak lokal LLM modelleri ile sentetik veri Ã¼retimini adÄ±m adÄ±m gÃ¶stermektedir.\n",
    "\n",
    "## DeepFabric Nedir?\n",
    "\n",
    "DeepFabric, yapay zeka modelleri iÃ§in yÃ¼ksek kaliteli eÄŸitim verileri oluÅŸturmak Ã¼zere tasarlanmÄ±ÅŸ bir sentetik veri Ã¼retim Ã§erÃ§evesidir. NVIDIA, Google ve GitHub mÃ¼hendisleri tarafÄ±ndan geliÅŸtirilmiÅŸtir.\n",
    "\n",
    "### Temel Ã–zellikler:\n",
    "\n",
    "- **HiyerarÅŸik Konu AÄŸacÄ±**: Bir ana konudan alt konular oluÅŸturur ve bunlardan sentetik veriler Ã¼retir\n",
    "- **Ã‡oklu LLM DesteÄŸi**: OpenAI, Anthropic, Google, Ollama ve OpenAI-uyumlu API'ler\n",
    "- **FarklÄ± Veri Tipleri**: Soru-cevap, konuÅŸmalar, araÃ§ Ã§aÄŸrÄ±larÄ±, akÄ±l yÃ¼rÃ¼tme zincirleri\n",
    "- **Esnek Formatlar**: TRL, ChatML, Alpaca, Ã¶zel formatlar\n",
    "- **Lokal ve Bulut**: Hem lokal hem de bulut tabanlÄ± modeller\n",
    "\n",
    "### NasÄ±l Ã‡alÄ±ÅŸÄ±r?\n",
    "\n",
    "1. **Konu AÄŸacÄ± OluÅŸturma**: VerdiÄŸiniz ana konudan hiyerarÅŸik alt konular Ã¼retir\n",
    "2. **Veri Ãœretimi**: Her konu iÃ§in soru-cevap Ã§iftleri, konuÅŸmalar veya Ã¶rnekler oluÅŸturur\n",
    "3. **Formatlama**: Ãœretilen veriyi model eÄŸitimi iÃ§in uygun formata dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Kurulum\n",
    "\n",
    "DeepFabric'i pip ile kuruyoruz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepFabric kÃ¼tÃ¼phanesini kur\n",
    "!pip install deepfabric -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerekli kÃ¼tÃ¼phaneleri import et\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Lokal LLM API KonfigÃ¼rasyonu\n",
    "\n",
    "### SeÃ§enek 1: OpenAI-Uyumlu API (vLLM, text-generation-webui, LM Studio vb.)\n",
    "\n",
    "EÄŸer lokal Llama modelinizi OpenAI-uyumlu bir API ile Ã§alÄ±ÅŸtÄ±rÄ±yorsanÄ±z (Ã¶rneÄŸin vLLM, text-generation-webui, LM Studio):\n",
    "\n",
    "```python\n",
    "# API bilgilerinizi buraya girin\n",
    "os.environ['OPENAI_API_KEY'] = 'dummy-key'  # Lokal API iÃ§in genelde gerekli deÄŸil ama zorunlu\n",
    "os.environ['OPENAI_BASE_URL'] = 'http://localhost:8000/v1'  # API URL'iniz\n",
    "```\n",
    "\n",
    "### SeÃ§enek 2: Ollama ile\n",
    "\n",
    "EÄŸer Ollama kullanÄ±yorsanÄ±z:\n",
    "```bash\n",
    "# Terminal'de Ollama'yÄ± baÅŸlatÄ±n\n",
    "ollama serve\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 2.6 Watsonx KullanÄ±mÄ± - Ã–zet ve KarÅŸÄ±laÅŸtÄ±rma\n\n### ğŸ¯ Ä°ki YÃ¶ntem:\n\n#### **YÃ¶ntem 1: LiteLLM Proxy + DeepFabric** (Ã–nerilen)\n\n**Avantajlar:**\n- âœ… DeepFabric'in tÃ¼m Ã¶zelliklerini kullanabilirsiniz (konu aÄŸacÄ±, otomatik format dÃ¶nÃ¼ÅŸÃ¼mÃ¼ vb.)\n- âœ… YAML konfigÃ¼rasyonu ile detaylÄ± kontrol\n- âœ… Birden fazla LLM provider'Ä± tek bir sistemde kullanabilirsiniz\n\n**Dezavantajlar:**\n- âŒ LiteLLM proxy kurulumu gerekir\n- âŒ Ekstra bir servis Ã§alÄ±ÅŸtÄ±rmanÄ±z gerekir\n\n**Ne zaman kullanmalÄ±:**\n- BÃ¼yÃ¼k Ã¶lÃ§ekli veri Ã¼retimi yapacaksanÄ±z\n- FarklÄ± formatlar (TRL, Alpaca, ChatML vb.) gerekiyorsa\n- Konu aÄŸacÄ± (topic tree) Ã¶zelliÄŸini kullanmak istiyorsanÄ±z\n\n---\n\n#### **YÃ¶ntem 2: Direkt Watsonx SDK**\n\n**Avantajlar:**\n- âœ… Ekstra servis gerekmez, doÄŸrudan Watsonx API kullanÄ±lÄ±r\n- âœ… Basit ve hÄ±zlÄ± kurulum\n- âœ… Tam kontrol (Ã¶zel prompt formatlarÄ±, parsing vb.)\n\n**Dezavantajlar:**\n- âŒ DeepFabric Ã¶zellikleri yok\n- âŒ Kendi veri Ã¼retim logic'inizi yazmanÄ±z gerekir\n- âŒ Format dÃ¶nÃ¼ÅŸÃ¼mleri manuel\n\n**Ne zaman kullanmalÄ±:**\n- KÃ¼Ã§Ã¼k Ã¶lÃ§ekli veri Ã¼retimi iÃ§in (< 100 Ã¶rnek)\n- HÄ±zlÄ± prototipleme iÃ§in\n- Ã–zel veri formatÄ± gerekiyorsa\n\n---\n\n### ğŸ“ Hangi YÃ¶ntemi SeÃ§meli?\n\n| Kriter | LiteLLM + DeepFabric | Direkt Watsonx SDK |\n|--------|---------------------|-------------------|\n| Kurulum KolaylÄ±ÄŸÄ± | â­â­â­ | â­â­â­â­â­ |\n| Ã–zellik ZenginliÄŸi | â­â­â­â­â­ | â­â­ |\n| Ã–lÃ§eklenebilirlik | â­â­â­â­â­ | â­â­â­ |\n| Esneklik | â­â­â­â­ | â­â­â­â­â­ |\n\n**Ã–neri:** EÄŸer DeepFabric'i Ã¶ÄŸrenmek ve uzun vadeli kullanmak istiyorsanÄ±z **YÃ¶ntem 1**'i seÃ§in. HÄ±zlÄ± test iÃ§in **YÃ¶ntem 2** yeterlidir.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Direkt Watsonx ile Ã¼retilen veriyi incele\nif os.path.exists('dataset_direct_watsonx.jsonl'):\n    print(\"\\nğŸ“Š DÄ°REKT WATSONX SDK Ä°LE ÃœRETÄ°LEN VERÄ°:\\n\")\n    print(\"=\" * 80)\n    \n    with open('dataset_direct_watsonx.jsonl', 'r', encoding='utf-8') as f:\n        for i, line in enumerate(f):\n            data = json.loads(line)\n            print(f\"\\nğŸ”¹ Soru-Cevap {i+1}:\")\n            print(\"-\" * 80)\n            \n            for msg in data['messages']:\n                role = msg['role'].upper()\n                content = msg['content']\n                print(f\"\\n[{role}]\")\n                print(content)\n            \n            print(\"\\n\" + \"=\" * 80)\nelse:\n    print(\"âŒ Veri seti bulunamadÄ±. Ã–nce yukarÄ±daki cell'i Ã§alÄ±ÅŸtÄ±rÄ±n.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Direkt Watsonx SDK ile txt dosyasÄ±ndan veri Ã¼ret\n# Ã–nce txt dosyasÄ±nÄ± okuyalÄ±m (eÄŸer yoksa oluÅŸturulacak)\n\nif not os.path.exists('ornek_konu.txt'):\n    # Ã–rnek txt oluÅŸtur\n    ornek_icerik = \"\"\"\nYapay Zeka ve Etik\n\nYapay zeka sistemlerinin geliÅŸimi beraberinde Ã¶nemli etik sorularÄ± getirmektedir:\n\n1. Ã–nyargÄ± ve Adalet: AI sistemleri eÄŸitim verilerindeki Ã¶nyargÄ±larÄ± yansÄ±tabilir\n2. Gizlilik: KiÅŸisel verilerin korunmasÄ± kritik Ã¶nem taÅŸÄ±r\n3. ÅeffaflÄ±k: AI kararlarÄ±nÄ±n aÃ§Ä±klanabilir olmasÄ± gerekir\n4. Sorumluluk: AI hatalarÄ±ndan kim sorumludur?\n5. Ä°ÅŸ GÃ¼cÃ¼ Etkisi: Otomasyon ve istihdam dengesi\n\nBu konular yapay zeka geliÅŸtirirken dikkate alÄ±nmalÄ±dÄ±r.\n\"\"\"\n    with open('ornek_konu.txt', 'w', encoding='utf-8') as f:\n        f.write(ornek_icerik)\n    print(\"âœ“ Ã–rnek txt dosyasÄ± oluÅŸturuldu\")\n\n# Txt dosyasÄ±nÄ± oku\nwith open('ornek_konu.txt', 'r', encoding='utf-8') as f:\n    konu = f.read()\n\nprint(f\"ğŸ“– Konu metni okundu ({len(konu)} karakter)\")\nprint(\"\\nÅimdi Watsonx ile sentetik veri Ã¼retiliyor...\")\nprint(\"(Bu iÅŸlem birkaÃ§ dakika sÃ¼rebilir)\\n\")\n\n# Watsonx ile veri Ã¼ret\n# NOT: Bu cell'i Ã§alÄ±ÅŸtÄ±rmak iÃ§in Watsonx API bilgilerinizin doÄŸru olmasÄ± gerekir\ntry:\n    dataset = watsonx_sentetik_veri_uret(\n        konu_metni=konu,\n        soru_sayisi=5  # Test iÃ§in 5 soru-cevap\n    )\n    \n    # JSONL formatÄ±nda kaydet\n    output_file = 'dataset_direct_watsonx.jsonl'\n    with open(output_file, 'w', encoding='utf-8') as f:\n        for item in dataset:\n            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n    \n    print(f\"\\nğŸ’¾ Veri seti kaydedildi: {output_file}\")\n    \nexcept Exception as e:\n    print(f\"\\nâŒ Hata: {e}\")\n    print(\"\\nLÃ¼tfen Watsonx API bilgilerinizi kontrol edin (WATSONX_API_KEY, WATSONX_PROJECT_ID, vb.)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Direkt Watsonx SDK ile Sentetik Veri Ãœretimi\n\nfrom ibm_watsonx_ai.foundation_models import Model\nfrom ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\nimport json\nimport time\n\ndef watsonx_sentetik_veri_uret(\n    konu_metni,\n    soru_sayisi=10,\n    api_key=None,\n    project_id=None,\n    url=None,\n    model_id=None\n):\n    \"\"\"\n    Watsonx kullanarak verilen konu metninden soru-cevap Ã§iftleri Ã¼retir.\n    \n    Args:\n        konu_metni: Ana konu metni (txt dosyasÄ±ndan okunmuÅŸ iÃ§erik)\n        soru_sayisi: KaÃ§ soru-cevap Ã§ifti Ã¼retileceÄŸi\n        api_key: Watsonx API key\n        project_id: Watsonx project ID\n        url: Watsonx URL\n        model_id: KullanÄ±lacak model ID\n    \n    Returns:\n        List of dict: Soru-cevap Ã§iftleri\n    \"\"\"\n    \n    # Watsonx model konfigÃ¼rasyonu\n    model_params = {\n        GenParams.DECODING_METHOD: \"greedy\",\n        GenParams.MAX_NEW_TOKENS: 500,\n        GenParams.TEMPERATURE: 0.7,\n        GenParams.MIN_NEW_TOKENS: 50,\n    }\n    \n    credentials = {\n        \"url\": url or WATSONX_URL,\n        \"apikey\": api_key or WATSONX_API_KEY\n    }\n    \n    # Model instance\n    model = Model(\n        model_id=model_id or WATSONX_MODEL,\n        params=model_params,\n        credentials=credentials,\n        project_id=project_id or WATSONX_PROJECT_ID\n    )\n    \n    # Veri seti\n    dataset = []\n    \n    print(f\"ğŸš€ {soru_sayisi} soru-cevap Ã§ifti Ã¼retiliyor...\\n\")\n    \n    for i in range(soru_sayisi):\n        # Prompt: Konu metninden soru-cevap Ã¼ret\n        prompt = f\"\"\"AÅŸaÄŸÄ±daki konu hakkÄ±nda 1 adet eÄŸitici soru ve detaylÄ± cevap Ã¼ret.\n\nKonu:\n{konu_metni}\n\nLÃ¼tfen ÅŸu formatta yanÄ±t ver:\nSORU: [soru metni]\nCEVAP: [detaylÄ± cevap metni]\n\nSoru numarasÄ±: {i+1}/{soru_sayisi}\n\"\"\"\n        \n        try:\n            # Model'den yanÄ±t al\n            yanit = model.generate_text(prompt=prompt)\n            \n            # Soru ve cevabÄ± parse et\n            if \"SORU:\" in yanit and \"CEVAP:\" in yanit:\n                parts = yanit.split(\"CEVAP:\")\n                soru = parts[0].replace(\"SORU:\", \"\").strip()\n                cevap = parts[1].strip()\n                \n                # ChatML formatÄ±nda kaydet\n                veri_ornegi = {\n                    \"messages\": [\n                        {\"role\": \"system\", \"content\": \"Sen yardÄ±mcÄ± bir TÃ¼rkÃ§e AI asistanÄ±sÄ±n.\"},\n                        {\"role\": \"user\", \"content\": soru},\n                        {\"role\": \"assistant\", \"content\": cevap}\n                    ]\n                }\n                \n                dataset.append(veri_ornegi)\n                print(f\"âœ“ Ã–rnek {i+1} oluÅŸturuldu\")\n            else:\n                print(f\"âš  Ã–rnek {i+1} formatÄ± hatalÄ±, atlandÄ±\")\n            \n            # Rate limiting iÃ§in kÄ±sa bekleme\n            time.sleep(0.5)\n            \n        except Exception as e:\n            print(f\"âŒ Ã–rnek {i+1} hatasÄ±: {e}\")\n            continue\n    \n    print(f\"\\nâœ… Toplam {len(dataset)} soru-cevap Ã§ifti oluÅŸturuldu!\")\n    return dataset\n\nprint(\"âœ“ Fonksiyon tanÄ±mlandÄ±: watsonx_sentetik_veri_uret()\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 2.5 Alternatif: LiteLLM Olmadan Direkt Watsonx KullanÄ±mÄ±\n\nEÄŸer LiteLLM proxy kurmak istemiyorsanÄ±z, Watsonx SDK'sÄ±nÄ± doÄŸrudan kullanarak basit bir sentetik veri Ã¼retimi yapabilirsiniz.\n\nBu yÃ¶ntemde DeepFabric kullanmayacaÄŸÄ±z, ancak benzer mantÄ±kla kendi sentetik veri Ã¼retim script'imizi yazacaÄŸÄ±z.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Watsonx ile Ã¼retilen soru-cevaplarÄ± incele\nif os.path.exists('dataset_watsonx_from_txt.jsonl'):\n    print(\"\\nğŸ“Š WATSONX Ä°LE ÃœRETÄ°LEN SORU-CEVAPLAR:\\n\")\n    print(\"=\" * 80)\n    \n    with open('dataset_watsonx_from_txt.jsonl', 'r', encoding='utf-8') as f:\n        veri_sayisi = 0\n        for i, line in enumerate(f):\n            veri_sayisi += 1\n            if i < 3:  # Ä°lk 3 Ã¶rneÄŸi gÃ¶ster\n                data = json.loads(line)\n                print(f\"\\nğŸ”¹ Ã–rnek {i+1}:\")\n                print(\"-\" * 80)\n                \n                if 'messages' in data:\n                    for msg in data['messages']:\n                        role = msg.get('role', 'unknown').upper()\n                        content = msg.get('content', '')\n                        print(f\"\\n[{role}]\")\n                        # TÃ¼rkÃ§e karakterleri doÄŸru gÃ¶ster\n                        if len(content) <= 500:\n                            print(content)\n                        else:\n                            print(content[:500] + \"...\")\n                else:\n                    print(json.dumps(data, indent=2, ensure_ascii=False)[:1000])\n                \n                print(\"\\n\" + \"=\" * 80)\n    \n    print(f\"\\nğŸ“ˆ Toplam {veri_sayisi} soru-cevap Ã§ifti Ã¼retildi!\")\n    \nelse:\n    print(\"âŒ Veri seti henÃ¼z oluÅŸturulmadÄ±.\")\n    print(\"Ã–nce yukarÄ±daki cell'i Ã§alÄ±ÅŸtÄ±rÄ±n.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Watsonx ile txt dosyasÄ±ndan soru-cevap Ã¼ret\n!deepfabric generate \\\n  --config watsonx_txt_config.yaml \\\n  --dataset-save-as dataset_watsonx_from_txt.jsonl\n\nprint(\"\\nâœ… Watsonx ile txt dosyasÄ±ndan sentetik veri Ã¼retimi tamamlandÄ±!\")\nprint(\"SonuÃ§lar: dataset_watsonx_from_txt.jsonl\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Watsonx ile txt iÃ§eriÄŸinden soru-cevap Ã¼retimi iÃ§in YAML config oluÅŸtur\nwatsonx_txt_config = f\"\"\"\ndataset_system_prompt: |\n  Sen yardÄ±mcÄ± bir TÃ¼rkÃ§e AI asistanÄ±sÄ±n. KullanÄ±cÄ±lara verilen konular \n  hakkÄ±nda detaylÄ± ve anlaÅŸÄ±lÄ±r bilgiler veriyorsun.\n\ntopic_tree:\n  # Txt dosyasÄ±nÄ±n iÃ§eriÄŸini konu olarak kullanÄ±yoruz\n  topic_prompt: |\n    AÅŸaÄŸÄ±daki iÃ§erik hakkÄ±nda detaylÄ± alt konular ve soru-cevap Ã§iftleri oluÅŸtur:\n    \n    {txt_icerik}\n  \n  provider: \"{PROVIDER}\"\n  model: \"{MODEL_NAME}\"\n  temperature: 0.7\n  degree: 3  # Her konudan 3 alt konu\n  depth: 2   # 2 seviye derinlik\n\ndata_engine:\n  generation_system_prompt: |\n    Verilen konular hakkÄ±nda TÃ¼rkÃ§e soru-cevap Ã§iftleri oluÅŸtur.\n    Her cevap:\n    - Net ve anlaÅŸÄ±lÄ±r olmalÄ±\n    - Ã–rnekler iÃ§ermeli\n    - EÄŸitici olmalÄ±\n    - Orta seviye teknik detay iÃ§ermeli\n  \n  temperature: 0.6\n  provider: \"{PROVIDER}\"\n  model: \"{MODEL_NAME}\"\n\ndataset:\n  creation:\n    num_steps: 20      # 20 soru-cevap Ã§ifti Ã¼ret\n    batch_size: 2      # 2'ÅŸer 2'ÅŸer Ã¼ret\n    sys_msg: true\n  template: \"builtin://chatml.py\"\n\"\"\"\n\n# Config dosyasÄ±nÄ± kaydet\nwith open('watsonx_txt_config.yaml', 'w', encoding='utf-8') as f:\n    f.write(watsonx_txt_config)\n\nprint(\"âœ“ Watsonx txt-based konfigÃ¼rasyon oluÅŸturuldu: watsonx_txt_config.yaml\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Kendi txt dosyanÄ±zÄ± okuyun (Ã¶rnek olarak ornek_konu.txt kullanÄ±yoruz)\n# EÄŸer kendi dosyanÄ±zÄ± kullanmak isterseniz dosya yolunu deÄŸiÅŸtirin\n\nTXT_DOSYA_YOLU = \"ornek_konu.txt\"  # Kendi dosyanÄ±zÄ±n yolunu girin\n\n# Txt dosyasÄ±nÄ± oku\ntry:\n    with open(TXT_DOSYA_YOLU, 'r', encoding='utf-8') as f:\n        txt_icerik = f.read()\n    \n    print(f\"âœ“ Dosya okundu: {TXT_DOSYA_YOLU}\")\n    print(f\"  Ä°Ã§erik uzunluÄŸu: {len(txt_icerik)} karakter\")\n    print(f\"\\n--- Dosya Ä°Ã§eriÄŸi (Ä°lk 300 karakter) ---\")\n    print(txt_icerik[:300])\n    print(\"...\")\n    \nexcept FileNotFoundError:\n    print(f\"âŒ Dosya bulunamadÄ±: {TXT_DOSYA_YOLU}\")\n    print(\"Ã–nce bir txt dosyasÄ± oluÅŸturun veya mevcut dosyanÄ±n yolunu kontrol edin.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 2.4 Watsonx ile TXT DosyasÄ±ndan Soru-Cevap Ãœretimi\n\nKendi txt dosyanÄ±zÄ± kullanarak Watsonx ile sentetik soru-cevap setleri oluÅŸturabilirsiniz.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Watsonx ile basit sentetik veri Ã¼retimi\n# NOT: LiteLLM proxy'nin Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olun!\n\n!deepfabric generate \\\n  --mode tree \\\n  --provider {PROVIDER} \\\n  --model {MODEL_NAME} \\\n  --depth 2 \\\n  --degree 2 \\\n  --num-steps 10 \\\n  --topic-prompt \"Yapay Zeka ve Makine Ã–ÄŸrenmesi Temelleri\" \\\n  --dataset-save-as dataset_watsonx.jsonl\n\nprint(\"\\nâœ“ Watsonx ile veri Ã¼retimi tamamlandÄ±!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 2.3 Watsonx ile Sentetik Veri Ãœretimi - Ã–rnek\n\nArtÄ±k Watsonx modelinizi kullanarak sentetik veri Ã¼retebilirsiniz!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# LiteLLM Proxy ile DeepFabric KullanÄ±mÄ±\n# Ã–NCE: Terminal'de 'litellm --config litellm_config.yaml --port 8000' komutunu Ã§alÄ±ÅŸtÄ±rÄ±n\n\n# OpenAI API konfigÃ¼rasyonunu LiteLLM proxy'ye yÃ¶nlendir\nos.environ['OPENAI_API_KEY'] = 'sk-1234'  # LiteLLM master_key\nos.environ['OPENAI_BASE_URL'] = 'http://localhost:8000'  # LiteLLM proxy URL\n\nPROVIDER = 'openai'\nMODEL_NAME = 'watsonx-llama'  # LiteLLM config'deki model_name\n\nprint(\"âœ“ DeepFabric, Watsonx'e LiteLLM proxy Ã¼zerinden baÄŸlanacak\")\nprint(f\"  Provider: {PROVIDER}\")\nprint(f\"  Model: {MODEL_NAME}\")\nprint(f\"  Proxy URL: {os.environ['OPENAI_BASE_URL']}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# LiteLLM Proxy KonfigÃ¼rasyon DosyasÄ± OluÅŸtur\nlitellm_config = f\"\"\"\nmodel_list:\n  - model_name: watsonx-llama\n    litellm_params:\n      model: watsonx/{WATSONX_MODEL}\n      api_key: {WATSONX_API_KEY}\n      project_id: {WATSONX_PROJECT_ID}\n      url: {WATSONX_URL}\n\ngeneral_settings:\n  master_key: sk-1234  # Basit bir key, gÃ¼venlik iÃ§in deÄŸiÅŸtirin\n\"\"\"\n\n# Config dosyasÄ±nÄ± kaydet\nwith open('litellm_config.yaml', 'w') as f:\n    f.write(litellm_config)\n\nprint(\"âœ“ LiteLLM config dosyasÄ± oluÅŸturuldu: litellm_config.yaml\")\nprint(\"\\nÅimdi yeni bir terminal aÃ§Ä±p ÅŸu komutu Ã§alÄ±ÅŸtÄ±rÄ±n:\")\nprint(\"  litellm --config litellm_config.yaml --port 8000\")\nprint(\"\\nProxy baÅŸladÄ±ktan sonra DeepFabric'i ÅŸu ÅŸekilde kullanabilirsiniz:\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 2.2 Watsonx ile DeepFabric KullanÄ±mÄ±\n\nDeepFabric doÄŸrudan Watsonx'i desteklemez, ancak **LiteLLM proxy** kullanarak Watsonx'i OpenAI-uyumlu bir endpoint haline getirebiliriz.\n\n### YÃ¶ntem 1: LiteLLM Proxy KullanÄ±mÄ± (Ã–nerilen)\n\nLiteLLM, Watsonx dahil birÃ§ok LLM provider'Ä± OpenAI API formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼ren bir proxy'dir.\n\n**AdÄ±mlar:**\n\n1. LiteLLM proxy config dosyasÄ± oluÅŸtur\n2. Proxy'yi baÅŸlat (localhost:8000)\n3. DeepFabric'i bu proxy ile kullan\n\n### YÃ¶ntem 2: Watsonx API Wrapper ile Ã–zel Implementation\n\nWatsonx API'sini doÄŸrudan kullanarak veri Ã¼retimi yapabiliriz (DeepFabric olmadan, benzer yaklaÅŸÄ±mla).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Watsonx API baÄŸlantÄ±sÄ±nÄ± test edelim\nfrom ibm_watsonx_ai.foundation_models import Model\nfrom ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n\n# Watsonx model parametreleri\nmodel_params = {\n    GenParams.DECODING_METHOD: \"greedy\",\n    GenParams.MAX_NEW_TOKENS: 100,\n    GenParams.TEMPERATURE: 0.7,\n}\n\n# Watsonx credentials\ncredentials = {\n    \"url\": WATSONX_URL,\n    \"apikey\": WATSONX_API_KEY\n}\n\ntry:\n    # Model instance oluÅŸtur\n    watsonx_model = Model(\n        model_id=WATSONX_MODEL,\n        params=model_params,\n        credentials=credentials,\n        project_id=WATSONX_PROJECT_ID\n    )\n    \n    # Test prompt\n    test_prompt = \"Python programlama dilinin en Ã¶nemli Ã¶zelliÄŸi nedir?\"\n    response = watsonx_model.generate_text(prompt=test_prompt)\n    \n    print(\"âœ… Watsonx API baÄŸlantÄ±sÄ± baÅŸarÄ±lÄ±!\")\n    print(f\"\\nTest Prompt: {test_prompt}\")\n    print(f\"\\nModel YanÄ±tÄ±:\\n{response}\")\n    \nexcept Exception as e:\n    print(f\"âŒ BaÄŸlantÄ± hatasÄ±: {e}\")\n    print(\"\\nLÃ¼tfen API key, Project ID ve URL bilgilerinizi kontrol edin.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# WATSONX API KONFÄ°GÃœRASYONU\n# Kendi bilgilerinizi buraya girin\n\n# 1. IBM Cloud API bilgileriniz\nWATSONX_API_KEY = \"your-ibm-cloud-api-key-here\"  # IBM Cloud IAM API Key\nWATSONX_PROJECT_ID = \"your-project-id-here\"      # Watsonx Project ID\nWATSONX_URL = \"https://us-south.ml.cloud.ibm.com\"  # Region'Ä±nÄ±za gÃ¶re deÄŸiÅŸebilir\n\n# 2. Model seÃ§imi\n# Watsonx'te kullanabileceÄŸiniz modeller:\n# - \"meta-llama/llama-3-70b-instruct\"\n# - \"meta-llama/llama-3-8b-instruct\"\n# - \"ibm/granite-13b-chat-v2\"\n# - \"mistralai/mixtral-8x7b-instruct-v01\"\nWATSONX_MODEL = \"meta-llama/llama-3-70b-instruct\"  # Kendi modelinizi seÃ§in\n\n# 3. Watsonx credentials'larÄ± environment variable olarak ayarla\nos.environ['WATSONX_APIKEY'] = WATSONX_API_KEY\nos.environ['WATSONX_PROJECT_ID'] = WATSONX_PROJECT_ID\nos.environ['WATSONX_URL'] = WATSONX_URL\n\nprint(\"âœ“ Watsonx API KonfigÃ¼rasyonu tamamlandÄ±\")\nprint(f\"  Model: {WATSONX_MODEL}\")\nprint(f\"  URL: {WATSONX_URL}\")\nprint(f\"  Project ID: {WATSONX_PROJECT_ID[:20]}...\")  # Ä°lk 20 karakter",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Watsonx entegrasyonu iÃ§in gerekli kÃ¼tÃ¼phaneleri kur\n# SeÃ§enek 1: LiteLLM ile (Ã–nerilen)\n!pip install litellm ibm-watsonx-ai -q\n\n# SeÃ§enek 2: Sadece Watsonx SDK\n# !pip install ibm-watsonx-ai -q\n\nprint(\"âœ“ Watsonx kÃ¼tÃ¼phaneleri kuruldu\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 2.1 Watsonx Platform KonfigÃ¼rasyonu (IBM Cloud)\n\n**Watsonx**, IBM Cloud platformunda Ã§alÄ±ÅŸan gÃ¼Ã§lÃ¼ bir LLM servisidir. Llama, Granite ve diÄŸer aÃ§Ä±k kaynak modelleri API Ã¼zerinden kullanmanÄ±za olanak tanÄ±r.\n\n### Watsonx API Bilgilerinizi Alma:\n\n1. **IBM Cloud Console**'a giriÅŸ yapÄ±n: https://cloud.ibm.com\n2. **Watsonx.ai** servisinize gidin\n3. AÅŸaÄŸÄ±daki bilgileri alÄ±n:\n   - **API Key**: IBM Cloud IAM API anahtarÄ±nÄ±z\n   - **Project ID**: Watsonx projenizin ID'si\n   - **URL**: Watsonx API endpoint URL'i (genelde `https://us-south.ml.cloud.ibm.com` veya region'Ä±nÄ±za gÃ¶re deÄŸiÅŸir)\n\n### Watsonx API'nin DeepFabric ile KullanÄ±mÄ±:\n\nWatsonx API'si OpenAI-uyumlu **deÄŸildir**, ancak Python ile direkt API Ã§aÄŸrÄ±larÄ± yaparak veya bir wrapper kullanarak DeepFabric'e entegre edebiliriz.\n\n**Ä°ki SeÃ§enek:**\n\n1. **LiteLLM Kullanarak** (Ã–nerilen - Watsonx'i OpenAI-compatible hale getirir)\n2. **Direkt Watsonx SDK** ile Ã¶zel implementation\n\nAÅŸaÄŸÄ±da her iki yÃ¶ntemi gÃ¶receÄŸiz.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API KONFÄ°GÃœRASYONU - KENDÄ° BÄ°LGÄ°LERÄ°NÄ°ZÄ° GÄ°RÄ°N\n",
    "\n",
    "# SeÃ§enek 1: OpenAI-uyumlu lokal API\n",
    "os.environ['OPENAI_API_KEY'] = 'sk-dummy-key'  # Lokal iÃ§in dummy key\n",
    "os.environ['OPENAI_BASE_URL'] = 'http://localhost:8000/v1'  # Kendi API URL'iniz\n",
    "\n",
    "# Model adÄ± - API'nizde kullandÄ±ÄŸÄ±nÄ±z model adÄ±\n",
    "LOCAL_MODEL_NAME = 'llama-3.1-8b'  # Kendi model adÄ±nÄ±zÄ± girin\n",
    "\n",
    "# EÄŸer Ollama kullanÄ±yorsanÄ±z:\n",
    "# PROVIDER = 'ollama'\n",
    "# LOCAL_MODEL_NAME = 'llama3.1:8b'\n",
    "\n",
    "PROVIDER = 'openai'  # OpenAI-uyumlu API iÃ§in 'openai', Ollama iÃ§in 'ollama'\n",
    "\n",
    "print(f\"âœ“ API KonfigÃ¼rasyonu tamamlandÄ±\")\n",
    "print(f\"  Provider: {PROVIDER}\")\n",
    "print(f\"  Model: {LOCAL_MODEL_NAME}\")\n",
    "print(f\"  Base URL: {os.environ.get('OPENAI_BASE_URL', 'default')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basit Ã–rnek: CLI ile Sentetik Veri Ãœretimi\n",
    "\n",
    "DeepFabric'in en basit kullanÄ±mÄ± CLI (komut satÄ±rÄ±) Ã¼zerinden yapÄ±lÄ±r:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basit bir sentetik veri seti oluÅŸtur\n",
    "# Bu komut:\n",
    "# 1. \"Python Programlama\" konusundan konu aÄŸacÄ± oluÅŸturur (depth=2, degree=2)\n",
    "# 2. Her konu iÃ§in 8 veri Ã¶rneÄŸi Ã¼retir (num-steps=8)\n",
    "# 3. SonuÃ§larÄ± 'dataset_basit.jsonl' dosyasÄ±na kaydeder\n",
    "\n",
    "!deepfabric generate \\\n",
    "  --mode tree \\\n",
    "  --provider {PROVIDER} \\\n",
    "  --model {LOCAL_MODEL_NAME} \\\n",
    "  --depth 2 \\\n",
    "  --degree 2 \\\n",
    "  --num-steps 8 \\\n",
    "  --topic-prompt \"Python Programlama Temelleri\" \\\n",
    "  --dataset-save-as dataset_basit.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OluÅŸturulan veriyi inceleyelim\n",
    "if os.path.exists('dataset_basit.jsonl'):\n",
    "    with open('dataset_basit.jsonl', 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i < 2:  # Ä°lk 2 Ã¶rneÄŸi gÃ¶ster\n",
    "                data = json.loads(line)\n",
    "                print(f\"\\n--- Ã–rnek {i+1} ---\")\n",
    "                print(json.dumps(data, indent=2, ensure_ascii=False))\n",
    "else:\n",
    "    print(\"âŒ Veri seti henÃ¼z oluÅŸturulmadÄ±\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. YAML KonfigÃ¼rasyonu ile Ä°leri Seviye KullanÄ±m\n",
    "\n",
    "Daha fazla kontrol iÃ§in YAML konfigÃ¼rasyon dosyasÄ± kullanabilirsiniz. Bu yÃ¶ntemle:\n",
    "- Prompt'larÄ± Ã¶zelleÅŸtirebilirsiniz\n",
    "- FarklÄ± parametreler ayarlayabilirsiniz\n",
    "- Ã‡Ä±ktÄ± formatÄ±nÄ± belirleyebilirsiniz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YAML konfigÃ¼rasyon dosyasÄ± oluÅŸtur\n",
    "config_yaml = f\"\"\"\n",
    "# Sistem prompt'u - veri setindeki genel davranÄ±ÅŸÄ± belirler\n",
    "dataset_system_prompt: |\n",
    "  Sen yardÄ±mcÄ± bir TÃ¼rkÃ§e AI asistanÄ±sÄ±n. KullanÄ±cÄ±lara detaylÄ± ve \n",
    "  anlaÅŸÄ±lÄ±r ÅŸekilde yanÄ±t veriyorsun.\n",
    "\n",
    "# Konu aÄŸacÄ± konfigÃ¼rasyonu\n",
    "topic_tree:\n",
    "  # Ana konu - buradan alt konular tÃ¼retilecek\n",
    "  topic_prompt: \"Makine Ã–ÄŸrenmesi ve Yapay Zeka\"\n",
    "  \n",
    "  # LLM provider ayarlarÄ±\n",
    "  provider: \"{PROVIDER}\"\n",
    "  model: \"{LOCAL_MODEL_NAME}\"\n",
    "  temperature: 0.7  # YaratÄ±cÄ±lÄ±k seviyesi (0.0-1.0)\n",
    "  \n",
    "  # AÄŸaÃ§ yapÄ±sÄ±\n",
    "  degree: 3  # Her konudan kaÃ§ alt konu tÃ¼retilebilir\n",
    "  depth: 2   # Konu aÄŸacÄ±nÄ±n derinliÄŸi\n",
    "\n",
    "# Veri Ã¼retimi ayarlarÄ±\n",
    "data_engine:\n",
    "  # Veri Ã¼retim prompt'u - Ã¼retilecek verilerin stilini belirler\n",
    "  generation_system_prompt: |\n",
    "    Sen bir yapay zeka eÄŸitmenisin. Ã–ÄŸrencilere kavramlarÄ± aÃ§Ä±k ve \n",
    "    anlaÅŸÄ±lÄ±r ÅŸekilde Ã¶ÄŸretiyorsun. Her soruya detaylÄ±, Ã¶rnekli ve \n",
    "    adÄ±m adÄ±m yanÄ±tlar veriyorsun.\n",
    "  \n",
    "  temperature: 0.6\n",
    "  provider: \"{PROVIDER}\"\n",
    "  model: \"{LOCAL_MODEL_NAME}\"\n",
    "\n",
    "# Veri seti oluÅŸturma parametreleri\n",
    "dataset:\n",
    "  creation:\n",
    "    num_steps: 12      # Toplam kaÃ§ veri Ã¶rneÄŸi Ã¼retilecek\n",
    "    batch_size: 2      # AynÄ± anda kaÃ§ Ã¶rnek Ã¼retilecek\n",
    "    sys_msg: true      # Sistem mesajÄ± dahil edilsin mi\n",
    "  \n",
    "  # Ã‡Ä±ktÄ± formatÄ± (opsiyonel)\n",
    "  # builtin://trl_sft_tools  - HuggingFace TRL iÃ§in\n",
    "  # builtin://chatml.py      - ChatML formatÄ±\n",
    "  # builtin://alpaca.py      - Alpaca formatÄ±\n",
    "  template: \"builtin://chatml.py\"\n",
    "\"\"\"\n",
    "\n",
    "# Config dosyasÄ±nÄ± kaydet\n",
    "with open('deepfabric_config.yaml', 'w', encoding='utf-8') as f:\n",
    "    f.write(config_yaml)\n",
    "\n",
    "print(\"âœ“ KonfigÃ¼rasyon dosyasÄ± oluÅŸturuldu: deepfabric_config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YAML konfigÃ¼rasyonu ile veri Ã¼ret\n",
    "!deepfabric generate \\\n",
    "  --config deepfabric_config.yaml \\\n",
    "  --dataset-save-as dataset_yaml.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. TXT DosyasÄ±ndan Soru-Cevap Ãœretimi\n",
    "\n",
    "DeepFabric doÄŸrudan txt dosyasÄ±ndan veri Ã¼retmez, ancak txt iÃ§eriÄŸini **konu (topic)** olarak kullanabilirsiniz.\n",
    "\n",
    "### YaklaÅŸÄ±m:\n",
    "1. Txt dosyanÄ±zÄ± okuyun\n",
    "2. Ä°Ã§eriÄŸi topic prompt olarak kullanÄ±n\n",
    "3. DeepFabric bu konudan soru-cevaplar Ã¼retir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ã–rnek bir txt dosyasÄ± oluÅŸtur\n",
    "ornek_metin = \"\"\"\n",
    "Python Veri YapÄ±larÄ±\n",
    "\n",
    "Python'da temel veri yapÄ±larÄ± ÅŸunlardÄ±r:\n",
    "\n",
    "1. Liste (List): SÄ±ralÄ±, deÄŸiÅŸtirilebilir koleksiyon. Ã–rnek: [1, 2, 3, 4]\n",
    "2. Demet (Tuple): SÄ±ralÄ±, deÄŸiÅŸtirilemez koleksiyon. Ã–rnek: (1, 2, 3)\n",
    "3. SÃ¶zlÃ¼k (Dictionary): Anahtar-deÄŸer Ã§iftleri. Ã–rnek: {'ad': 'Ali', 'yaÅŸ': 25}\n",
    "4. KÃ¼me (Set): SÄ±rasÄ±z, benzersiz elemanlar. Ã–rnek: {1, 2, 3, 4}\n",
    "\n",
    "Listeler en Ã§ok kullanÄ±lan veri yapÄ±sÄ±dÄ±r Ã§Ã¼nkÃ¼ esnektir ve birÃ§ok metoda sahiptir.\n",
    "\"\"\"\n",
    "\n",
    "with open('ornek_konu.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(ornek_metin)\n",
    "\n",
    "print(\"âœ“ Ã–rnek txt dosyasÄ± oluÅŸturuldu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Txt dosyasÄ±nÄ± oku ve iÃ§eriÄŸi topic olarak kullan\n",
    "with open('ornek_konu.txt', 'r', encoding='utf-8') as f:\n",
    "    txt_icerik = f.read()\n",
    "\n",
    "# Ã–zel prompt ile YAML oluÅŸtur\n",
    "txt_config = f\"\"\"\n",
    "dataset_system_prompt: |\n",
    "  Sen bir Python programlama eÄŸitmenisin. Ã–ÄŸrencilere veri yapÄ±larÄ±nÄ± \n",
    "  aÃ§Ä±k ve Ã¶rneklerle Ã¶ÄŸretiyorsun.\n",
    "\n",
    "topic_tree:\n",
    "  # Txt dosyasÄ±nÄ±n iÃ§eriÄŸini konu olarak kullanÄ±yoruz\n",
    "  topic_prompt: |\n",
    "    {txt_icerik}\n",
    "    \n",
    "    YukarÄ±daki konular hakkÄ±nda detaylÄ± alt baÅŸlÄ±klar oluÅŸtur.\n",
    "  \n",
    "  provider: \"{PROVIDER}\"\n",
    "  model: \"{LOCAL_MODEL_NAME}\"\n",
    "  temperature: 0.7\n",
    "  degree: 4\n",
    "  depth: 2\n",
    "\n",
    "data_engine:\n",
    "  generation_system_prompt: |\n",
    "    Python veri yapÄ±larÄ± hakkÄ±nda soru-cevap Ã§iftleri oluÅŸtur.\n",
    "    Her cevap:\n",
    "    - AÃ§Ä±k ve anlaÅŸÄ±lÄ±r olmalÄ±\n",
    "    - Kod Ã¶rnekleri iÃ§ermeli\n",
    "    - Pratik kullanÄ±m senaryolarÄ± gÃ¶stermeli\n",
    "  \n",
    "  temperature: 0.5\n",
    "  provider: \"{PROVIDER}\"\n",
    "  model: \"{LOCAL_MODEL_NAME}\"\n",
    "\n",
    "dataset:\n",
    "  creation:\n",
    "    num_steps: 16\n",
    "    batch_size: 2\n",
    "    sys_msg: true\n",
    "  template: \"builtin://chatml.py\"\n",
    "\"\"\"\n",
    "\n",
    "with open('txt_based_config.yaml', 'w', encoding='utf-8') as f:\n",
    "    f.write(txt_config)\n",
    "\n",
    "print(\"âœ“ Txt tabanlÄ± konfigÃ¼rasyon oluÅŸturuldu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Txt iÃ§eriÄŸinden soru-cevap Ã¼ret\n",
    "!deepfabric generate \\\n",
    "  --config txt_based_config.yaml \\\n",
    "  --dataset-save-as dataset_from_txt.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ãœretilen soru-cevaplarÄ± incele\n",
    "if os.path.exists('dataset_from_txt.jsonl'):\n",
    "    print(\"\\nğŸ“Š TXT DOSYASINDAN ÃœRETÄ°LEN SORU-CEVAPLAR:\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    with open('dataset_from_txt.jsonl', 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i < 3:  # Ä°lk 3 Ã¶rneÄŸi gÃ¶ster\n",
    "                data = json.loads(line)\n",
    "                print(f\"\\nğŸ”¹ Ã–rnek {i+1}:\")\n",
    "                print(\"-\" * 80)\n",
    "                \n",
    "                # Messages varsa gÃ¼zel formatlayalÄ±m\n",
    "                if 'messages' in data:\n",
    "                    for msg in data['messages']:\n",
    "                        role = msg.get('role', 'unknown').upper()\n",
    "                        content = msg.get('content', '')\n",
    "                        print(f\"\\n[{role}]\")\n",
    "                        print(content[:500])  # Ä°lk 500 karakter\n",
    "                        if len(content) > 500:\n",
    "                            print(\"... (devamÄ± var)\")\n",
    "                else:\n",
    "                    print(json.dumps(data, indent=2, ensure_ascii=False)[:1000])\n",
    "                \n",
    "                print(\"\\n\" + \"=\" * 80)\n",
    "else:\n",
    "    print(\"âŒ Veri seti henÃ¼z oluÅŸturulmadÄ±\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. FarklÄ± Veri FormatlarÄ±\n",
    "\n",
    "DeepFabric, Ã¼retilen veriyi farklÄ± formatlara dÃ¶nÃ¼ÅŸtÃ¼rebilir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mevcut bir veri setini farklÄ± formata Ã§evir\n",
    "# ChatML formatÄ±na Ã§evir\n",
    "!deepfabric format \\\n",
    "  --dataset dataset_from_txt.jsonl \\\n",
    "  --format builtin://chatml.py \\\n",
    "  --output dataset_chatml.jsonl\n",
    "\n",
    "print(\"\\nâœ“ ChatML formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼ldÃ¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpaca formatÄ±na Ã§evir\n",
    "!deepfabric format \\\n",
    "  --dataset dataset_from_txt.jsonl \\\n",
    "  --format builtin://alpaca.py \\\n",
    "  --output dataset_alpaca.jsonl\n",
    "\n",
    "print(\"\\nâœ“ Alpaca formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼ldÃ¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Parametrelerin AnlamÄ± ve Ã–zelleÅŸtirme\n",
    "\n",
    "### Konu AÄŸacÄ± Parametreleri:\n",
    "\n",
    "- **`depth`**: Konu aÄŸacÄ±nÄ±n derinliÄŸi. Ã–rnek: `depth=3` â†’ Ana konu â†’ Alt konu â†’ Alt-alt konu\n",
    "- **`degree`**: Her konudan kaÃ§ alt konu Ã¼retilir. Ã–rnek: `degree=4` â†’ Her konudan 4 alt konu\n",
    "- **`topic_prompt`**: Ana konu. Buradan tÃ¼m alt konular tÃ¼retilir\n",
    "\n",
    "**Toplam konu sayÄ±sÄ±**: `degree^depth` (Ã¶rn: 3^2 = 9 konu)\n",
    "\n",
    "### Veri Ãœretim Parametreleri:\n",
    "\n",
    "- **`num_steps`**: Toplam kaÃ§ veri Ã¶rneÄŸi Ã¼retilecek\n",
    "- **`batch_size`**: AynÄ± anda kaÃ§ Ã¶rnek Ã¼retilecek (paralel iÅŸlem)\n",
    "- **`temperature`**: YaratÄ±cÄ±lÄ±k seviyesi (0.0 = deterministik, 1.0 = yaratÄ±cÄ±)\n",
    "- **`sys_msg`**: Sistem mesajÄ± dahil edilsin mi?\n",
    "\n",
    "### Prompt'lar:\n",
    "\n",
    "- **`dataset_system_prompt`**: Genel sistem davranÄ±ÅŸÄ±\n",
    "- **`generation_system_prompt`**: Veri Ã¼retim stili\n",
    "- **`topic_prompt`**: Ana konu (txt dosyanÄ±zÄ±n iÃ§eriÄŸi de olabilir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ã–zel Senaryolar\n",
    "\n",
    "### Senaryo 1: Kod Ã–rnekleri Ãœretimi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kod_config = f\"\"\"\n",
    "dataset_system_prompt: |\n",
    "  Sen bir programlama eÄŸitmenisin. Her yanÄ±tta Ã§alÄ±ÅŸan kod Ã¶rnekleri veriyorsun.\n",
    "\n",
    "topic_tree:\n",
    "  topic_prompt: |\n",
    "    Python ile veri analizi: Pandas, NumPy ve Matplotlib kullanÄ±mÄ±\n",
    "  \n",
    "  provider: \"{PROVIDER}\"\n",
    "  model: \"{LOCAL_MODEL_NAME}\"\n",
    "  temperature: 0.6\n",
    "  degree: 3\n",
    "  depth: 2\n",
    "\n",
    "data_engine:\n",
    "  generation_system_prompt: |\n",
    "    Veri analizi hakkÄ±nda soru-cevap Ã¼ret. Her cevap:\n",
    "    1. Konsepti aÃ§Ä±kla\n",
    "    2. Kod Ã¶rneÄŸi ver (Ã§alÄ±ÅŸÄ±r kod)\n",
    "    3. Ã‡Ä±ktÄ±yÄ± gÃ¶ster\n",
    "    4. KullanÄ±m alanlarÄ±nÄ± aÃ§Ä±kla\n",
    "  \n",
    "  temperature: 0.4\n",
    "  provider: \"{PROVIDER}\"\n",
    "  model: \"{LOCAL_MODEL_NAME}\"\n",
    "\n",
    "dataset:\n",
    "  creation:\n",
    "    num_steps: 20\n",
    "    batch_size: 2\n",
    "    sys_msg: true\n",
    "\"\"\"\n",
    "\n",
    "with open('kod_ornekleri_config.yaml', 'w', encoding='utf-8') as f:\n",
    "    f.write(kod_config)\n",
    "\n",
    "print(\"âœ“ Kod Ã¶rnekleri konfigÃ¼rasyonu hazÄ±r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Senaryo 2: Ã‡ok Turlu KonuÅŸmalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "konusma_config = f\"\"\"\n",
    "dataset_system_prompt: |\n",
    "  Sen yardÄ±mcÄ± bir mÃ¼ÅŸteri hizmetleri temsilcisisin. Nazik ve Ã§Ã¶zÃ¼m odaklÄ±sÄ±n.\n",
    "\n",
    "topic_tree:\n",
    "  topic_prompt: |\n",
    "    E-ticaret mÃ¼ÅŸteri hizmetleri: SipariÅŸ sorunlarÄ±, iade iÅŸlemleri, \n",
    "    Ã¼rÃ¼n bilgileri, teslimat takibi\n",
    "  \n",
    "  provider: \"{PROVIDER}\"\n",
    "  model: \"{LOCAL_MODEL_NAME}\"\n",
    "  temperature: 0.7\n",
    "  degree: 4\n",
    "  depth: 2\n",
    "\n",
    "data_engine:\n",
    "  generation_system_prompt: |\n",
    "    MÃ¼ÅŸteri-temsilci konuÅŸmalarÄ± oluÅŸtur. Her konuÅŸma:\n",
    "    - GerÃ§ekÃ§i olmalÄ±\n",
    "    - 3-5 mesaj alÄ±ÅŸveriÅŸi iÃ§ermeli\n",
    "    - Sorunu Ã§Ã¶zmeli\n",
    "    - Profesyonel ama samimi olmalÄ±\n",
    "  \n",
    "  temperature: 0.7\n",
    "  provider: \"{PROVIDER}\"\n",
    "  model: \"{LOCAL_MODEL_NAME}\"\n",
    "\n",
    "dataset:\n",
    "  creation:\n",
    "    num_steps: 15\n",
    "    batch_size: 1\n",
    "    sys_msg: true\n",
    "  template: \"builtin://chatml.py\"\n",
    "\"\"\"\n",
    "\n",
    "with open('konusma_config.yaml', 'w', encoding='utf-8') as f:\n",
    "    f.write(konusma_config)\n",
    "\n",
    "print(\"âœ“ KonuÅŸma konfigÃ¼rasyonu hazÄ±r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Veri Setini Ä°nceleme ve Ä°statistikler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def veri_seti_analizi(dosya_yolu):\n",
    "    \"\"\"\n",
    "    JSONL formatÄ±ndaki veri setini analiz eder\n",
    "    \"\"\"\n",
    "    if not os.path.exists(dosya_yolu):\n",
    "        print(f\"âŒ Dosya bulunamadÄ±: {dosya_yolu}\")\n",
    "        return\n",
    "    \n",
    "    veriler = []\n",
    "    with open(dosya_yolu, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            veriler.append(json.loads(line))\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Veri Seti Analizi: {dosya_yolu}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Toplam Ã¶rnek sayÄ±sÄ±: {len(veriler)}\")\n",
    "    \n",
    "    # Mesaj sayÄ±larÄ±\n",
    "    if veriler and 'messages' in veriler[0]:\n",
    "        mesaj_sayilari = [len(v['messages']) for v in veriler if 'messages' in v]\n",
    "        print(f\"Ortalama mesaj sayÄ±sÄ±: {sum(mesaj_sayilari) / len(mesaj_sayilari):.1f}\")\n",
    "        print(f\"Min mesaj sayÄ±sÄ±: {min(mesaj_sayilari)}\")\n",
    "        print(f\"Max mesaj sayÄ±sÄ±: {max(mesaj_sayilari)}\")\n",
    "    \n",
    "    # Alan isimleri\n",
    "    if veriler:\n",
    "        print(f\"\\nVeri alanlarÄ±: {list(veriler[0].keys())}\")\n",
    "    \n",
    "    # Ä°lk Ã¶rneÄŸi gÃ¶ster\n",
    "    if veriler:\n",
    "        print(\"\\nğŸ“ Ã–rnek veri:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(json.dumps(veriler[0], indent=2, ensure_ascii=False)[:1500])\n",
    "        print(\"...\")\n",
    "    \n",
    "    return veriler\n",
    "\n",
    "# Ã–rnek kullanÄ±m\n",
    "# veri_seti_analizi('dataset_from_txt.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model EÄŸitimi iÃ§in Veri HazÄ±rlÄ±ÄŸÄ±\n",
    "\n",
    "Ãœretilen veriyi model eÄŸitimi iÃ§in kullanmak:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace format iÃ§in\n",
    "!deepfabric format \\\n",
    "  --dataset dataset_from_txt.jsonl \\\n",
    "  --format builtin://trl_sft_tools \\\n",
    "  --output dataset_hf_ready.jsonl\n",
    "\n",
    "print(\"\\nâœ“ HuggingFace TRL iÃ§in hazÄ±r veri seti oluÅŸturuldu\")\n",
    "print(\"\\nBu veri setini ÅŸu ÅŸekilde kullanabilirsiniz:\")\n",
    "print(\"\"\"\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "\n",
    "dataset = load_dataset('json', data_files='dataset_hf_ready.jsonl')\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset['train'],\n",
    "    ...\n",
    ")\n",
    "trainer.train()\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Ä°puÃ§larÄ± ve En Ä°yi Pratikler\n",
    "\n",
    "### ğŸ¯ Kaliteli Veri Ãœretimi Ä°Ã§in:\n",
    "\n",
    "1. **Spesifik Konular SeÃ§in**: \n",
    "   - âŒ \"Programlama\" yerine\n",
    "   - âœ… \"Python'da list comprehension kullanÄ±mÄ± ve performans optimizasyonu\"\n",
    "\n",
    "2. **Prompt'larÄ± Ã–zelleÅŸtirin**:\n",
    "   - `generation_system_prompt` ile veri stilini belirleyin\n",
    "   - Ã–rnekler, aÃ§Ä±klamalar, kod bloklarÄ± gibi isteklerinizi ekleyin\n",
    "\n",
    "3. **Temperature AyarlarÄ±**:\n",
    "   - Konu oluÅŸturma: 0.7-0.8 (daha yaratÄ±cÄ±)\n",
    "   - Veri Ã¼retimi: 0.4-0.6 (daha tutarlÄ±)\n",
    "   - Teknik iÃ§erik: 0.3-0.5 (daha deterministik)\n",
    "\n",
    "4. **KÃ¼Ã§Ã¼k BaÅŸlayÄ±n**:\n",
    "   - Ä°lk denemede `num_steps=5-10` kullanÄ±n\n",
    "   - Kaliteyi kontrol edin\n",
    "   - Sonra artÄ±rÄ±n\n",
    "\n",
    "5. **Batch Size**:\n",
    "   - Lokal API: 1-2 (kaynak sÄ±nÄ±rlÄ±)\n",
    "   - GÃ¼Ã§lÃ¼ API: 4-8 (daha hÄ±zlÄ±)\n",
    "\n",
    "### ğŸ”§ Sorun Giderme:\n",
    "\n",
    "**API BaÄŸlantÄ± HatasÄ±:**\n",
    "```python\n",
    "# API'nizin Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olun\n",
    "import requests\n",
    "response = requests.get(os.environ['OPENAI_BASE_URL'].replace('/v1', '/health'))\n",
    "print(response.status_code)  # 200 olmalÄ±\n",
    "```\n",
    "\n",
    "**DÃ¼ÅŸÃ¼k Kaliteli Veri:**\n",
    "- `generation_system_prompt` daha detaylÄ± yapÄ±n\n",
    "- Temperature'Ä± dÃ¼ÅŸÃ¼rÃ¼n\n",
    "- Daha gÃ¼Ã§lÃ¼ bir model deneyin\n",
    "\n",
    "**YavaÅŸ Ãœretim:**\n",
    "- `batch_size` artÄ±rÄ±n\n",
    "- Daha kÃ¼Ã§Ã¼k bir model kullanÄ±n\n",
    "- `depth` ve `degree` azaltÄ±n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Ã–zet ve Sonraki AdÄ±mlar\n",
    "\n",
    "### ğŸ“ Ã–ÄŸrendikleriniz:\n",
    "\n",
    "âœ… DeepFabric'i kurdunuz ve yapÄ±landÄ±rdÄ±nÄ±z  \n",
    "âœ… Lokal LLM API ile baÄŸlandÄ±nÄ±z  \n",
    "âœ… Basit ve geliÅŸmiÅŸ veri setleri oluÅŸturdunuz  \n",
    "âœ… Txt dosyasÄ±ndan soru-cevap Ã¼rettiniz  \n",
    "âœ… FarklÄ± formatlarÄ± Ã¶ÄŸrendiniz  \n",
    "âœ… Prompt'larÄ± Ã¶zelleÅŸtirdiniz  \n",
    "\n",
    "### ğŸš€ Sonraki AdÄ±mlar:\n",
    "\n",
    "1. **Kendi Verilerinizi OluÅŸturun**: \n",
    "   - Kendi txt dosyalarÄ±nÄ±zÄ± kullanÄ±n\n",
    "   - Domain-spesifik konular seÃ§in\n",
    "\n",
    "2. **Kaliteyi ArtÄ±rÄ±n**:\n",
    "   - FarklÄ± prompt'lar deneyin\n",
    "   - Temperature ayarlarÄ±nÄ± optimize edin\n",
    "   - Ãœretilen veriyi gÃ¶zden geÃ§irin\n",
    "\n",
    "3. **Model EÄŸitimi**:\n",
    "   - ÃœrettiÄŸiniz veri ile modelinizi fine-tune edin\n",
    "   - QLoRA veya LoRA kullanÄ±n\n",
    "   - SonuÃ§larÄ± deÄŸerlendirin\n",
    "\n",
    "4. **Ä°leri Seviye**:\n",
    "   - Ã–zel formatlar yazÄ±n\n",
    "   - AraÃ§ Ã§aÄŸrÄ±larÄ± (tool calling) ekleyin\n",
    "   - Multi-turn konuÅŸmalar oluÅŸturun\n",
    "\n",
    "### ğŸ“š Kaynaklar:\n",
    "\n",
    "- GitHub: https://github.com/lukehinds/deepfabric\n",
    "- DokÃ¼mantasyon: Repository README\n",
    "- Ã–rnekler: `examples/` klasÃ¶rÃ¼\n",
    "\n",
    "---\n",
    "\n",
    "**Ä°yi Ã§alÄ±ÅŸmalar! ğŸ‰**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}