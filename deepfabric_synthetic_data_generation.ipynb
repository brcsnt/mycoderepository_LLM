{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepFabric ile Sentetik Data Ãœretimi\n",
    "\n",
    "Bu notebook, DeepFabric kÃ¼tÃ¼phanesini kullanarak lokal LLM API ile sentetik veri Ã¼retimini gÃ¶stermektedir.\n",
    "\n",
    "## Ä°Ã§indekiler\n",
    "1. Kurulum\n",
    "2. Temel KullanÄ±m\n",
    "3. Lokal LLM ile BaÄŸlantÄ± (Ollama)\n",
    "4. FarklÄ± TÃ¼rde Sentetik Data Ãœretimi\n",
    "5. Ãœretilen Verinin GÃ¶rselleÅŸtirilmesi ve Analizi\n",
    "6. Fine-tuning iÃ§in HazÄ±rlÄ±k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Kurulum\n",
    "\n",
    "### DeepFabric KÃ¼tÃ¼phanesini Kurma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepFabric kÃ¼tÃ¼phanesini GitHub'dan kuruyoruz\n",
    "!pip install git+https://github.com/lukehinds/deepfabric.git -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerekli yardÄ±mcÄ± kÃ¼tÃ¼phaneleri kuruyoruz\n",
    "!pip install datasets pandas matplotlib seaborn pyyaml -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerekli KÃ¼tÃ¼phaneleri Ä°Ã§e Aktarma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# GÃ¶rselleÅŸtirme ayarlarÄ±\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ“ KÃ¼tÃ¼phaneler baÅŸarÄ±yla yÃ¼klendi!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Lokal LLM Kurulumu (Ollama)\n",
    "\n",
    "DeepFabric, lokal LLM kullanÄ±mÄ± iÃ§in Ollama'yÄ± destekliyor. Ã–nce Ollama'nÄ±n kurulu olduÄŸundan emin olun.\n",
    "\n",
    "### Ollama Kurulum TalimatlarÄ±:\n",
    "```bash\n",
    "# Linux iÃ§in:\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "# MacOS iÃ§in:\n",
    "brew install ollama\n",
    "\n",
    "# Ollama'yÄ± baÅŸlatÄ±n:\n",
    "ollama serve\n",
    "\n",
    "# Bir model indirin (Ã¶rnek: Llama 3.1):\n",
    "ollama pull llama3.1:8b\n",
    "# veya daha hafif bir model:\n",
    "ollama pull qwen2.5:3b\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama'nÄ±n Ã§alÄ±ÅŸÄ±p Ã§alÄ±ÅŸmadÄ±ÄŸÄ±nÄ± kontrol et\n",
    "try:\n",
    "    result = subprocess.run(['ollama', 'list'], capture_output=True, text=True)\n",
    "    print(\"Mevcut Ollama Modelleri:\")\n",
    "    print(result.stdout)\n",
    "except FileNotFoundError:\n",
    "    print(\"âš ï¸ Ollama bulunamadÄ±! LÃ¼tfen yukarÄ±daki talimatlarÄ± takip ederek Ollama'yÄ± kurun.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. YapÄ±landÄ±rma DosyasÄ± OluÅŸturma\n",
    "\n",
    "DeepFabric'i YAML yapÄ±landÄ±rma dosyasÄ± ile kullanacaÄŸÄ±z. Bu, daha fazla kontrol saÄŸlar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YapÄ±landÄ±rma dizini oluÅŸtur\n",
    "config_dir = Path('deepfabric_configs')\n",
    "config_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Ã‡Ä±ktÄ± dizini oluÅŸtur\n",
    "output_dir = Path('synthetic_datasets')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"âœ“ Dizinler oluÅŸturuldu!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ã–rnek 1: Basit Chain of Thought (CoT) YapÄ±landÄ±rmasÄ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basit CoT yapÄ±landÄ±rmasÄ±\n",
    "cot_config = {\n",
    "    'topic_tree': {\n",
    "        'provider': 'ollama',\n",
    "        'model': 'llama3.1:8b',  # Modelinizi buraya yazÄ±n\n",
    "        'temperature': 0.7,\n",
    "        'degree': 3,  # Her dÃ¼ÄŸÃ¼mden 3 alt konu Ã¼retilecek\n",
    "        'depth': 2,   # 2 seviye derinlik\n",
    "    },\n",
    "    'data_engine': {\n",
    "        'provider': 'ollama',\n",
    "        'model': 'llama3.1:8b',\n",
    "        'generation_system_prompt': 'Sen yardÄ±mcÄ± bir Python programlama eÄŸitmenisin. Ã–ÄŸrencilere adÄ±m adÄ±m dÃ¼ÅŸÃ¼nmeyi Ã¶ÄŸretiyorsun.',\n",
    "        'temperature': 0.3,\n",
    "        'num_steps': 20,  # 20 Ã¶rnek Ã¼ret\n",
    "        'batch_size': 5,\n",
    "        'max_retries': 3,\n",
    "        'reasoning_template': 'chain_of_thought',\n",
    "    },\n",
    "    'output': {\n",
    "        'formatter': 'builtin://trl_sft_tools',\n",
    "        'sys_msg': True,\n",
    "    }\n",
    "}\n",
    "\n",
    "# YAML dosyasÄ± olarak kaydet\n",
    "cot_config_path = config_dir / 'cot_config.yaml'\n",
    "with open(cot_config_path, 'w', encoding='utf-8') as f:\n",
    "    yaml.dump(cot_config, f, allow_unicode=True)\n",
    "\n",
    "print(f\"âœ“ CoT yapÄ±landÄ±rmasÄ± kaydedildi: {cot_config_path}\")\n",
    "print(\"\\nYapÄ±landÄ±rma iÃ§eriÄŸi:\")\n",
    "print(yaml.dump(cot_config, allow_unicode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sentetik Data Ãœretimi\n",
    "\n",
    "### Ã–rnek 1: Python Programlama iÃ§in CoT Verisi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain of Thought verisi Ã¼ret\n",
    "output_file_cot = output_dir / 'python_cot_dataset.jsonl'\n",
    "\n",
    "cmd = [\n",
    "    'deepfabric', 'generate',\n",
    "    str(cot_config_path),\n",
    "    '--topic-prompt', 'Python Programlama Temelleri: Veri yapÄ±larÄ±, algoritmalar ve fonksiyonlar',\n",
    "    '--output', str(output_file_cot)\n",
    "]\n",
    "\n",
    "print(\"Sentetik veri Ã¼retiliyor... (Bu birkaÃ§ dakika sÃ¼rebilir)\")\n",
    "print(f\"Komut: {' '.join(cmd)}\")\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"Hatalar/UyarÄ±lar:\")\n",
    "    print(result.stderr)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"âœ“ Veri Ã¼retimi tamamlandÄ±! Dosya: {output_file_cot}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ã–rnek 2: Tool Calling (Fonksiyon Ã‡aÄŸÄ±rma) YapÄ±landÄ±rmasÄ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool calling yapÄ±landÄ±rmasÄ±\n",
    "tool_config = {\n",
    "    'topic_tree': {\n",
    "        'provider': 'ollama',\n",
    "        'model': 'llama3.1:8b',\n",
    "        'temperature': 0.8,\n",
    "        'degree': 4,\n",
    "        'depth': 2,\n",
    "    },\n",
    "    'data_engine': {\n",
    "        'provider': 'ollama',\n",
    "        'model': 'llama3.1:8b',\n",
    "        'generation_system_prompt': 'Sen bir API yardÄ±mcÄ±sÄ±sÄ±n. KullanÄ±cÄ±lara doÄŸru fonksiyonlarÄ± seÃ§ip Ã§aÄŸÄ±rmayÄ± Ã¶ÄŸretiyorsun.',\n",
    "        'temperature': 0.2,\n",
    "        'num_steps': 15,\n",
    "        'batch_size': 5,\n",
    "        'reasoning_template': 'tool_calling',\n",
    "    },\n",
    "    'output': {\n",
    "        'formatter': 'builtin://trl_sft_tools',\n",
    "        'sys_msg': True,\n",
    "    }\n",
    "}\n",
    "\n",
    "tool_config_path = config_dir / 'tool_config.yaml'\n",
    "with open(tool_config_path, 'w', encoding='utf-8') as f:\n",
    "    yaml.dump(tool_config, f, allow_unicode=True)\n",
    "\n",
    "print(f\"âœ“ Tool calling yapÄ±landÄ±rmasÄ± kaydedildi: {tool_config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool calling verisi Ã¼ret\n",
    "output_file_tool = output_dir / 'tool_calling_dataset.jsonl'\n",
    "\n",
    "cmd = [\n",
    "    'deepfabric', 'generate',\n",
    "    str(tool_config_path),\n",
    "    '--topic-prompt', 'API ve fonksiyon Ã§aÄŸÄ±rma: VeritabanÄ± sorgularÄ±, dosya iÅŸlemleri ve aÄŸ istekleri',\n",
    "    '--output', str(output_file_tool)\n",
    "]\n",
    "\n",
    "print(\"Tool calling verisi Ã¼retiliyor...\")\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"Hatalar/UyarÄ±lar:\")\n",
    "    print(result.stderr)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"âœ“ Tool calling verisi oluÅŸturuldu: {output_file_tool}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ã–rnek 3: Multi-Turn Conversation (Ã‡ok Turlu KonuÅŸma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-turn conversation yapÄ±landÄ±rmasÄ±\n",
    "multiturn_config = {\n",
    "    'topic_tree': {\n",
    "        'provider': 'ollama',\n",
    "        'model': 'llama3.1:8b',\n",
    "        'temperature': 0.7,\n",
    "        'degree': 3,\n",
    "        'depth': 3,\n",
    "    },\n",
    "    'data_engine': {\n",
    "        'provider': 'ollama',\n",
    "        'model': 'llama3.1:8b',\n",
    "        'generation_system_prompt': 'Sen teknoloji danÄ±ÅŸmanÄ±sÄ±n. KullanÄ±cÄ±larla detaylÄ±, Ã§ok turlu konuÅŸmalar yaparak sorunlarÄ±nÄ± Ã§Ã¶zÃ¼yorsun.',\n",
    "        'temperature': 0.5,\n",
    "        'num_steps': 10,\n",
    "        'batch_size': 3,\n",
    "        'reasoning_template': 'multi_turn',\n",
    "    },\n",
    "    'output': {\n",
    "        'formatter': 'builtin://chatml',\n",
    "        'sys_msg': True,\n",
    "    }\n",
    "}\n",
    "\n",
    "multiturn_config_path = config_dir / 'multiturn_config.yaml'\n",
    "with open(multiturn_config_path, 'w', encoding='utf-8') as f:\n",
    "    yaml.dump(multiturn_config, f, allow_unicode=True)\n",
    "\n",
    "print(f\"âœ“ Multi-turn yapÄ±landÄ±rmasÄ± kaydedildi: {multiturn_config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-turn verisi Ã¼ret\n",
    "output_file_multiturn = output_dir / 'multiturn_dataset.jsonl'\n",
    "\n",
    "cmd = [\n",
    "    'deepfabric', 'generate',\n",
    "    str(multiturn_config_path),\n",
    "    '--topic-prompt', 'YazÄ±lÄ±m geliÅŸtirme sorunlarÄ±: Debugging, optimizasyon ve mimari kararlar',\n",
    "    '--output', str(output_file_multiturn)\n",
    "]\n",
    "\n",
    "print(\"Multi-turn conversation verisi Ã¼retiliyor...\")\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"Hatalar/UyarÄ±lar:\")\n",
    "    print(result.stderr)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"âœ“ Multi-turn verisi oluÅŸturuldu: {output_file_multiturn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Ãœretilen Verinin Analizi ve GÃ¶rselleÅŸtirilmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(file_path):\n",
    "    \"\"\"JSONL dosyasÄ±nÄ± yÃ¼kle\"\"\"\n",
    "    data = []\n",
    "    if not Path(file_path).exists():\n",
    "        print(f\"âš ï¸ Dosya bulunamadÄ±: {file_path}\")\n",
    "        return data\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "def analyze_dataset(data, dataset_name):\n",
    "    \"\"\"Veri setini analiz et\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Toplam Ã¶rnek sayÄ±sÄ±: {len(data)}\")\n",
    "    \n",
    "    if len(data) > 0:\n",
    "        # Ä°lk Ã¶rneÄŸi gÃ¶ster\n",
    "        print(\"\\nğŸ“ Ä°lk Ã–rnek:\")\n",
    "        print(json.dumps(data[0], indent=2, ensure_ascii=False)[:1000] + \"...\")\n",
    "        \n",
    "        # Mesaj uzunluklarÄ±nÄ± hesapla\n",
    "        message_lengths = []\n",
    "        for item in data:\n",
    "            if 'messages' in item:\n",
    "                for msg in item['messages']:\n",
    "                    if 'content' in msg:\n",
    "                        message_lengths.append(len(msg['content']))\n",
    "        \n",
    "        if message_lengths:\n",
    "            print(f\"\\nğŸ“Š Ä°statistikler:\")\n",
    "            print(f\"  - Ortalama mesaj uzunluÄŸu: {sum(message_lengths)/len(message_lengths):.0f} karakter\")\n",
    "            print(f\"  - En kÄ±sa mesaj: {min(message_lengths)} karakter\")\n",
    "            print(f\"  - En uzun mesaj: {max(message_lengths)} karakter\")\n",
    "            print(f\"  - Toplam mesaj sayÄ±sÄ±: {len(message_lengths)}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# TÃ¼m veri setlerini yÃ¼kle ve analiz et\n",
    "datasets = {\n",
    "    'Chain of Thought': output_file_cot,\n",
    "    'Tool Calling': output_file_tool,\n",
    "    'Multi-Turn': output_file_multiturn\n",
    "}\n",
    "\n",
    "loaded_datasets = {}\n",
    "for name, path in datasets.items():\n",
    "    data = load_jsonl(path)\n",
    "    loaded_datasets[name] = analyze_dataset(data, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veri setlerini gÃ¶rselleÅŸtir\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Veri seti boyutlarÄ±\n",
    "dataset_sizes = {name: len(data) for name, data in loaded_datasets.items() if len(data) > 0}\n",
    "if dataset_sizes:\n",
    "    axes[0].bar(dataset_sizes.keys(), dataset_sizes.values(), color=['#3498db', '#e74c3c', '#2ecc71'])\n",
    "    axes[0].set_xlabel('Veri Seti', fontsize=12)\n",
    "    axes[0].set_ylabel('Ã–rnek SayÄ±sÄ±', fontsize=12)\n",
    "    axes[0].set_title('Veri Seti BoyutlarÄ±', fontsize=14, fontweight='bold')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Mesaj uzunluk daÄŸÄ±lÄ±mÄ±\n",
    "all_lengths = []\n",
    "labels = []\n",
    "for name, data in loaded_datasets.items():\n",
    "    if len(data) > 0:\n",
    "        lengths = []\n",
    "        for item in data:\n",
    "            if 'messages' in item:\n",
    "                for msg in item['messages']:\n",
    "                    if 'content' in msg:\n",
    "                        lengths.append(len(msg['content']))\n",
    "        if lengths:\n",
    "            all_lengths.append(lengths)\n",
    "            labels.append(name)\n",
    "\n",
    "if all_lengths:\n",
    "    axes[1].boxplot(all_lengths, labels=labels)\n",
    "    axes[1].set_xlabel('Veri Seti', fontsize=12)\n",
    "    axes[1].set_ylabel('Mesaj UzunluÄŸu (karakter)', fontsize=12)\n",
    "    axes[1].set_title('Mesaj Uzunluk DaÄŸÄ±lÄ±mÄ±', fontsize=14, fontweight='bold')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'dataset_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ“ GÃ¶rselleÅŸtirme kaydedildi: {output_dir / 'dataset_analysis.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ã–rnek MesajlarÄ± Ä°nceleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_example(data, dataset_name, index=0):\n",
    "    \"\"\"Belirli bir Ã¶rneÄŸi gÃ¼zel formatta gÃ¶ster\"\"\"\n",
    "    if len(data) == 0:\n",
    "        print(f\"âš ï¸ {dataset_name} veri seti boÅŸ!\")\n",
    "        return\n",
    "    \n",
    "    if index >= len(data):\n",
    "        index = 0\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Dataset: {dataset_name} - Ã–rnek #{index + 1}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    item = data[index]\n",
    "    \n",
    "    if 'messages' in item:\n",
    "        for i, msg in enumerate(item['messages']):\n",
    "            role = msg.get('role', 'unknown').upper()\n",
    "            content = msg.get('content', '')\n",
    "            \n",
    "            print(f\"[{role}]\")\n",
    "            print(f\"{content}\")\n",
    "            print(f\"\\n{'-'*80}\\n\")\n",
    "    else:\n",
    "        print(json.dumps(item, indent=2, ensure_ascii=False))\n",
    "\n",
    "# Her veri setinden birer Ã¶rnek gÃ¶ster\n",
    "for name, data in loaded_datasets.items():\n",
    "    if len(data) > 0:\n",
    "        display_example(data, name, index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Fine-tuning iÃ§in Veri HazÄ±rlama\n",
    "\n",
    "Ãœretilen veriler direkt olarak HuggingFace TRL SFTTrainer ile kullanÄ±labilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veri setlerini birleÅŸtir (opsiyonel)\n",
    "combined_data = []\n",
    "for name, data in loaded_datasets.items():\n",
    "    combined_data.extend(data)\n",
    "\n",
    "# BirleÅŸtirilmiÅŸ veri setini kaydet\n",
    "combined_output = output_dir / 'combined_dataset.jsonl'\n",
    "with open(combined_output, 'w', encoding='utf-8') as f:\n",
    "    for item in combined_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"âœ“ BirleÅŸtirilmiÅŸ veri seti kaydedildi: {combined_output}\")\n",
    "print(f\"  Toplam Ã¶rnek sayÄ±sÄ±: {len(combined_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace datasets kÃ¼tÃ¼phanesi ile yÃ¼kle\n",
    "try:\n",
    "    dataset = load_dataset('json', data_files=str(combined_output))\n",
    "    print(\"âœ“ Veri seti baÅŸarÄ±yla yÃ¼klendi!\")\n",
    "    print(f\"\\nVeri seti bilgileri:\")\n",
    "    print(dataset)\n",
    "    \n",
    "    # Ã–rnek bir satÄ±r gÃ¶ster\n",
    "    if len(dataset['train']) > 0:\n",
    "        print(\"\\nğŸ“ Ã–rnek bir satÄ±r:\")\n",
    "        print(dataset['train'][0])\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Hata: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Fine-tuning Ã–rnek Kodu (TRL SFTTrainer)\n",
    "\n",
    "Bu kod Ã¼retilen veri ile model fine-tuning yapmak iÃ§in kullanÄ±labilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning iÃ§in Ã¶rnek kod (Ã§alÄ±ÅŸtÄ±rmak iÃ§in gerekli kÃ¼tÃ¼phaneleri kurun)\n",
    "example_training_code = '''\n",
    "# Gerekli kÃ¼tÃ¼phaneler:\n",
    "# pip install transformers trl peft accelerate\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "\n",
    "# Model ve tokenizer yÃ¼kle\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"  # veya baÅŸka bir model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Veri setini yÃ¼kle\n",
    "dataset = load_dataset(\"json\", data_files=\"synthetic_datasets/combined_dataset.jsonl\")\n",
    "\n",
    "# LoRA yapÄ±landÄ±rmasÄ±\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Training parametreleri\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finetuned_model\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    warmup_steps=50,\n",
    ")\n",
    "\n",
    "# Trainer oluÅŸtur\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=2048,\n",
    ")\n",
    "\n",
    "# EÄŸitimi baÅŸlat\n",
    "trainer.train()\n",
    "\n",
    "# Modeli kaydet\n",
    "trainer.save_model(\"./finetuned_model\")\n",
    "'''\n",
    "\n",
    "print(\"Fine-tuning Ã–rnek Kodu:\")\n",
    "print(\"=\"*80)\n",
    "print(example_training_code)\n",
    "\n",
    "# Kodu dosyaya kaydet\n",
    "with open(output_dir / 'example_finetuning.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(example_training_code)\n",
    "\n",
    "print(f\"\\nâœ“ Ã–rnek kod kaydedildi: {output_dir / 'example_finetuning.py'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Ã–zet ve Sonraki AdÄ±mlar\n",
    "\n",
    "### YapÄ±lanlar:\n",
    "1. âœ“ DeepFabric kÃ¼tÃ¼phanesi kuruldu\n",
    "2. âœ“ Lokal LLM (Ollama) ile baÄŸlantÄ± yapÄ±ldÄ±\n",
    "3. âœ“ 3 farklÄ± tÃ¼rde sentetik veri Ã¼retildi:\n",
    "   - Chain of Thought (CoT)\n",
    "   - Tool Calling\n",
    "   - Multi-Turn Conversations\n",
    "4. âœ“ Veri setleri analiz edildi ve gÃ¶rselleÅŸtirildi\n",
    "5. âœ“ Fine-tuning iÃ§in hazÄ±r hale getirildi\n",
    "\n",
    "### Sonraki AdÄ±mlar:\n",
    "\n",
    "1. **Daha Fazla Veri Ãœretimi:**\n",
    "   - `num_steps` parametresini artÄ±rarak daha fazla Ã¶rnek Ã¼retin\n",
    "   - FarklÄ± topic_prompt'lar kullanarak Ã§eÅŸitli konularda veri Ã¼retin\n",
    "   - `degree` ve `depth` parametreleri ile daha karmaÅŸÄ±k konu aÄŸaÃ§larÄ± oluÅŸturun\n",
    "\n",
    "2. **Model Fine-tuning:**\n",
    "   - Ãœretilen veri ile kÃ¼Ã§Ã¼k bir model (Llama-3.2-1B) fine-tune edin\n",
    "   - LoRA veya QLoRA kullanarak verimli eÄŸitim yapÄ±n\n",
    "   - FarklÄ± hyperparameter'lar deneyin\n",
    "\n",
    "3. **Veri Kalitesi Ä°yileÅŸtirme:**\n",
    "   - Temperature deÄŸerlerini ayarlayarak Ã§eÅŸitliliÄŸi kontrol edin\n",
    "   - System prompt'larÄ± Ã¶zelleÅŸtirerek istediÄŸiniz formata uygun veri Ã¼retin\n",
    "   - FarklÄ± reasoning template'leri deneyin\n",
    "\n",
    "4. **DeÄŸerlendirme:**\n",
    "   - Fine-tune edilmiÅŸ modeli test edin\n",
    "   - Benchmark'larda performansÄ±nÄ± Ã¶lÃ§Ã¼n\n",
    "   - Ä°yileÅŸtirme iÃ§in geri bildirim toplayÄ±n\n",
    "\n",
    "### FaydalÄ± Kaynaklar:\n",
    "- DeepFabric GitHub: https://github.com/lukehinds/deepfabric\n",
    "- Ollama: https://ollama.com/\n",
    "- HuggingFace TRL: https://huggingface.co/docs/trl/\n",
    "- PEFT/LoRA: https://huggingface.co/docs/peft/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Temizlik ve BakÄ±m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TÃ¼m Ã¼retilen dosyalarÄ± listele\n",
    "print(\"Ãœretilen Dosyalar:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for file in sorted(output_dir.glob('*')):\n",
    "    size = file.stat().st_size\n",
    "    size_mb = size / (1024 * 1024)\n",
    "    print(f\"  ğŸ“„ {file.name:40s} - {size_mb:8.2f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "total_size = sum(f.stat().st_size for f in output_dir.glob('*'))\n",
    "print(f\"Toplam boyut: {total_size / (1024 * 1024):.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
