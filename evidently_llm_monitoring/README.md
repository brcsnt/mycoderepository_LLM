# ğŸš€ Evidently LLM Monitoring Sistemi

AÃ§Ä±k kaynaklÄ± LLM'leri API Ã¼zerinden kullanÄ±p **Evidently** ile izleyen, analiz eden ve raporlayan komple bir sistem.

## ğŸ“‹ Ã–zellikler

- âœ… Ã‡oklu LLM provider desteÄŸi (Ollama, OpenAI, Groq, Together, Hugging Face)
- âœ… Real-time monitoring ve metrik toplama
- âœ… Evidently ile otomatik rapor oluÅŸturma
- âœ… Response kalitesi analizi
- âœ… Ä°nteraktif sohbet modu
- âœ… Batch processing desteÄŸi
- âœ… HTML ve JSON rapor Ã§Ä±ktÄ±larÄ±

## ğŸ—ï¸ Mimari

```
evidently_llm_monitoring/
â”œâ”€â”€ config.py          # KonfigÃ¼rasyon ayarlarÄ±
â”œâ”€â”€ llm_client.py      # LLM API client'Ä±
â”œâ”€â”€ monitoring.py      # Evidently monitoring modÃ¼lÃ¼
â”œâ”€â”€ main.py           # Ana uygulama
â”œâ”€â”€ .env.example      # Environment variables Ã¶rneÄŸi
â””â”€â”€ README.md         # Bu dosya
```

## ğŸ“¦ Kurulum

### 1. BaÄŸÄ±mlÄ±lÄ±klarÄ± YÃ¼kle

```bash
pip install evidently requests pandas python-dotenv
```

### 2. Environment Variables Ayarla

`.env.example` dosyasÄ±nÄ± `.env` olarak kopyalayÄ±n:

```bash
cp .env.example .env
```

ArdÄ±ndan `.env` dosyasÄ±nÄ± dÃ¼zenleyin:

```bash
# Yerel ve Ã¼cretsiz baÅŸlangÄ±Ã§ iÃ§in
LLM_PROVIDER=ollama

# veya cloud API iÃ§in
# LLM_PROVIDER=groq
# GROQ_API_KEY=your-api-key
```

### 3. LLM Provider Kurulumu

#### Option A: Ollama (Ã–nerilen - Ãœcretsiz & Yerel)

```bash
# Ollama'yÄ± indirin: https://ollama.ai
# Model Ã§ekin
ollama pull llama2

# Ollama'yÄ± baÅŸlatÄ±n
ollama serve
```

#### Option B: Groq (HÄ±zlÄ± & Ucuz Cloud)

```bash
# https://console.groq.com adresinden API key alÄ±n
# .env dosyasÄ±na ekleyin
LLM_PROVIDER=groq
GROQ_API_KEY=gsk_your-api-key-here
```

#### Option C: OpenAI

```bash
# https://platform.openai.com adresinden API key alÄ±n
LLM_PROVIDER=openai
OPENAI_API_KEY=sk-your-api-key-here
```

## ğŸš€ KullanÄ±m

### Ä°nteraktif Mod (Ã–nerilen)

```bash
cd evidently_llm_monitoring
python main.py --mode interactive
```

Ä°nteraktif modda:
- Soru sorun ve cevaplar alÄ±n
- `stats` yazarak istatistikleri gÃ¶rÃ¼n
- `report` yazarak Evidently raporu oluÅŸturun
- `save` yazarak veriyi kaydedin
- `exit` ile Ã§Ä±kÄ±n

### Demo Mod

HazÄ±r sorularla hÄ±zlÄ± test:

```bash
python main.py --mode demo
```

### Batch Mod

Toplu soru iÅŸleme:

```bash
python main.py --mode batch --prompts "Python nedir?" "Machine learning nedir?" "Docker nedir?"
```

### Programatik KullanÄ±m

```python
from llm_client import LLMClient
from monitoring import LLMMonitor

# Client ve monitor baÅŸlat
client = LLMClient()
monitor = LLMMonitor()

# LLM'den cevap al
result = client.generate("Python'da liste comprehension nedir?")

# Monitoring sistemine kaydet
monitor.add_interaction(result)

# Ä°statistikleri gÃ¶ster
monitor.print_statistics()

# Rapor oluÅŸtur
monitor.generate_report()
```

## ğŸ“Š Evidently RaporlarÄ±

Raporlar `./evidently_reports/` dizinine kaydedilir:

- **HTML Rapor**: TarayÄ±cÄ±da gÃ¶rselleÅŸtirme
- **JSON Rapor**: Programatik eriÅŸim iÃ§in

### Rapor Ä°Ã§eriÄŸi

- âœ… **Text Evaluations**: Sentiment, toxicity, text quality
- âœ… **Response Metrics**: Uzunluk, kelime sayÄ±sÄ±, cÃ¼mle sayÄ±sÄ±
- âœ… **Performance Metrics**: Response time, throughput
- âœ… **Quality Analysis**: Ã‡ok kÄ±sa/uzun cevaplar, hatalar

## ğŸ”§ KonfigÃ¼rasyon

### Desteklenen LLM Provider'lar

| Provider | Tip | Maliyet | HÄ±z | Kurulum |
|----------|-----|---------|-----|---------|
| **Ollama** | Yerel | Ãœcretsiz | Orta | Kolay |
| **Groq** | Cloud | Ã‡ok Ucuz | Ã‡ok HÄ±zlÄ± | Ã‡ok Kolay |
| **OpenAI** | Cloud | Orta | HÄ±zlÄ± | Kolay |
| **Together** | Cloud | Ucuz | HÄ±zlÄ± | Kolay |
| **Hugging Face** | Cloud | Ãœcretsiz/Ãœcretli | YavaÅŸ | Orta |

### Model Ã–rnekleri

```bash
# Ollama
LLM_MODEL=llama2
LLM_MODEL=mistral
LLM_MODEL=codellama

# Groq
LLM_MODEL=mixtral-8x7b-32768
LLM_MODEL=llama2-70b-4096

# OpenAI
LLM_MODEL=gpt-3.5-turbo
LLM_MODEL=gpt-4

# Together
LLM_MODEL=mistralai/Mixtral-8x7B-Instruct-v0.1
```

## ğŸ“ˆ Metrikler ve Ä°zleme

Sistem otomatik olarak ÅŸu metrikleri toplar:

1. **Response Quality**
   - Text length
   - Word count
   - Sentence count
   - Sentiment score

2. **Performance**
   - Response time
   - Throughput (requests/second)
   - Error rate

3. **Usage Statistics**
   - Total interactions
   - Provider distribution
   - Model usage

## ğŸ¯ KullanÄ±m SenaryolarÄ±

### 1. Model KarÅŸÄ±laÅŸtÄ±rma

FarklÄ± modelleri test edin ve karÅŸÄ±laÅŸtÄ±rÄ±n:

```bash
# Model 1
LLM_PROVIDER=ollama LLM_MODEL=llama2 python main.py --mode demo

# Model 2
LLM_PROVIDER=groq LLM_MODEL=mixtral-8x7b-32768 python main.py --mode demo
```

### 2. Production Monitoring

CanlÄ± sistemdeki LLM'i izleyin:

```python
from monitoring import LLMMonitor

monitor = LLMMonitor()

# Her API Ã§aÄŸrÄ±sÄ±nda
monitor.add_interaction(llm_result)

# Periyodik rapor
if interaction_count % 100 == 0:
    monitor.generate_report()
```

### 3. Quality Assurance

YanÄ±t kalitesini kontrol edin:

```python
quality = monitor.analyze_quality()

if quality['error_rate'] > 5.0:
    send_alert("LLM error rate too high!")
```

## ğŸ› ï¸ Troubleshooting

### Ollama BaÄŸlantÄ± HatasÄ±

```bash
# Ollama'nÄ±n Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olun
ollama list

# Yeniden baÅŸlatÄ±n
ollama serve
```

### API Key HatasÄ±

```bash
# .env dosyasÄ±nÄ± kontrol edin
cat .env

# DoÄŸru key'in ayarlandÄ±ÄŸÄ±ndan emin olun
echo $OPENAI_API_KEY
```

### Evidently Import HatasÄ±

```bash
# Evidently'yi yÃ¼kleyin
pip install evidently

# Versiyon kontrolÃ¼
python -c "import evidently; print(evidently.__version__)"
```

## ğŸ“š Ek Kaynaklar

- [Evidently DokÃ¼mantasyonu](https://docs.evidentlyai.com/)
- [Ollama](https://ollama.ai)
- [Groq API](https://console.groq.com)
- [OpenAI API](https://platform.openai.com)

## ğŸ¤ KatkÄ±da Bulunma

Pull request'ler kabul edilir. BÃ¼yÃ¼k deÄŸiÅŸiklikler iÃ§in Ã¶nce bir issue aÃ§Ä±n.

## ğŸ“„ Lisans

MIT

## ğŸ‰ BaÅŸarÄ±lÄ± KullanÄ±mlar Ä°Ã§in Ä°puÃ§larÄ±

1. **BaÅŸlangÄ±Ã§**: Ollama ile yerel olarak baÅŸlayÄ±n
2. **ProdÃ¼ksiyon**: Groq gibi hÄ±zlÄ± cloud servislere geÃ§in
3. **Monitoring**: Her 50-100 interaction'da rapor oluÅŸturun
4. **Optimizasyon**: Evidently raporlarÄ±nÄ± kullanarak prompt'larÄ±nÄ±zÄ± iyileÅŸtirin
5. **Backup**: Periyodik olarak veriyi kaydedin (`monitor.save_data()`)

---

**Not**: Bu sistem aÃ§Ä±k kaynaklÄ± LLM'lerin izlenmesi ve deÄŸerlendirilmesi iÃ§in tasarlanmÄ±ÅŸtÄ±r. Production kullanÄ±mÄ± iÃ§in ek gÃ¼venlik Ã¶nlemleri almanÄ±z Ã¶nerilir.
