"""
Evidently LLM Monitoring ModÃ¼lÃ¼
LLM Ã§Ä±ktÄ±larÄ±nÄ± izler ve deÄŸerlendirir.
"""

import os
import json
import pandas as pd
from datetime import datetime
from typing import List, Dict, Any, Optional
from pathlib import Path

try:
    from evidently import ColumnMapping
    from evidently.report import Report
    from evidently.metric_preset import TextEvals
    from evidently.metrics import (
        ColumnSummaryMetric,
        TextDescriptorsDriftMetric,
    )
    from evidently.descriptors import (
        TextLength,
        SentenceCount,
        WordCount,
        Sentiment,
        TriggerWordsPresence,
    )
    EVIDENTLY_AVAILABLE = True
except ImportError:
    EVIDENTLY_AVAILABLE = False
    print("âš ï¸  Evidently kurulu deÄŸil. 'pip install evidently' ile kurabilirsiniz.")

from config import EvidentlyConfig


class LLMMonitor:
    """LLM Ã§Ä±ktÄ±larÄ±nÄ± Evidently ile izler"""

    def __init__(self, reports_dir: Optional[str] = None):
        if not EVIDENTLY_AVAILABLE:
            raise ImportError("Evidently kÃ¼tÃ¼phanesi kurulu deÄŸil!")

        self.reports_dir = Path(reports_dir or EvidentlyConfig.REPORTS_DIR)
        self.reports_dir.mkdir(parents=True, exist_ok=True)

        self.data_history: List[Dict[str, Any]] = []

        print(f"âœ… LLM Monitor baÅŸlatÄ±ldÄ±. Raporlar: {self.reports_dir}")

    def add_interaction(self, interaction: Dict[str, Any]) -> None:
        """
        LLM etkileÅŸimini kaydet

        Args:
            interaction: prompt, response, metrikler iÃ§eren dict
        """
        # Timestamp ekle
        interaction['timestamp'] = datetime.now().isoformat()

        # Temel metrikleri hesapla
        interaction['response_length'] = len(interaction.get('response', ''))
        interaction['prompt_length'] = len(interaction.get('prompt', ''))

        self.data_history.append(interaction)

        print(f"âœ… EtkileÅŸim kaydedildi. Toplam: {len(self.data_history)}")

    def create_dataframe(self) -> pd.DataFrame:
        """Kaydedilen etkileÅŸimlerden DataFrame oluÅŸtur"""
        if not self.data_history:
            return pd.DataFrame()

        return pd.DataFrame(self.data_history)

    def generate_report(self, save: bool = True) -> Optional[Report]:
        """
        Evidently raporu oluÅŸtur

        Args:
            save: Raporu dosyaya kaydet mi?

        Returns:
            Evidently Report objesi
        """
        if not self.data_history:
            print("âš ï¸  HenÃ¼z veri yok, rapor oluÅŸturulamÄ±yor.")
            return None

        df = self.create_dataframe()

        print(f"ðŸ“Š {len(df)} etkileÅŸim iÃ§in rapor oluÅŸturuluyor...")

        # Text descriptors tanÄ±mla
        text_descriptors = [
            TextLength(column_name="response"),
            WordCount(column_name="response"),
            SentenceCount(column_name="response"),
            Sentiment(column_name="response"),
        ]

        # Evidently raporu oluÅŸtur
        report = Report(metrics=[
            TextEvals(column_name="response", descriptors=text_descriptors),
            ColumnSummaryMetric(column_name="response_time"),
            ColumnSummaryMetric(column_name="response_length"),
        ])

        # Raporu Ã§alÄ±ÅŸtÄ±r
        report.run(reference_data=None, current_data=df)

        if save:
            # Raporu HTML olarak kaydet
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_path = self.reports_dir / f"llm_report_{timestamp}.html"

            report.save_html(str(report_path))
            print(f"âœ… Rapor kaydedildi: {report_path}")

            # JSON versiyonu da kaydet
            json_path = self.reports_dir / f"llm_report_{timestamp}.json"
            report.save_json(str(json_path))
            print(f"âœ… JSON rapor kaydedildi: {json_path}")

        return report

    def get_statistics(self) -> Dict[str, Any]:
        """Temel istatistikleri hesapla"""
        if not self.data_history:
            return {}

        df = self.create_dataframe()

        stats = {
            'total_interactions': len(df),
            'avg_response_time': df['response_time'].mean() if 'response_time' in df else None,
            'avg_response_length': df['response_length'].mean(),
            'avg_prompt_length': df['prompt_length'].mean(),
            'providers': df['provider'].value_counts().to_dict() if 'provider' in df else {},
            'models': df['model'].value_counts().to_dict() if 'model' in df else {},
        }

        return stats

    def save_data(self, filename: Optional[str] = None) -> None:
        """Veriyi JSON dosyasÄ±na kaydet"""
        if not self.data_history:
            print("âš ï¸  Kaydedilecek veri yok.")
            return

        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"llm_data_{timestamp}.json"

        filepath = self.reports_dir / filename

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(self.data_history, f, indent=2, ensure_ascii=False)

        print(f"âœ… Veri kaydedildi: {filepath}")

    def load_data(self, filepath: str) -> None:
        """JSON dosyasÄ±ndan veri yÃ¼kle"""
        with open(filepath, 'r', encoding='utf-8') as f:
            self.data_history = json.load(f)

        print(f"âœ… {len(self.data_history)} etkileÅŸim yÃ¼klendi.")

    def print_statistics(self) -> None:
        """Ä°statistikleri konsola yazdÄ±r"""
        stats = self.get_statistics()

        if not stats:
            print("âš ï¸  HenÃ¼z istatistik yok.")
            return

        print("\n" + "=" * 60)
        print("ðŸ“Š LLM MONÄ°TORÄ°NG Ä°STATÄ°STÄ°KLERÄ°")
        print("=" * 60)

        print(f"\nðŸ“ˆ Toplam EtkileÅŸim: {stats['total_interactions']}")

        if stats.get('avg_response_time'):
            print(f"â±ï¸  Ortalama Response Time: {stats['avg_response_time']:.2f}s")

        print(f"ðŸ“ Ortalama Response UzunluÄŸu: {stats['avg_response_length']:.0f} karakter")
        print(f"ðŸ“ Ortalama Prompt UzunluÄŸu: {stats['avg_prompt_length']:.0f} karakter")

        if stats.get('providers'):
            print(f"\nðŸ”Œ KullanÄ±lan Provider'lar:")
            for provider, count in stats['providers'].items():
                print(f"   - {provider}: {count} istek")

        if stats.get('models'):
            print(f"\nðŸ¤– KullanÄ±lan Modeller:")
            for model, count in stats['models'].items():
                print(f"   - {model}: {count} istek")

        print("=" * 60 + "\n")

    def analyze_quality(self, df: Optional[pd.DataFrame] = None) -> Dict[str, Any]:
        """
        Response kalitesini analiz et

        Args:
            df: Analiz edilecek DataFrame (None ise mevcut data kullanÄ±lÄ±r)

        Returns:
            Kalite metrikleri
        """
        if df is None:
            df = self.create_dataframe()

        if df.empty:
            return {}

        quality_metrics = {
            'very_short_responses': len(df[df['response_length'] < 50]),
            'very_long_responses': len(df[df['response_length'] > 1000]),
            'slow_responses': len(df[df['response_time'] > 5.0]) if 'response_time' in df else 0,
            'fast_responses': len(df[df['response_time'] < 1.0]) if 'response_time' in df else 0,
        }

        # Hata oranÄ±
        if 'error' in df.columns:
            quality_metrics['error_rate'] = (df['error'].notna().sum() / len(df)) * 100
        else:
            quality_metrics['error_rate'] = 0.0

        return quality_metrics


if __name__ == "__main__":
    # Test
    print("ðŸ§ª Monitoring modÃ¼lÃ¼ test ediliyor...\n")

    monitor = LLMMonitor()

    # Ã–rnek etkileÅŸimler
    test_interactions = [
        {
            'prompt': 'Python nedir?',
            'response': 'Python, yÃ¼ksek seviyeli, yorumlamalÄ± bir programlama dilidir.',
            'response_time': 1.2,
            'provider': 'test',
            'model': 'test-model'
        },
        {
            'prompt': 'Machine learning nedir?',
            'response': 'Machine learning, bilgisayarlarÄ±n deneyimlerden Ã¶ÄŸrenmesini saÄŸlayan yapay zeka dalÄ±dÄ±r.',
            'response_time': 1.5,
            'provider': 'test',
            'model': 'test-model'
        }
    ]

    for interaction in test_interactions:
        monitor.add_interaction(interaction)

    monitor.print_statistics()

    print("\nâœ… Monitoring modÃ¼lÃ¼ test tamamlandÄ±!")
