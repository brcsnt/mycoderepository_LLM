{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Llama Modelini bitsandbytes ile 8-Bit/4-Bit Kuantizasyon + Kaydetme\n",
        "\n",
        "Bu not defteri, [*bitsandbytes*](https://github.com/TimDettmers/bitsandbytes) kullanarak *Hugging Face Transformers* üzerinden bir Llama modelini 8-bit veya 4-bit formatında **yükleyip** sonrasında **kaydetmeyi** gösterir.\n",
        "\n",
        "## İçindekiler\n",
        "1. [Kurulumlar](#kurulum)\n",
        "2. [Modelin 8-Bit Olarak Yüklenmesi ve Kaydedilmesi](#8bit)\n",
        "3. [Modelin 4-Bit Olarak Yüklenmesi ve Kaydedilmesi](#4bit)\n",
        "4. [Perplexity/Accuracy Testi (Opsiyonel)](#perplexity)\n",
        "5. [Örnek Üretim (Inference)](#inference)\n",
        "\n",
        "> **Uyarı**: Bu not defterinin GPU (CUDA) destekli bir ortamda çalıştığından emin olunuz. `bitsandbytes`, çoğunlukla NVIDIA GPU ortamlarında sorunsuzdur."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kurulum"
      },
      "source": [
        "## 1. Kurulumlar <a id='kurulum'></a>\n",
        "\n",
        "Bu hücreyi çalıştırarak gerekli paketleri yükleyeceğiz. Ardından runtime'ı yeniden başlatmanız önerilir.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%bash\n",
        "pip install --upgrade pip\n",
        "pip install torch\n",
        "pip install transformers accelerate\n",
        "pip install bitsandbytes  # 4-bit ve 8-bit için\n",
        "\n",
        "# İsteğe bağlı olarak evaluate paketiyle perplexity ölçümü yapmak isterseniz:\n",
        "pip install datasets evaluate\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bit"
      },
      "source": [
        "## 2. Modelin 8-Bit Olarak Yüklenmesi ve Kaydedilmesi <a id='8bit'></a>\n",
        "\n",
        "Aşağıdaki kod, `bitsandbytes`ı kullanarak bir Llama modelini 8-bit formatında yükler **ve** sonrasında `save_pretrained` ile diske kaydeder.\n",
        "\n",
        "### Adımlar\n",
        "1. **Tokenizer** ve **Model**’i `from_pretrained` ile indir veya yerel dizinden yükle.\n",
        "2. `load_in_8bit=True` parametresini ekle.\n",
        "3. `device_map='auto'` diyerek, GPU kullanılabilirliğine göre otomatik atama yap.\n",
        "4. Kaydetme adımı: `model_8bit.save_pretrained(\"my-llama-8bit\")` + `tokenizer.save_pretrained(\"my-llama-8bit\")`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Örneğin, Llama 2 7B model (veya Llama 3.3 / 3.8B benzeri bir model) için:\n",
        "model_name = \"meta-llama/Llama-2-7b-hf\"  # Kendi model yolunuzu girin.\n",
        "\n",
        "# 1. Tokenizer yükle\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# 2. Modeli 8-bit olarak yükle\n",
        "model_8bit = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    load_in_8bit=True,       # 8-bit quantization\n",
        "    device_map=\"auto\"       # GPU otomatik seçilir\n",
        ")\n",
        "\n",
        "print(\"Model successfully loaded in 8-bit!\")\n",
        "\n",
        "# 3. Kaydetme\n",
        "# Bu adım, 8-bit modelin parametrelerini local dizine kaydeder.\n",
        "# Not: Tekrar yükleme esnasında load_in_8bit parametresini koyman gerekebilir.\n",
        "model_8bit.save_pretrained(\"my-llama-8bit\")\n",
        "tokenizer.save_pretrained(\"my-llama-8bit\")\n",
        "print(\"\\nModel & tokenizer saved to 'my-llama-8bit' folder!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bit"
      },
      "source": [
        "## 3. Modelin 4-Bit Olarak Yüklenmesi ve Kaydedilmesi <a id='4bit'></a>\n",
        "\n",
        "Eğer 8-bit’ten daha az VRAM kullanımı istiyorsanız, 4-bit kuantizasyon deneyebilirsiniz. **Ancak**, 4-bit şu anda daha deneysel ve bazı modellerde hata verebilir.\n",
        "\n",
        "### Adımlar\n",
        "1. Tekrar `AutoModelForCausalLM.from_pretrained` çağır.\n",
        "2. Bu kez `load_in_4bit=True` parametresini kullan.\n",
        "3. Kaydetme adımında yine `model_4bit.save_pretrained(\"my-llama-4bit\")`.\n",
        "\n",
        "> **Not**: 4-bit’te bazı GPU’larda katman yükleme hataları yaşanabiliyor. Sürümlerin (CUDA, PyTorch, bitsandbytes) uyumlu olması önemli."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1. Tokenizer yeniden yüklenebilir veya aynısını kullanabilirsiniz.\n",
        "tokenizer_4bit = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# 2. Modeli 4-bit olarak yükle\n",
        "model_4bit = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    load_in_4bit=True,       # 4-bit quantization\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "print(\"Model successfully loaded in 4-bit!\")\n",
        "\n",
        "# 3. Kaydetme\n",
        "model_4bit.save_pretrained(\"my-llama-4bit\")\n",
        "tokenizer_4bit.save_pretrained(\"my-llama-4bit\")\n",
        "\n",
        "print(\"\\nModel & tokenizer saved to 'my-llama-4bit' folder!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "perplexity"
      },
      "source": [
        "## 4. Perplexity veya Accuracy Testi (Opsiyonel) <a id='perplexity'></a>\n",
        "\n",
        "Makaledeki yaklaşıma benzer şekilde, modele basit bir **perplexity** ölçümü uygulayabilirsiniz. Bu, modelin \"metni ne kadar iyi tahmin ettiği\"nin bir göstergesidir.\n",
        "\n",
        "### Adımlar\n",
        "1. [Hugging Face `datasets`](https://github.com/huggingface/datasets) kütüphanesinden bir test verisi çekin (ör. `wikitext`).\n",
        "2. [Hugging Face `evaluate`](https://github.com/huggingface/evaluate) ile perplexity metriğini hesaplayın.\n",
        "\n",
        "Aşağıdaki örnek, WikiText-2 ile kabaca bir test yapar (metinde *padding* / *truncation* vb. ayarlar basit tutulmuştur). **Gerçek senaryolarda** *data collator* ve ileri seviye ayarlara ihtiyaç duyabilirsiniz."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import evaluate\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Perplexity metriği\n",
        "perplexity_metric = evaluate.load(\"perplexity\")\n",
        "\n",
        "# WikiText-2 test veri setini yükleyelim (küçük bir örnek)\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test[:1%]\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "result_8bit = perplexity_metric.compute(\n",
        "    model=model_8bit,\n",
        "    tokenizer=tokenizer,\n",
        "    inputs=tokenized_dataset\n",
        ")\n",
        "\n",
        "print(\"Perplexity (8-bit model):\", result_8bit[\"perplexity\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inference"
      },
      "source": [
        "## 5. Örnek Üretim (Inference) <a id='inference'></a>\n",
        "\n",
        "Son olarak, modelden bir prompt’a karşılık nasıl çıktı alabileceğinizi gösteren basit bir örnek.\n",
        "\n",
        "### Adımlar\n",
        "1. `generate` fonksiyonunu çağırın.\n",
        "2. `temperature`, `max_new_tokens` gibi parametreleri ihtiyacınıza göre ayarlayın.\n",
        "3. Çıktıyı `tokenizer.decode(...)` ile string’e çevirin."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "prompt = \"Bana kısaca kuantizasyonun ne olduğunu anlat.\"  # Örnek Türkçe prompt\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model_8bit.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    generated_tokens = model_8bit.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=100,\n",
        "        temperature=0.7,\n",
        "        do_sample=True\n",
        "    )\n",
        "\n",
        "output = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
        "print(\"\\n--- Model Output (8-bit) ---\\n\", output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Aynı şekilde 4-bit model için örnek:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "prompt_4bit = \"Kuantizasyon yöntemleri ve avantajları nelerdir?\"\n",
        "inputs_4bit = tokenizer_4bit(prompt_4bit, return_tensors=\"pt\").to(model_4bit.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    generated_tokens_4bit = model_4bit.generate(\n",
        "        **inputs_4bit,\n",
        "        max_new_tokens=100,\n",
        "        temperature=0.7,\n",
        "        do_sample=True\n",
        "    )\n",
        "\n",
        "output_4bit = tokenizer_4bit.decode(generated_tokens_4bit[0], skip_special_tokens=True)\n",
        "print(\"\\n--- Model Output (4-bit) ---\\n\", output_4bit)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sonuç ve Öneriler\n",
        "\n",
        "- Bu not defterinde, **bitsandbytes** kütüphanesi ile **8-bit** ve **4-bit** kuantizasyonun nasıl yapılacağını ve sonrasında bu modellerin nasıl kaydedileceğini gösterdik.\n",
        "- 8-bit kuantizasyon, genelde **daha stabil** sonuç verir. 4-bit ile VRAM daha da düşer ama **hatalar** veya **tahmin kalitesinde** biraz kayıp gözlemlenebilir.\n",
        "- Kaydettiğin klasörden tekrar yüklerken `from_pretrained(\"my-llama-8bit\", load_in_8bit=True)` gibi bir kodla modele erişebilirsin (aynı parametreleri ekleyerek).\n",
        "- Daha gelişmiş veya kalıcı (offline) kuantizasyon için [**GPTQ**](https://github.com/PanQiWei/Auto-GPTQ) veya [**llama.cpp**](https://github.com/ggerganov/llama.cpp) yöntemleri de kullanılabilir.\n",
        "\n",
        "### Kaynaklar\n",
        "- [Towards Data Science: Quantize Llama 3.8B with bitsandbytes](https://towardsdatascience.com/quantize-llama-3-8b-with-bitsandbytes-to-preserve-its-accuracy-e84283b233f7)\n",
        "- [bitsandbytes GitHub Repo](https://github.com/TimDettmers/bitsandbytes)\n",
        "- [Transformers Belgeleri](https://huggingface.co/docs/transformers)\n",
        "- [Evaluate Kütüphanesi (perplexity)](https://github.com/huggingface/evaluate)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
