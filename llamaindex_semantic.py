# -*- coding: utf-8 -*-
"""llamaindex_semantic.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Wi9O8ti8oyZ3yJ1Oj231rRT0TSd-sv1Z
"""

!pip install llama-index
!pip install sentence-transformers
!pip install langchain
!pip install llama-index-embeddings-openai
#!pip install langchain-community

from llama_index.core import SimpleDirectoryReader

# load documents
#ocuments = SimpleDirectoryReader(input_files=["/content/pg_essay.txt"]).load_data()
documents = SimpleDirectoryReader(input_files=["/content/YGYO 2023 YILLIK FAAL_YET RAPORU.pdf"]).load_data()
print(documents)

# Gerekli kütüphanelerin import edilmesi
from llama_index.core.node_parser import (
    SentenceSplitter,
    SemanticSplitterNodeParser,
)
from langchain.embeddings import HuggingFaceEmbeddings
from llama_index.embeddings.langchain import LangchainEmbedding

from langchain.embeddings import HuggingFaceEmbeddings
from sentence_transformers import SentenceTransformer

# Sentence Transformers modelini yükleme
model_name = 'all-MiniLM-L6-v2'

# HuggingFaceEmbeddings kullanarak embedding modeli oluşturma
hf_embeddings = HuggingFaceEmbeddings(model_name=model_name)

# LangchainEmbedding kullanarak embeddings modelini sarmalama
embedding_model = LangchainEmbedding(hf_embeddings)

# SemanticSplitterNodeParser'ı oluşturma
splitter = SemanticSplitterNodeParser(
    buffer_size=1,
    breakpoint_percentile_threshold=95,
    embed_model=embedding_model
)

# Alternatif olarak, temel SentenceSplitter
# base_splitter = SentenceSplitter(chunk_size=512)

print("OK")

nodes = splitter.get_nodes_from_documents(documents)

# Parçaları ekrana yazdırma
for i, node in enumerate(nodes):
    print(f"Parça {i+1}:\n{node.get_content()}\n")

# Gerekli kütüphanelerin import edilmesi
from llama_index.core.node_parser import (
    SentenceSplitter,
    SemanticSplitterNodeParser,
)
from langchain.embeddings import HuggingFaceEmbeddings
from llama_index.embeddings.langchain import LangchainEmbedding

from langchain.embeddings import HuggingFaceEmbeddings
from sentence_transformers import SentenceTransformer

# Sentence Transformers modelini yükleme
model_name = 'paraphrase-multilingual-MiniLM-L12-v2'

# HuggingFaceEmbeddings kullanarak embedding modeli oluşturma
hf_embeddings = HuggingFaceEmbeddings(model_name=model_name)

# LangchainEmbedding kullanarak embeddings modelini sarmalama
embedding_model = LangchainEmbedding(hf_embeddings)

# SemanticSplitterNodeParser'ı oluşturma
splitter = SemanticSplitterNodeParser(
    buffer_size=1,
    breakpoint_percentile_threshold=95,
    embed_model=embedding_model
)

# Alternatif olarak, temel SentenceSplitter
# base_splitter = SentenceSplitter(chunk_size=512)

print("OK")

nodes = splitter.get_nodes_from_documents(documents)

# Parçaları ekrana yazdırma
for i, node in enumerate(nodes):
    print(f"Parça {i+1}:\n{node.get_content()}\n")

