import pandas as pd
import numpy as np
from transformers import pipeline

# Load the Excel file
df = pd.read_excel('comments.xlsx')

# Ensure there's a column named 'Comment' in the dataframe
comments = df['Comment'].dropna()

# Filter out comments less than 10 characters
valid_comments = comments[comments.str.len() > 10]

# Collect exceptions (comments less than or equal to 10 characters)
exceptions = comments[comments.str.len() <= 10]


# Split the valid comments into chunks of 500
chunks = [valid_comments[i:i + 500] for i in range(0, valid_comments.shape[0], 500)]

def categorize_comments(chunk):
    # Initialize the model pipeline (placeholder for LLaMA 3.1)
    # Note: Replace 'model_name' with the actual model name or path
    model = pipeline('text-generation', model='model_name')

    results = []

    for comment in chunk:
        # Prepare the prompts
        system_prompt = (
            "You are an AI language model tasked with analyzing customer comments. Your goal is to categorize each "
        "comment into up to three relevant categories based on its content. The categories should be concise, "
        "descriptive, and in Turkish. If a category is not applicable, denote it as 'nan'. Ignore comments that "
        "are empty or less than 10 characters long, and collect them separately.\n\n"
        "Here are some examples:\n\n"
        "Comment: \"Şubeye gittiğimde çok sıra bekliyorum. Telefonda geri dönüş alamıyorum ulaşamıyorum. Bankaya "
        "gittiğimde bekleme süresi çok fazla\"\n"
        "Output:\n"
        "{\n"
        "    \"CATEGORY 1\": \"Kanal - Şube - Bekleme süresi uzun\",\n"
        "    \"CATEGORY 2\": \"Kanal - Çağrı Merkezi - Müşteri temsilcisine bağlanamama\",\n"
        "    \"CATEGORY 3\": \"nan\"\n"
        "}\n\n"
        "Comment: \"ATM'den para çekemiyorum.\"\n"
        "Output:\n"
        "{\n"
        "    \"CATEGORY 1\": \"Kanal - ATM - Para çekme sorunu\",\n"
        "    \"CATEGORY 2\": \"nan\",\n"
        "    \"CATEGORY 3\": \"nan\"\n"
        "}\n\n"
        "Comment: \"Mobil uygulama çok yavaş çalışıyor.\"\n"
        "Output:\n"
        "{\n"
        "    \"CATEGORY 1\": \"Kanal - Mobil Uygulama - Performans Sorunu\",\n"
        "    \"CATEGORY 2\": \"nan\",\n"
        "    \"CATEGORY 3\": \"nan\"\n"
        "}\n\n"
        "Now, please analyze new comments in the same way."
        )

        user_prompt = (
            f"Please analyze the following comment and provide up to three categories it belongs to. "
            f"Format the output as a JSON object with keys \"CATEGORY 1\", \"CATEGORY 2\", and \"CATEGORY 3\". "
            f"Ensure the categories are in Turkish.\n\nComment: \"{comment}\"\nOutput:"
        )

        # Generate the response
        response = model(f"{system_prompt}\n\n{user_prompt}", max_length=500)
        
        # Extract the generated text
        generated_text = response[0]['generated_text']
        
        # Parse the JSON output
        try:
            # Find the JSON object in the generated text
            json_start = generated_text.find('{')
            json_end = generated_text.rfind('}') + 1
            json_str = generated_text[json_start:json_end]
            categories = pd.json.loads(json_str)
        except:
            # If parsing fails, assign nan
            categories = {"CATEGORY 1": np.nan, "CATEGORY 2": np.nan, "CATEGORY 3": np.nan}
        
        results.append({
            "Comment": comment,
            "CATEGORY 1": categories.get("CATEGORY 1", np.nan),
            "CATEGORY 2": categories.get("CATEGORY 2", np.nan),
            "CATEGORY 3": categories.get("CATEGORY 3", np.nan)
        })
    
    return pd.DataFrame(results)



processed_chunks = []

for chunk in chunks:
    df_chunk = categorize_comments(chunk)
    processed_chunks.append(df_chunk)


# Concatenate all processed chunks into a single DataFrame
df_all = pd.concat(processed_chunks, ignore_index=True)



# Extract all unique categories from CATEGORY 1, 2, and 3
categories = pd.unique(df_all[['CATEGORY 1', 'CATEGORY 2', 'CATEGORY 3']].values.ravel('K'))
categories = [cat for cat in categories if pd.notna(cat)]

# Create a mapping of similar categories (this step may require NLP techniques or manual mapping)
# For simplicity, we'll assume categories are already standardized

# Add category columns to the DataFrame
for category in categories:
    df_all[category] = df_all[['CATEGORY 1', 'CATEGORY 2', 'CATEGORY 3']].apply(lambda x: category in x.values, axis=1)



# The final DataFrame now contains binary columns for each category
# You can sum these columns to get counts per category




def unify_categories(categories_list):
    # Initialize the model pipeline
    model = pipeline('text-generation', model='model_name')

    # Prepare the prompts
    system_prompt = (
        "You are an AI language model that unifies and standardizes categories from multiple analyses. "
        "Your task is to analyze the provided categories and merge similar ones into a unified set. "
        "Provide the final list of unified categories in Turkish.\n\n"
        "Here are some examples:\n\n"
        "Original Categories:\n"
        "- \"Kanal - Şube - Bekleme süresi uzun\"\n"
        "- \"Kanal - Şube - Bekleme zamanı fazla\"\n\n"
        "Unified Category:\n"
        "- \"Kanal - Şube - Bekleme Süresi Uzun\"\n\n"
        "Original Categories:\n"
        "- \"Kanal - Çağrı Merkezi - Müşteri temsilcisine bağlanamama\"\n"
        "- \"Kanal - Çağrı Merkezi - Ulaşım Sorunu\"\n\n"
        "Unified Category:\n"
        "- \"Kanal - Çağrı Merkezi - Ulaşılamıyor\"\n\n"
        "Now, please process the categories provided in the same way."
    )

    user_prompt = (
       f"Given the following list of categories extracted from previous analyses:\n\n{categories_text}\n\n"
        "Please analyze these categories and merge any that are similar or redundant. Provide a final list of unified categories in Turkish."
    
    )

    # Generate the response
    response = model(f"{system_prompt}\n\n{user_prompt}", max_length=1000)
    
    # Extract the generated text
    unified_categories_text = response[0]['generated_text']
    
    # Parse the list of categories from the generated text
    # This step may require custom parsing logic depending on the output format
    unified_categories = parse_categories(unified_categories_text)
    
    return unified_categories

def parse_categories(text):
    # Implement parsing logic to extract categories from the text
    # This is a placeholder function
    categories = text.split('\n')
    categories = [cat.strip('- ') for cat in categories if cat.strip()]
    return categories


# Get the list of all categories
all_categories = df_all[['CATEGORY 1', 'CATEGORY 2', 'CATEGORY 3']].values.ravel()
all_categories = [cat for cat in all_categories if pd.notna(cat)]
all_categories = list(set(all_categories))

# Use the model to unify categories
unified_categories = unify_categories(all_categories)

# Create a mapping from original categories to unified categories
# This may require manual intervention or additional NLP processing
category_mapping = {original: unified for original, unified in zip(all_categories, unified_categories)}

# Apply the mapping to the DataFrame
df_all['Unified CATEGORY 1'] = df_all['CATEGORY 1'].map(category_mapping)
df_all['Unified CATEGORY 2'] = df_all['CATEGORY 2'].map(category_mapping)
df_all['Unified CATEGORY 3'] = df_all['CATEGORY 3'].map(category_mapping)



# Now, df_all contains the unified categories
# You can perform further analysis or export the DataFrame

# Export to Excel
df_all.to_excel('categorized_comments.xlsx', index=False)


