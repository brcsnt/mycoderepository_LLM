# -*- coding: utf-8 -*-
"""app_ai.ipynb adlÄ± not defterinin kopyasÄ±

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LurhiR35H2fWQRurKZdqLabVc2U1YeO2
"""

# 1. Colab'da gerekli paketleri yÃ¼kle
!pip install transformers torch ipywidgets accelerate

# AI MÃ¼ÅŸteri Destek Sistemi - Google Colab Notebook
# Hugging Face LLM entegrasyonlu versiyon

import os
import sys
import logging
import json
from pathlib import Path
from IPython.display import display, HTML, clear_output
import ipywidgets as widgets
from typing import Dict, List, Optional, Any

# Logging setup
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ColabConfigManager:
    """Google Colab iÃ§in optimize edilmiÅŸ config manager"""

    def __init__(self):
        self.load_config()

    def load_config(self):
        """Configuration ayarlarÄ±"""
        # Hugging Face token - Colab secrets'dan al
        try:
            from google.colab import userdata
            self.hf_token = userdata.get("HUGGINGFACE_TOKEN")
            print("Token Colab secrets'dan alÄ±ndÄ±")
        except:
            self.hf_token = os.getenv("HUGGINGFACE_TOKEN", "")
            if self.hf_token:
                print("Token environment'dan alÄ±ndÄ±")
            else:
                print("Hugging Face token bulunamadÄ±")

        # Model ayarlarÄ± - Colab iÃ§in optimize edilmiÅŸ
        self.model_id = "google/gemma-2-2b-it"
        # Alternatif modeller:
        # "microsoft/DialoGPT-small" - En hafif
        # "facebook/blenderbot-400M-distill" - KonuÅŸma iÃ§in iyi
        # "google/flan-t5-base" - Instruction following

        self.max_new_tokens = 512
        self.temperature = 0.3
        self.use_real_llm = bool(self.hf_token)  # Token varsa gerÃ§ek model kullan

        # Dil ayarlarÄ±
        self.supported_languages = ["tr", "en"]
        self.default_language = "tr"

        # Colab path yapÄ±sÄ±
        self.base_path = "/content"
        self.data_path = "/content/data"

        # Dizinleri oluÅŸtur
        os.makedirs(self.data_path, exist_ok=True)

    def get_hf_token(self) -> str:
        return self.hf_token

    def get_model_id(self) -> str:
        return self.model_id

    def get_max_tokens(self) -> int:
        return self.max_new_tokens

    def get_temperature(self) -> float:
        return self.temperature


class ColabLLMService:
    """Google Colab iÃ§in optimize edilmiÅŸ LLM servisi"""

    def __init__(self, config: ColabConfigManager):
        self.config = config
        self.model = None
        self.tokenizer = None
        self.device = None
        self.initialized = False

        if config.use_real_llm:
            self.initialize_model()

    def initialize_model(self):
        """Model ve tokenizer'Ä± yÃ¼kle"""
        try:
            import torch
            from transformers import AutoTokenizer, AutoModelForCausalLM

            print("Model yÃ¼kleniyor...")

            # GPU kontrolÃ¼
            if torch.cuda.is_available():
                self.device = "cuda"
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
                print(f"GPU bulundu: {gpu_memory:.1f}GB")

                # Memory'ye gÃ¶re model seÃ§imi
                if gpu_memory < 8:
                    print("DÃ¼ÅŸÃ¼k GPU memory, kÃ¼Ã§Ã¼k model kullanÄ±lÄ±yor")
                    self.config.model_id = "google/gemma-2-2b-it"
            else:
                self.device = "cpu"
                print("GPU bulunamadÄ±, CPU kullanÄ±lÄ±yor")
                self.config.model_id = "google/gemma-2-2b-it"

            # Tokenizer yÃ¼kle
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.config.model_id,
                token=self.config.hf_token,
                trust_remote_code=True
            )

            # Pad token kontrolÃ¼
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

            # Model yÃ¼kle
            self.model = AutoModelForCausalLM.from_pretrained(
                self.config.model_id,
                token=self.config.hf_token,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                device_map="auto" if self.device == "cuda" else None,
                trust_remote_code=True
            )

            if self.device == "cpu":
                self.model = self.model.to(self.device)

            self.initialized = True
            print(f"Model baÅŸarÄ±yla yÃ¼klendi: {self.config.model_id}")

        except Exception as e:
            print(f"Model yÃ¼kleme hatasÄ±: {e}")
            print("Mock mode'a geÃ§iliyor...")
            self.initialized = False

    def generate_response(self, prompt: str, max_tokens: int = 256) -> str:
        """Response Ã¼ret"""
        if not self.initialized or not self.model:
            return self._mock_response(prompt)

        try:
            import torch

            # Input hazÄ±rla
            inputs = self.tokenizer.encode(prompt + self.tokenizer.eos_token, return_tensors="pt")
            inputs = inputs.to(self.device)

            # Generate
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_length=inputs.shape[1] + max_tokens,
                    temperature=self.config.temperature,
                    do_sample=True,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    no_repeat_ngram_size=2
                )

            # Decode
            response = self.tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)
            return response.strip()

        except Exception as e:
            print(f"Generation hatasÄ±: {e}")
            return self._mock_response(prompt)

    def _mock_response(self, prompt: str) -> str:
        """Mock response (model Ã§alÄ±ÅŸmazsa)"""
        if "problem" in prompt.lower() or "sorun" in prompt.lower():
            return "Sorununuzu anlÄ±yorum. Size yardÄ±mcÄ± olmaya Ã§alÄ±ÅŸacaÄŸÄ±m."
        elif "?" in prompt:
            return "Bu konuda daha fazla bilgiye ihtiyacÄ±m var."
        else:
            return "AnlayÄ±ÅŸÄ±nÄ±z iÃ§in teÅŸekkÃ¼rler. Devam edelim."

    def extract_information_with_llm(self, text: str, questions: List[Dict], language: str) -> Dict:
        """LLM ile bilgi Ã§Ä±karÄ±mÄ±"""
        if not self.initialized:
            return self._mock_extraction(text, questions)

        # Prompt oluÅŸtur
        questions_text = "\n".join([f"- {q['key']}: {q['question']}" for q in questions])

        if language == "tr":
            prompt = f"""Metinden bilgi Ã§Ä±kar:
Metin: "{text}"

Sorular:
{questions_text}

Sadece metinde aÃ§Ä±kÃ§a belirtilen bilgileri Ã§Ä±kar. Yoksa "unknown" yaz.
JSON formatÄ±nda yanÄ±tla.
"""
        else:
            prompt = f"""Extract information from text:
Text: "{text}"

Questions:
{questions_text}

Extract only clearly stated information. Use "unknown" if not found.
Respond in JSON format.
"""

        response = self.generate_response(prompt, 256)

        # JSON parse et
        try:
            import json
            import re

            # JSON bul
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                return json.loads(json_str)
            else:
                return self._mock_extraction(text, questions)

        except:
            return self._mock_extraction(text, questions)

    def _mock_extraction(self, text: str, questions: List[Dict]) -> Dict:
        """Mock bilgi Ã§Ä±karÄ±mÄ±"""
        extracted = {}
        text_lower = text.lower()

        for question in questions:
            key = question['key']

            # Basit pattern matching
            if any(word in text_lower for word in ['atm', 'bankamatik']):
                if key in ['lokasyon', 'location']:
                    extracted[key] = "ATM lokasyonu belirtilmedi"
                elif key in ['problem_tipi', 'problem_type']:
                    extracted[key] = "ATM sorunu"
                else:
                    extracted[key] = "unknown"
            elif any(word in text_lower for word in ['kart', 'card']):
                if key in ['problem_tipi', 'problem_type']:
                    extracted[key] = "Kart sorunu"
                else:
                    extracted[key] = "unknown"
            elif any(word in text_lower for word in ['para', 'money', 'tl', 'lira', 'dollar']):
                if key in ['para_miktari', 'amount', 'tutar']:
                    import re
                    numbers = re.findall(r'\d+', text)
                    if numbers:
                        extracted[key] = f"{numbers[0]} TL"
                    else:
                        extracted[key] = "Miktar belirtilmedi"
                else:
                    extracted[key] = "unknown"
            else:
                extracted[key] = "unknown"

        return extracted


class ColabAISupport:
    """Google Colab iÃ§in AI Destek Sistemi - GerÃ§ek LLM Entegrasyonlu"""

    def __init__(self):
        print("AI Destek Sistemi baÅŸlatÄ±lÄ±yor...")

        self.config = ColabConfigManager()
        self.llm_service = ColabLLMService(self.config)

        self.conversation_history = []
        self.current_conversation_id = None
        self.current_language = "tr"
        self.selected_category = None
        self.conversation_state = "idle"
        self.extracted_info = {}
        self.missing_questions = []
        self.current_question_index = 0

        # Kategoriler
        self.categories = {
            "tr": {
                "ATM": [
                    {"key": "problem_tipi", "question": "ATM'de yaÅŸadÄ±ÄŸÄ±nÄ±z problem ne?"},
                    {"key": "para_miktari", "question": "Ne kadar paranÄ±z sÄ±kÄ±ÅŸtÄ±?"},
                    {"key": "lokasyon", "question": "Hangi ATM'de problem yaÅŸadÄ±nÄ±z?"},
                    {"key": "tarih_saat", "question": "Problem hangi tarih ve saatte yaÅŸandÄ±?"}
                ],
                "Kredi KartÄ±": [
                    {"key": "problem_tipi", "question": "Kredi kartÄ±nÄ±zla ilgili problem ne?"},
                    {"key": "islem_tarihi", "question": "Ä°ÅŸlem tarihi neydi?"},
                    {"key": "islem_miktari", "question": "Ä°ÅŸlem tutarÄ± ne kadardÄ±?"},
                    {"key": "merchant", "question": "Ä°ÅŸlemi yaptÄ±ÄŸÄ±nÄ±z yer neresi?"}
                ],
                "Ä°nternet BankacÄ±lÄ±ÄŸÄ±": [
                    {"key": "problem_tipi", "question": "Ä°nternet bankacÄ±lÄ±ÄŸÄ± ile ilgili problem ne?"},
                    {"key": "cihaz_bilgisi", "question": "Hangi cihazdan baÄŸlanmaya Ã§alÄ±ÅŸtÄ±nÄ±z?"},
                    {"key": "tarayici", "question": "Hangi web tarayÄ±cÄ±sÄ±nÄ± kullanÄ±yorsunuz?"},
                    {"key": "hata_mesaji", "question": "AldÄ±ÄŸÄ±nÄ±z hata mesajÄ± neydi?"}
                ]
            },
            "en": {
                "ATM": [
                    {"key": "problem_type", "question": "What problem did you experience at the ATM?"},
                    {"key": "amount", "question": "How much money got stuck?"},
                    {"key": "location", "question": "Which ATM did you have the problem at?"},
                    {"key": "datetime", "question": "When did the problem occur?"}
                ],
                "Credit Card": [
                    {"key": "problem_type", "question": "What's the problem with your credit card?"},
                    {"key": "transaction_date", "question": "When was the transaction?"},
                    {"key": "amount", "question": "What was the transaction amount?"},
                    {"key": "merchant", "question": "Where was the transaction made?"}
                ]
            }
        }

        self.setup_ui()

        if self.config.use_real_llm:
            print(f"Sistem hazÄ±r - GerÃ§ek LLM kullanÄ±lÄ±yor: {self.config.model_id}")
        else:
            print("Sistem hazÄ±r - Mock mode (HF token gerekli)")

    def setup_ui(self):
        """UI bileÅŸenlerini hazÄ±rla"""
        # Dil seÃ§imi
        self.language_dropdown = widgets.Dropdown(
            options=['tr', 'en'],
            value='tr',
            description='Dil:',
            style={'description_width': 'initial'}
        )

        # Kategori seÃ§imi
        self.category_dropdown = widgets.Dropdown(
            options=list(self.categories['tr'].keys()),
            value=None,
            description='Kategori:',
            style={'description_width': 'initial'}
        )

        # Mesaj input
        self.message_input = widgets.Textarea(
            placeholder='MesajÄ±nÄ±zÄ± yazÄ±n...',
            layout=widgets.Layout(width='100%', height='100px')
        )

        # GÃ¶nder butonu
        self.send_button = widgets.Button(
            description='GÃ¶nder',
            button_style='primary',
            layout=widgets.Layout(width='100px')
        )

        # Reset butonu
        self.reset_button = widgets.Button(
            description='Yeni KonuÅŸma',
            button_style='warning',
            layout=widgets.Layout(width='150px')
        )

        # Chat output
        self.chat_output = widgets.Output(
            layout=widgets.Layout(height='400px', border='1px solid #ccc', padding='10px')
        )

        # Event handlers
        self.language_dropdown.observe(self.on_language_change, names='value')
        self.category_dropdown.observe(self.on_category_change, names='value')
        self.send_button.on_click(self.on_send_message)
        self.reset_button.on_click(self.on_reset_conversation)

    def on_language_change(self, change):
        """Dil deÄŸiÅŸtiÄŸinde kategori listesini gÃ¼ncelle"""
        new_lang = change['new']
        self.current_language = new_lang

        # Kategori dropdown'unu gÃ¼ncelle
        available_categories = list(self.categories[new_lang].keys())
        self.category_dropdown.options = available_categories
        self.category_dropdown.value = None
        self.selected_category = None

        self.update_chat_display("info", f"Dil deÄŸiÅŸtirildi: {new_lang.upper()}")

    def on_category_change(self, change):
        """Kategori deÄŸiÅŸtiÄŸinde"""
        self.selected_category = change['new']
        if self.selected_category:
            self.update_chat_display("info", f"Kategori seÃ§ildi: {self.selected_category}")

    def on_send_message(self, button):
        """Mesaj gÃ¶nder"""
        message = self.message_input.value.strip()
        if not message:
            return

        if not self.selected_category:
            self.update_chat_display("error", "LÃ¼tfen Ã¶nce bir kategori seÃ§in!")
            return

        # MesajÄ± temizle
        self.message_input.value = ""

        # MesajÄ± gÃ¶rÃ¼ntÃ¼le
        self.update_chat_display("user", message)

        # MesajÄ± iÅŸle
        self.process_message(message)

    def on_reset_conversation(self, button):
        """KonuÅŸmayÄ± sÄ±fÄ±rla"""
        self.conversation_history = []
        self.current_conversation_id = None
        self.conversation_state = "idle"
        self.extracted_info = {}
        self.missing_questions = []
        self.current_question_index = 0

        self.chat_output.clear_output()
        self.update_chat_display("info", "Yeni konuÅŸma baÅŸlatÄ±ldÄ±")

    def update_chat_display(self, message_type: str, content: str):
        """Chat ekranÄ±nÄ± gÃ¼ncelle"""
        colors = {
            "user": "#007bff",
            "bot": "#28a745",
            "info": "#ffc107",
            "error": "#dc3545"
        }

        icons = {
            "user": "KullanÄ±cÄ±",
            "bot": "Bot",
            "info": "Bilgi",
            "error": "Hata"
        }

        color = colors.get(message_type, "#6c757d")
        icon = icons.get(message_type, "Mesaj")

        with self.chat_output:
            display(HTML(f"""
            <div style="margin: 10px 0; padding: 10px; border-left: 4px solid {color}; background-color: #f8f9fa;">
                <strong style="color: {color};">{icon}</strong><br>
                {content}
            </div>
            """))

    def process_message(self, message: str):
        """MesajÄ± iÅŸle"""
        try:
            if self.conversation_state == "idle":
                # Ä°lk mesaj - bilgi Ã§Ä±karÄ±mÄ± yap
                self.extract_information(message)
            elif self.conversation_state == "collecting":
                # Soru cevabÄ± - cevabÄ± iÅŸle
                self.process_answer(message)
            else:
                self.update_chat_display("error", "Beklenmeyen durum")

        except Exception as e:
            logger.error(f"Mesaj iÅŸleme hatasÄ±: {e}")
            self.update_chat_display("error", f"Ä°ÅŸleme hatasÄ±: {str(e)}")

    def extract_information(self, user_text: str):
        """Metinden bilgi Ã§Ä±karÄ±mÄ± - GerÃ§ek LLM ile"""
        try:
            category_questions = self.categories[self.current_language][self.selected_category]

            # LLM ile bilgi Ã§Ä±karÄ±mÄ±
            extracted = self.llm_service.extract_information_with_llm(
                user_text, category_questions, self.current_language
            )

            self.extracted_info = extracted

            # Eksik bilgileri tespit et
            self.missing_questions = []
            for question in category_questions:
                key = question['key']
                if extracted.get(key, "unknown") in ["unknown", "", None, "bilinmiyor"]:
                    self.missing_questions.append(question)

            # Sonucu gÃ¶ster
            self.show_extraction_result()

        except Exception as e:
            logger.error(f"Bilgi Ã§Ä±karÄ±m hatasÄ±: {e}")
            self.update_chat_display("error", f"Bilgi Ã§Ä±karÄ±mÄ±nda hata: {str(e)}")

    def show_extraction_result(self):
        """Ã‡Ä±karÄ±m sonucunu gÃ¶ster"""
        # Ã‡Ä±karÄ±lan bilgileri gÃ¶ster
        if self.extracted_info:
            info_text = "<br>".join([f"â€¢ {k}: {v}" for k, v in self.extracted_info.items() if v != "unknown"])
            if info_text:
                self.update_chat_display("bot", f"Tespit edilen bilgiler:<br>{info_text}")

        # Eksik bilgi varsa soru sor
        if self.missing_questions:
            self.conversation_state = "collecting"
            self.current_question_index = 0
            self.ask_next_question()
        else:
            self.complete_conversation()

    def generate_smart_question(self, question_data: Dict) -> str:
        """LLM ile akÄ±llÄ± soru Ã¼retimi"""
        if not self.llm_service.initialized:
            return question_data['question']  # Fallback

        context = " ".join([msg.get('content', '') for msg in self.conversation_history[-3:]])

        if self.current_language == "tr":
            prompt = f"""KonuÅŸma baÄŸlamÄ±: {context}

Sormak istediÄŸim soru: {question_data['question']}

Bu soruyu konuÅŸma akÄ±ÅŸÄ±na daha uygun ve doÄŸal bir ÅŸekilde sor. Samimi ol.
Sadece soruyu dÃ¶ndÃ¼r, baÅŸka aÃ§Ä±klama yapma."""
        else:
            prompt = f"""Conversation context: {context}

Question to ask: {question_data['question']}

Make this question more natural and conversational. Be friendly.
Return only the question, no other explanation."""

        smart_question = self.llm_service.generate_response(prompt, 100)

        # Basit temizlik
        if smart_question and len(smart_question) > 10:
            return smart_question.strip()
        else:
            return question_data['question']

    def ask_next_question(self):
        """Sonraki soruyu sor - LLM ile akÄ±llÄ± soru"""
        if self.current_question_index < len(self.missing_questions):
            question_data = self.missing_questions[self.current_question_index]

            # LLM ile akÄ±llÄ± soru oluÅŸtur
            smart_question = self.generate_smart_question(question_data)

            progress = f"({self.current_question_index + 1}/{len(self.missing_questions)})"
            self.update_chat_display("bot", f"{progress} {smart_question}")

            # KonuÅŸma geÃ§miÅŸine ekle
            self.conversation_history.append({
                "role": "bot",
                "content": smart_question,
                "question_key": question_data['key']
            })
        else:
            self.complete_conversation()

    def validate_answer_with_llm(self, question_key: str, question_text: str, answer: str) -> Dict:
        """LLM ile cevap validasyonu"""
        if not self.llm_service.initialized:
            return {"valid": True, "extracted_info": answer}  # Fallback

        if self.current_language == "tr":
            prompt = f"""Soru: {question_text}
Cevap: "{answer}"

Bu cevap soruya uygun mu? DeÄŸerlendir.

JSON formatÄ±nda yanÄ±tla:
{{
    "valid": true/false,
    "confidence": 0-100,
    "extracted_info": "Ã§Ä±karÄ±lan bilgi"
}}"""
        else:
            prompt = f"""Question: {question_text}
Answer: "{answer}"

Is this answer appropriate for the question? Evaluate.

Respond in JSON format:
{{
    "valid": true/false,
    "confidence": 0-100,
    "extracted_info": "extracted information"
}}"""

        response = self.llm_service.generate_response(prompt, 150)

        try:
            import json
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                return json.loads(json_match.group(0))
        except:
            pass

        # Fallback validation
        return {"valid": True, "extracted_info": answer}

    def process_answer(self, answer: str):
        """KullanÄ±cÄ± cevabÄ±nÄ± iÅŸle - LLM ile validasyon"""
        if self.current_question_index < len(self.missing_questions):
            current_question = self.missing_questions[self.current_question_index]
            key = current_question['key']

            # LLM ile cevabÄ± validate et
            validation = self.validate_answer_with_llm(
                key, current_question['question'], answer
            )

            # CevabÄ± kaydet
            if validation.get('valid', True):
                extracted_info = validation.get('extracted_info', answer)
                self.extracted_info[key] = extracted_info

                # KonuÅŸma geÃ§miÅŸine ekle
                self.conversation_history.append({
                    "role": "user",
                    "content": answer,
                    "question_key": key
                })
            else:
                # GeÃ§ersiz cevap - tekrar sor veya atla
                self.update_chat_display("bot", "Bu bilgiyi tam anlayamadÄ±m. Devam edelim.")
                self.extracted_info[key] = "belirsiz"

            # Sonraki soruya geÃ§
            self.current_question_index += 1

            if self.current_question_index < len(self.missing_questions):
                self.ask_next_question()
            else:
                self.complete_conversation()

    def complete_conversation(self):
        """KonuÅŸmayÄ± tamamla"""
        self.conversation_state = "completed"

        # Ã–zet oluÅŸtur
        summary_text = "<h4>Toplanan Bilgiler:</h4>"
        for key, value in self.extracted_info.items():
            if value and value != "unknown":
                readable_key = key.replace('_', ' ').title()
                summary_text += f"â€¢ <strong>{readable_key}:</strong> {value}<br>"

        summary_text += "<br><em>Bilgileriniz kaydedildi ve ilgili birime iletilecektir.</em>"

        self.update_chat_display("bot", summary_text)
        self.update_chat_display("info", "KonuÅŸma tamamlandÄ±. Yeni konuÅŸma baÅŸlatabilirsiniz.")

    def display(self):
        """Ana interface'i gÃ¶ster"""
        # BaÅŸlÄ±k
        display(HTML("<h2>AI MÃ¼ÅŸteri Destek Sistemi</h2>"))

        # Kontrol paneli
        controls = widgets.HBox([
            self.language_dropdown,
            self.category_dropdown,
            self.reset_button
        ])

        # Mesaj giriÅŸi
        message_area = widgets.VBox([
            widgets.Label("MesajÄ±nÄ±zÄ± yazÄ±n:"),
            self.message_input,
            self.send_button
        ])

        # Ana layout
        main_layout = widgets.VBox([
            controls,
            widgets.HTML("<hr>"),
            widgets.Label("Sohbet:"),
            self.chat_output,
            widgets.HTML("<hr>"),
            message_area
        ])

        display(main_layout)

        # BaÅŸlangÄ±Ã§ mesajÄ±
        self.update_chat_display("info", "Sistem hazÄ±r! YukarÄ±dan dil ve kategori seÃ§in, sonra mesajÄ±nÄ±zÄ± yazÄ±n.")


# KullanÄ±m fonksiyonlarÄ±
def setup_colab_environment():
    """Colab ortamÄ±nÄ± hazÄ±rla"""
    print("Gerekli paketler yÃ¼kleniyor...")

    try:
        import subprocess
        import sys

        # Gerekli paketleri yÃ¼kle
        packages = [
            "transformers",
            "torch",
            "ipywidgets",
            "accelerate"
        ]

        for package in packages:
            subprocess.check_call([sys.executable, "-m", "pip", "install", package])

        print("Paketler baÅŸarÄ±yla yÃ¼klendi!")
        return True

    except Exception as e:
        print(f"Paket yÃ¼kleme hatasÄ±: {e}")
        return False

def setup_hf_token():
    """HuggingFace token kurulumu rehberi"""
    print("""
HuggingFace Token Kurulumu:

1. https://huggingface.co/settings/tokens adresine gidin
2. 'New token' ile yeni token oluÅŸturun
3. Token'Ä± kopyalayÄ±n

Colab'da kullanÄ±m iÃ§in iki seÃ§enek:

A) Colab Secrets (Ã–nerilen):
   - Sol panelde 'Secrets' (ğŸ”‘) bÃ¶lÃ¼mÃ¼ne tÄ±klayÄ±n
   - 'Add new secret' butonuna tÄ±klayÄ±n
   - Name: HUGGINGFACE_TOKEN
   - Value: [token'Ä±nÄ±zÄ± yapÄ±ÅŸtÄ±rÄ±n]
   - Save edin

B) GeÃ§ici kullanÄ±m:
   import os
   os.environ["HUGGINGFACE_TOKEN"] = "your_token_here"
    """)

# Ana Ã§alÄ±ÅŸtÄ±rma fonksiyonu
def run_ai_support():
    """AI destek sistemini Ã§alÄ±ÅŸtÄ±r"""
    print("AI MÃ¼ÅŸteri Destek Sistemi baÅŸlatÄ±lÄ±yor...\n")

    # Sistemi baÅŸlat
    support = ColabAISupport()
    support.display()

    return support

# KullanÄ±m Ã¶rneÄŸi
if __name__ == "__main__":
    print("AI MÃ¼ÅŸteri Destek Sistemi - Google Colab")
    print("=" * 50)

    # OrtamÄ± hazÄ±rla
    if setup_colab_environment():
        print("\nToken kurulumu iÃ§in setup_hf_token() fonksiyonunu Ã§alÄ±ÅŸtÄ±rÄ±n")
        print("Sistemi baÅŸlatmak iÃ§in run_ai_support() fonksiyonunu Ã§alÄ±ÅŸtÄ±rÄ±n")

    # Ã–rnek kullanÄ±m:
    # setup_hf_token()  # Token kurulum rehberi
    # support = run_ai_support()  # Sistemi baÅŸlat

# 2. HF Token'Ä± ayarla (iki yoldan biri):
# Yol 1: Colab Secrets (Ã–nerilen)
# - Sol panelde ğŸ”‘ Secrets'e tÄ±kla
# - "HUGGINGFACE_TOKEN" ekle
# - Token'Ä± yapÄ±ÅŸtÄ±r

# Yol B: GeÃ§ici
import os
os.environ["HUGGINGFACE_TOKEN"] = "x"

# 5. Sistemi baÅŸlat
support = run_ai_support()

# Sistemi baÅŸlattÄ±ktan sonra:
support.selected_category = "ATM"
support.process_message("ATM'de param sÄ±kÄ±ÅŸtÄ±, 500 TL kaybettim, Ankara'daki KÄ±zÄ±lay ATM'sinde")













"""
AI MÃ¼ÅŸteri Destek Sistemi - Google Colab Simple Version
Widget kullanmayan, sadece input/print ile Ã§alÄ±ÅŸan versiyon
"""

import os
import sys
import logging
import json
import time
import re
from datetime import datetime
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
from enum import Enum

# Logging setup
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class ConversationState(Enum):
    """KonuÅŸma durumlarÄ±"""
    IDLE = "idle"
    COLLECTING = "collecting"
    COMPLETED = "completed"
    ERROR = "error"


@dataclass
class QuestionData:
    """Soru veri yapÄ±sÄ±"""
    key: str
    question: str
    validation_pattern: Optional[str] = None
    required: bool = True


class SimpleConfigManager:
    """Basit konfigÃ¼rasyon yÃ¶neticisi"""

    def __init__(self):
        self.load_config()
        self.detect_environment()

    def load_config(self):
        """KonfigÃ¼rasyon yÃ¼kle"""
        # Token yÃ¶netimi
        self.hf_token = self._get_token()

        # Model ayarlarÄ±
        self.available_memory = self._get_gpu_memory()
        self.model_id = self._get_optimal_model()

        # Generation parametreleri
        self.generation_config = {
            "max_new_tokens": 256,
            "temperature": 0.7,
            "do_sample": True,
            "top_p": 0.9,
            "repetition_penalty": 1.1,
            "no_repeat_ngram_size": 3
        }

        # Yol ayarlarÄ±
        self.base_path = "/content"
        self.data_path = "/content/ai_support_data"
        os.makedirs(self.data_path, exist_ok=True)

        # Debug mod
        self.debug_mode = os.getenv("DEBUG", "false").lower() == "true"

    def _get_token(self) -> str:
        """Token'Ä± al"""
        token = os.getenv("HUGGINGFACE_TOKEN", "")
        if token:
            print("âœ… HuggingFace token bulundu")
            return token
        else:
            print("âš ï¸ HuggingFace token bulunamadÄ± - Mock mode")
            return ""

    def _get_gpu_memory(self) -> float:
        """GPU memory miktarÄ±nÄ± tespit et"""
        try:
            import torch
            if torch.cuda.is_available():
                memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                print(f"ğŸ® GPU Memory: {memory_gb:.1f}GB")
                return memory_gb
            else:
                print("ğŸ’» CPU kullanÄ±lacak")
                return 0.0
        except ImportError:
            print("ğŸ’» PyTorch bulunamadÄ± - CPU mode")
            return 0.0

    def _get_optimal_model(self) -> str:
        """Optimal model seÃ§"""
        if self.available_memory >= 6:
            return "microsoft/DialoGPT-medium"
        elif self.available_memory >= 4:
            return "google/gemma-2-2b-it"
        else:
            return "microsoft/DialoGPT-small"

    def detect_environment(self):
        """Ã‡alÄ±ÅŸma ortamÄ±nÄ± tespit et"""
        self.environment = {
            "gpu_available": self.available_memory > 0,
            "token_available": bool(self.hf_token),
            "model_selected": self.model_id
        }


class SimpleLLMService:
    """Basit LLM servisi"""

    def __init__(self, config: SimpleConfigManager):
        self.config = config
        self.model = None
        self.tokenizer = None
        self.device = "cpu"
        self.initialized = False

        if config.hf_token:
            self.initialize_model()

    def initialize_model(self):
        """Model yÃ¼kleme"""
        try:
            print(f"ğŸ”„ Model yÃ¼kleniyor: {self.config.model_id}")
            print("   Bu iÅŸlem 1-3 dakika sÃ¼rebilir...")

            import torch
            from transformers import AutoTokenizer, AutoModelForCausalLM

            # Device seÃ§imi
            if torch.cuda.is_available() and self.config.available_memory > 2:
                self.device = "cuda"
                dtype = torch.float16
            else:
                self.device = "cpu"
                dtype = torch.float32

            # Tokenizer yÃ¼kle
            print("   ğŸ“š Tokenizer yÃ¼kleniyor...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.config.model_id,
                token=self.config.hf_token,
                trust_remote_code=True
            )

            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

            # Model yÃ¼kle
            print("   ğŸ§  Model yÃ¼kleniyor...")
            self.model = AutoModelForCausalLM.from_pretrained(
                self.config.model_id,
                torch_dtype=dtype,
                trust_remote_code=True,
                token=self.config.hf_token,
                device_map="auto" if self.device == "cuda" else None
            )

            if self.device == "cpu":
                self.model = self.model.to(self.device)

            self.initialized = True
            print(f"âœ… Model baÅŸarÄ±yla yÃ¼klendi!")
            print(f"   ğŸ“± Device: {self.device}")

        except Exception as e:
            print(f"âŒ Model yÃ¼kleme hatasÄ±: {e}")
            print("ğŸ”„ Mock mode'a geÃ§iliyor...")
            self.initialized = False

    def generate_response(self, prompt: str, max_tokens: int = None) -> str:
        """Response Ã¼retimi"""
        if not self.initialized:
            return self._mock_response(prompt)

        try:
            max_tokens = max_tokens or self.config.generation_config["max_new_tokens"]

            inputs = self.tokenizer.encode(prompt, return_tensors="pt").to(self.device)

            gen_kwargs = self.config.generation_config.copy()
            gen_kwargs["max_new_tokens"] = max_tokens
            gen_kwargs["pad_token_id"] = self.tokenizer.pad_token_id
            gen_kwargs["eos_token_id"] = self.tokenizer.eos_token_id

            with torch.no_grad():
                outputs = self.model.generate(inputs, **gen_kwargs)

            response = self.tokenizer.decode(
                outputs[0][inputs.shape[1]:],
                skip_special_tokens=True
            ).strip()

            return response

        except Exception as e:
            print(f"âš ï¸ Generation hatasÄ±: {e}")
            return self._mock_response(prompt)

    def _mock_response(self, prompt: str) -> str:
        """Mock yanÄ±tlar"""
        prompt_lower = prompt.lower()

        # TÃ¼rkÃ§e patterns
        if any(word in prompt_lower for word in ["sorun", "problem", "hata"]):
            return "Sorununuzu anlÄ±yorum. Bu konuda size yardÄ±mcÄ± olmaya Ã§alÄ±ÅŸacaÄŸÄ±m."
        elif any(word in prompt_lower for word in ["teÅŸekkÃ¼r", "saÄŸol"]):
            return "Rica ederim, yardÄ±mcÄ± olabildiysem ne mutlu bana."
        elif any(word in prompt_lower for word in ["merhaba", "selam"]):
            return "Merhaba! Size nasÄ±l yardÄ±mcÄ± olabilirim?"
        elif "?" in prompt:
            return "Bu konuda daha fazla bilgiye ihtiyacÄ±m var."
        else:
            return "AnlayÄ±ÅŸÄ±nÄ±z iÃ§in teÅŸekkÃ¼rler. Devam edelim."

    def extract_information(self, text: str, questions: List[QuestionData], language: str) -> Dict[str, str]:
        """AkÄ±llÄ± bilgi Ã§Ä±karÄ±mÄ±"""
        if not self.initialized:
            return self._mock_extraction(text, questions, language)

        # Structured prompt
        questions_text = "\n".join([f"- {q.key}: {q.question}" for q in questions])

        if language == "tr":
            prompt = f"""Metinden bilgi Ã§Ä±kar:

METIN: "{text}"

Ã‡IKARILACAK BÄ°LGÄ°LER:
{questions_text}

KURALLAR:
- Sadece metinde aÃ§Ä±kÃ§a belirtilen bilgileri Ã§Ä±kar
- Bilgi yoksa "bilinmiyor" yaz
- JSON formatÄ±nda yanÄ±tla

JSON:"""
        else:
            prompt = f"""Extract information from text:

TEXT: "{text}"

INFORMATION TO EXTRACT:
{questions_text}

RULES:
- Extract only clearly stated information
- Use "unknown" if not found
- Respond in JSON format

JSON:"""

        response = self.generate_response(prompt, 300)

        # JSON parse
        try:
            json_match = re.search(r'\{[^{}]*\}', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                extracted = json.loads(json_str)

                # Validation
                validated = {}
                for question in questions:
                    key = question.key
                    value = extracted.get(key, "bilinmiyor" if language == "tr" else "unknown")
                    validated[key] = value

                return validated
            else:
                return self._mock_extraction(text, questions, language)

        except Exception as e:
            return self._mock_extraction(text, questions, language)

    def _mock_extraction(self, text: str, questions: List[QuestionData], language: str) -> Dict[str, str]:
        """Mock extraction"""
        extracted = {}
        text_lower = text.lower()
        unknown_word = "bilinmiyor" if language == "tr" else "unknown"

        for question in questions:
            key = question.key
            value = unknown_word

            # Basit pattern matching
            if any(word in text_lower for word in ['atm', 'bankamatik']):
                if "problem" in key or "tip" in key:
                    value = "ATM sorunu"
                elif "lokasyon" in key or "location" in key:
                    value = "ATM lokasyonu belirtilmedi"
            elif any(word in text_lower for word in ['kart', 'card']):
                if "problem" in key or "tip" in key:
                    value = "Kart sorunu"
            elif any(word in text_lower for word in ['para', 'money', 'tl']):
                if "miktar" in key or "amount" in key:
                    import re
                    numbers = re.findall(r'\d+', text)
                    if numbers:
                        value = f"{numbers[0]} TL"

            extracted[key] = value

        return extracted


class SimpleAISupport:
    """Basit AI Destek Sistemi - Console tabanlÄ±"""

    def __init__(self):
        print("ğŸ¤– AI MÃ¼ÅŸteri Destek Sistemi")
        print("=" * 50)

        self.config = SimpleConfigManager()
        self.llm_service = SimpleLLMService(self.config)

        # State management
        self.current_language = "tr"
        self.selected_category = None
        self.conversation_state = ConversationState.IDLE
        self.extracted_info = {}
        self.missing_questions = []
        self.current_question_index = 0
        self.conversation_history = []

        # Categories
        self.categories = {
            "tr": {
                "ATM": [
                    QuestionData("problem_tipi", "ATM'de yaÅŸadÄ±ÄŸÄ±nÄ±z problem ne?", required=True),
                    QuestionData("para_miktari", "Ne kadar paranÄ±z sÄ±kÄ±ÅŸtÄ± veya eksik Ã§ekildi?", required=False),
                    QuestionData("lokasyon", "Hangi ATM'de problem yaÅŸadÄ±nÄ±z? (Åube/Adres)", required=True),
                    QuestionData("tarih_saat", "Problem hangi tarih ve saatte yaÅŸandÄ±?", required=True),
                    QuestionData("islem_numarasi", "Ä°ÅŸlem numaranÄ±z var mÄ±?", required=False)
                ],
                "Kredi KartÄ±": [
                    QuestionData("problem_tipi", "Kredi kartÄ±nÄ±zla ilgili problem ne?", required=True),
                    QuestionData("islem_tarihi", "Ä°ÅŸlem tarihi neydi?", required=True),
                    QuestionData("islem_miktari", "Ä°ÅŸlem tutarÄ± ne kadardÄ±?", required=True),
                    QuestionData("merchant", "Ä°ÅŸlemi yaptÄ±ÄŸÄ±nÄ±z yer neresi?", required=True),
                    QuestionData("kart_son4", "KartÄ±nÄ±zÄ±n son 4 hanesi nedir?", required=False)
                ],
                "Ä°nternet BankacÄ±lÄ±ÄŸÄ±": [
                    QuestionData("problem_tipi", "Ä°nternet bankacÄ±lÄ±ÄŸÄ± ile ilgili problem ne?", required=True),
                    QuestionData("cihaz_bilgisi", "Hangi cihazdan baÄŸlanmaya Ã§alÄ±ÅŸtÄ±nÄ±z?", required=True),
                    QuestionData("tarayici", "Hangi web tarayÄ±cÄ±sÄ±nÄ± kullanÄ±yorsunuz?", required=False),
                    QuestionData("hata_mesaji", "AldÄ±ÄŸÄ±nÄ±z hata mesajÄ± neydi?", required=True)
                ],
                "Genel Åikayet": [
                    QuestionData("sikayet_konusu", "Åikayetinizin konusu nedir?", required=True),
                    QuestionData("yaÅŸanan_durum", "YaÅŸadÄ±ÄŸÄ±nÄ±z durumu detaylÄ± anlatÄ±r mÄ±sÄ±nÄ±z?", required=True),
                    QuestionData("beklenti", "Bu konuda beklentiniz nedir?", required=True)
                ]
            },
            "en": {
                "ATM": [
                    QuestionData("problem_type", "What problem did you experience at the ATM?", required=True),
                    QuestionData("amount", "How much money got stuck?", required=False),
                    QuestionData("location", "Which ATM did you have the problem at?", required=True),
                    QuestionData("datetime", "When did the problem occur?", required=True)
                ],
                "Credit Card": [
                    QuestionData("problem_type", "What's the problem with your credit card?", required=True),
                    QuestionData("transaction_date", "When was the transaction?", required=True),
                    QuestionData("amount", "What was the transaction amount?", required=True),
                    QuestionData("merchant", "Where was the transaction made?", required=True)
                ]
            }
        }

        self.print_startup_info()

    def print_startup_info(self):
        """BaÅŸlangÄ±Ã§ bilgileri"""
        status = "âœ… GERÃ‡EK AI" if self.llm_service.initialized else "âš ï¸ MOCK MODE"

        print(f"""
ğŸ“Š SÄ°STEM BÄ°LGÄ°LERÄ°:
   Model: {self.config.model_id}
   Device: {self.llm_service.device}
   GPU Memory: {self.config.available_memory:.1f}GB
   Durum: {status}

ğŸŒ Diller: TÃ¼rkÃ§e (tr), English (en)
ğŸ“‹ Kategoriler: {', '.join(self.categories['tr'].keys())}
""")

    def run(self):
        """Ana Ã§alÄ±ÅŸtÄ±rma dÃ¶ngÃ¼sÃ¼"""
        print("\nğŸ¯ AI MÃ¼ÅŸteri Destek Sistemi'ne HoÅŸ Geldiniz!")
        print("-" * 50)

        while True:
            try:
                # Dil seÃ§imi
                self.select_language()

                # Kategori seÃ§imi
                self.select_category()

                # Ana konuÅŸma
                self.start_conversation()

                # Devam etmek istiyor mu?
                if not self.ask_continue():
                    break

            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ GÃ¶rÃ¼ÅŸmek Ã¼zere!")
                break
            except Exception as e:
                print(f"\nâŒ Hata oluÅŸtu: {e}")
                continue

    def select_language(self):
        """Dil seÃ§imi"""
        while True:
            print(f"\nğŸŒ DÄ°L SEÃ‡Ä°MÄ° (Mevcut: {self.current_language.upper()})")
            print("1. TÃ¼rkÃ§e (tr)")
            print("2. English (en)")
            print("3. Devam et")

            choice = input("SeÃ§iminiz (1-3): ").strip()

            if choice == "1":
                self.current_language = "tr"
                print("âœ… TÃ¼rkÃ§e seÃ§ildi")
                break
            elif choice == "2":
                self.current_language = "en"
                print("âœ… English selected")
                break
            elif choice == "3":
                break
            else:
                print("âŒ GeÃ§ersiz seÃ§im!")

    def select_category(self):
        """Kategori seÃ§imi"""
        categories = list(self.categories[self.current_language].keys())

        while True:
            print(f"\nğŸ“‹ KATEGORÄ° SEÃ‡Ä°MÄ°")
            for i, category in enumerate(categories, 1):
                print(f"{i}. {category}")

            try:
                choice = input("Kategori numarasÄ±: ").strip()
                choice_num = int(choice)

                if 1 <= choice_num <= len(categories):
                    self.selected_category = categories[choice_num - 1]
                    print(f"âœ… SeÃ§ilen kategori: {self.selected_category}")
                    break
                else:
                    print("âŒ GeÃ§ersiz numara!")

            except ValueError:
                print("âŒ LÃ¼tfen bir numara girin!")

    def start_conversation(self):
        """Ana konuÅŸmayÄ± baÅŸlat"""
        self.conversation_state = ConversationState.IDLE
        self.extracted_info = {}
        self.missing_questions = []
        self.current_question_index = 0

        print(f"\nğŸ’¬ KONUÅMA BAÅLADI - {self.selected_category}")
        print("=" * 50)

        if self.current_language == "tr":
            print("Sorununuzu detaylÄ± bir ÅŸekilde anlatÄ±n:")
            print("(Ne kadar detay verirseniz, o kadar az soru sorarÄ±m)")
        else:
            print("Please describe your problem in detail:")
            print("(The more details you provide, the fewer questions I'll ask)")

        print("\nÃ–rnek: 'ATM'den para Ã§ekerken kartÄ±m sÄ±kÄ±ÅŸtÄ±, 500 TL Ã§ekmeye Ã§alÄ±ÅŸÄ±yordum...'")
        print("-" * 50)

        # Ä°lk mesajÄ± al
        user_message = input("MesajÄ±nÄ±z: ").strip()

        if not user_message:
            print("âŒ BoÅŸ mesaj gÃ¶nderemezsiniz!")
            return

        # MesajÄ± iÅŸle
        self.process_initial_message(user_message)

    def process_initial_message(self, message: str):
        """Ä°lk mesajÄ± iÅŸle"""
        print("\nğŸ” Bilgiler Ã§Ä±karÄ±lÄ±yor...")

        category_questions = self.categories[self.current_language][self.selected_category]

        # Bilgi Ã§Ä±karÄ±mÄ±
        extracted = self.llm_service.extract_information(
            message, category_questions, self.current_language
        )

        self.extracted_info = extracted

        # SonuÃ§larÄ± gÃ¶ster
        self.show_extraction_results()

        # Eksik bilgileri tespit et
        self.find_missing_information(category_questions)

        # Eksik bilgi varsa sor
        if self.missing_questions:
            self.collect_missing_information()
        else:
            self.complete_conversation()

    def show_extraction_results(self):
        """Ã‡Ä±karÄ±lan bilgileri gÃ¶ster"""
        valid_info = {k: v for k, v in self.extracted_info.items()
                     if v and str(v).lower() not in ["unknown", "bilinmiyor", "belirsiz"]}

        if valid_info:
            print("\nâœ… TESPÄ°T EDÄ°LEN BÄ°LGÄ°LER:")
            print("-" * 30)
            for key, value in valid_info.items():
                readable_key = key.replace('_', ' ').title()
                print(f"â€¢ {readable_key}: {value}")
        else:
            print("\nâš ï¸ HenÃ¼z yeterli bilgi tespit edilemedi")

    def find_missing_information(self, category_questions):
        """Eksik bilgileri bul"""
        self.missing_questions = []

        for question in category_questions:
            key = question.key
            value = self.extracted_info.get(key, "")
            unknown_values = ["unknown", "bilinmiyor", "", None, "belirsiz"]

            if not value or str(value).lower() in unknown_values:
                if question.required:
                    self.missing_questions.append(question)

        if self.missing_questions:
            if self.current_language == "tr":
                print(f"\nâ“ {len(self.missing_questions)} ek bilgiye ihtiyacÄ±m var.")
            else:
                print(f"\nâ“ I need {len(self.missing_questions)} additional pieces of information.")

    def collect_missing_information(self):
        """Eksik bilgileri topla"""
        self.conversation_state = ConversationState.COLLECTING
        self.current_question_index = 0

        for i, question in enumerate(self.missing_questions):
            progress = f"({i+1}/{len(self.missing_questions)})"

            print(f"\n{progress} {question.question}")

            if not question.required:
                skip_text = "(isteÄŸe baÄŸlÄ± - 'geÃ§' yazabilirsiniz)" if self.current_language == "tr" else "(optional - type 'skip')"
                print(f"   {skip_text}")

            while True:
                answer = input("YanÄ±tÄ±nÄ±z: ").strip()

                if not answer:
                    print("âŒ BoÅŸ yanÄ±t verilemez!")
                    continue

                # Skip kontrolÃ¼
                skip_commands = ["geÃ§", "skip", "bilmiyorum", "don't know"]
                if answer.lower() in skip_commands:
                    if not question.required:
                        print("â­ï¸ AtlandÄ±")
                        self.extracted_info[question.key] = "atlandÄ±" if self.current_language == "tr" else "skipped"
                        break
                    else:
                        required_msg = "Bu bilgi gerekli!" if self.current_language == "tr" else "This information is required!"
                        print(f"âŒ {required_msg}")
                        continue

                # YanÄ±tÄ± kaydet
                self.extracted_info[question.key] = answer
                print("âœ… Kaydedildi")
                break

        self.complete_conversation()

    def complete_conversation(self):
        """KonuÅŸmayÄ± tamamla"""
        self.conversation_state = ConversationState.COMPLETED

        print(f"\nğŸ‰ BÄ°LGÄ° TOPLAMA TAMAMLANDI!")
        print("=" * 50)

        # Ã–zet gÃ¶ster
        print("ğŸ“‹ TOPLANAN BÄ°LGÄ°LER:")
        print("-" * 25)

        for key, value in self.extracted_info.items():
            if value and str(value).lower() not in ["unknown", "bilinmiyor", "atlandÄ±", "skipped"]:
                readable_key = key.replace('_', ' ').title()
                print(f"â€¢ {readable_key}: {value}")

        # Referans numarasÄ±
        ref_no = f"REF-{int(time.time())}"
        print(f"\nğŸ”¢ Referans NumarasÄ±: {ref_no}")

        if self.current_language == "tr":
            print("\nâœ… Bilgileriniz kaydedildi ve ilgili birime iletilecektir.")
            print("ğŸ“ Gerekirse bu referans numarasÄ±yla bize ulaÅŸabilirsiniz.")
        else:
            print("\nâœ… Your information has been recorded and will be forwarded.")
            print("ğŸ“ You can contact us with this reference number if needed.")

        # Dosyaya kaydet
        self.save_conversation(ref_no)

    def save_conversation(self, ref_no: str):
        """KonuÅŸmayÄ± dosyaya kaydet"""
        try:
            conversation_data = {
                "reference_no": ref_no,
                "timestamp": datetime.now().isoformat(),
                "language": self.current_language,
                "category": self.selected_category,
                "extracted_info": self.extracted_info,
                "status": "completed"
            }

            filename = f"conversation_{ref_no}.json"
            filepath = os.path.join(self.config.data_path, filename)

            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(conversation_data, f, ensure_ascii=False, indent=2)

            print(f"ğŸ’¾ KonuÅŸma kaydedildi: {filename}")

        except Exception as e:
            print(f"âš ï¸ Kaydetme hatasÄ±: {e}")

    def ask_continue(self) -> bool:
        """Devam etmek isteyip istemediÄŸini sor"""
        print("\n" + "="*50)

        if self.current_language == "tr":
            question = "Yeni bir konuÅŸma baÅŸlatmak istiyor musunuz? (e/h): "
            yes_answers = ["e", "evet", "yes", "y"]
        else:
            question = "Would you like to start a new conversation? (y/n): "
            yes_answers = ["y", "yes", "e", "evet"]

        while True:
            answer = input(question).strip().lower()

            if answer in yes_answers:
                print("\nğŸ”„ Yeni konuÅŸma baÅŸlatÄ±lÄ±yor...\n")
                return True
            elif answer in ["h", "hayÄ±r", "no", "n"]:
                return False
            else:
                print("âŒ GeÃ§ersiz cevap! (e/h)")


# Ana fonksiyonlar
def setup_environment():
    """OrtamÄ± hazÄ±rla"""
    print("ğŸ”§ Ortam kontrol ediliyor...")

    # Gerekli paketler
    required_packages = ["transformers", "torch"]
    missing_packages = []

    for package in required_packages:
        try:
            __import__(package)
            print(f"   âœ… {package}")
        except ImportError:
            missing_packages.append(package)
            print(f"   âŒ {package}")

    if missing_packages:
        print(f"\nâš ï¸ Eksik paketler: {', '.join(missing_packages)}")
        print("Ã–nce ÅŸu komutu Ã§alÄ±ÅŸtÄ±rÄ±n:")
        print(f"!pip install {' '.join(missing_packages)}")
        return False

    return True

def setup_token_guide():
    """Token kurulum rehberi"""
    print("""
ğŸ”‘ HUGGINGFACE TOKEN KURULUMU
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1ï¸âƒ£ Token Alma:
   https://huggingface.co/settings/tokens
   â†’ "New token" â†’ "Read" â†’ Token'Ä± kopyala

2ï¸âƒ£ Colab'da Ayarlama:
   import os
   os.environ["HUGGINGFACE_TOKEN"] = "your_token_here"

3ï¸âƒ£ Sistemi BaÅŸlatma:
   support = run_simple_ai_support()
""")

def run_simple_ai_support():
    """Basit AI desteÄŸi Ã§alÄ±ÅŸtÄ±r"""
    print("ğŸš€ AI MÃ¼ÅŸteri Destek Sistemi baÅŸlatÄ±lÄ±yor...\n")

    if not setup_environment():
        print("âŒ Gereksinimler karÅŸÄ±lanmadÄ±!")
        return None

    try:
        support = SimpleAISupport()
        support.run()
        return support

    except Exception as e:
        print(f"âŒ BaÅŸlatma hatasÄ±: {e}")
        import traceback
        traceback.print_exc()
        return None

def demo_usage():
    """Demo kullanÄ±m"""
    print("""
ğŸ¬ DEMO KULLANIM
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. Dil seÃ§in: TÃ¼rkÃ§e (1)
2. Kategori seÃ§in: ATM (1)
3. Problem yazÄ±n: "ATM'den para Ã§ekerken kartÄ±m sÄ±kÄ±ÅŸtÄ±, 500 TL Ã§ekmeye Ã§alÄ±ÅŸÄ±yordum"
4. Ek sorularÄ± yanÄ±tlayÄ±n
5. Ã–zeti gÃ¶rÃ¼n

ğŸ’¡ Ne kadar detay verirseniz, o kadar az soru sorar!
""")

# Ana Ã§alÄ±ÅŸtÄ±rma
if __name__ == "__main__":
    print("ğŸ¤– AI MÃ¼ÅŸteri Destek Sistemi - Basit Versiyon")
    print("=" * 60)

    print("\nğŸ“š KullanÄ±labilir Fonksiyonlar:")
    print("â€¢ setup_token_guide() - Token kurulum rehberi")
    print("â€¢ run_simple_ai_support() - Sistemi baÅŸlat")
    print("â€¢ demo_usage() - KullanÄ±m Ã¶rneÄŸi")

    print("\nğŸš€ HIZLI BAÅLATMA:")
    print("support = run_simple_ai_support()")


# ===============================
# GOOGLE COLAB Ã‡ALIÅTIRMA REHBERÄ°
# ===============================

"""
ğŸ¯ ADIM ADIM Ã‡ALIÅTIRMA REHBERÄ°:

1ï¸âƒ£ PAKET YÃœKLEMESÄ°:
!pip install transformers torch

2ï¸âƒ£ TOKEN AYARLAMA:
import os
os.environ["HUGGINGFACE_TOKEN"] = "hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"

3ï¸âƒ£ KOD Ã‡ALIÅTIRMA:
# Bu dosyayÄ± tamamen Ã§alÄ±ÅŸtÄ±rÄ±n (Ctrl+F9)

4ï¸âƒ£ SÄ°STEM BAÅLATMA:
support = run_simple_ai_support()

ğŸ“± KULLANIM:
â€¢ Terminal tabanlÄ± - widget yok!
â€¢ Dil seÃ§imi (TÃ¼rkÃ§e/Ä°ngilizce)
â€¢ Kategori seÃ§imi (ATM, Kredi KartÄ±, vs.)
â€¢ Problem yazma
â€¢ Soru-cevap
â€¢ Ã–zet gÃ¶rme

ğŸ’¡ Ã–ZELLÄ°KLER:
âœ… Widget gerektirmez - sadece input/print
âœ… GerÃ§ek LLM entegrasyonu
âœ… AkÄ±llÄ± bilgi Ã§Ä±karÄ±mÄ±
âœ… Ã‡oklu dil desteÄŸi
âœ… KonuÅŸma kaydÄ± (JSON)
âœ… GPU/CPU otomatik algÄ±lama
âœ… Mock mode (token olmadan da Ã§alÄ±ÅŸÄ±r)
âœ… Hata yÃ¶netimi

ğŸ”§ SORUN GÄ°DERME:
â€¢ Token yoksa mock mode'da Ã§alÄ±ÅŸÄ±r
â€¢ GPU yoksa CPU kullanÄ±r
â€¢ Model yÃ¼klenemezse mock yanÄ±tlar verir

ğŸ¬ Ã–RNEK KULLANIM:
1. Sistemi baÅŸlatÄ±n: run_simple_ai_support()
2. Dil seÃ§in: 1 (TÃ¼rkÃ§e)
3. Kategori: 1 (ATM)
4. Problem: "ATM'den para Ã§ekerken kartÄ±m sÄ±kÄ±ÅŸtÄ±"
5. SorularÄ± yanÄ±tlayÄ±n
6. Ã–zeti gÃ¶rÃ¼n
"""

def check_system():
    """Sistem durumunu kontrol et"""
    print("ğŸ” SÄ°STEM KONTROL")
    print("=" * 30)

    # Python version
    print(f"Python: {sys.version.split()[0]}")

    # Packages
    packages = ["transformers", "torch"]
    for package in packages:
        try:
            __import__(package)
            print(f"âœ… {package}: YÃ¼klÃ¼")
        except ImportError:
            print(f"âŒ {package}: Eksik")

    # GPU
    try:
        import torch
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            print(f"ğŸ® GPU: {gpu_name} ({memory:.1f}GB)")
        else:
            print("ğŸ’» GPU: BulunamadÄ± (CPU kullanÄ±lacak)")
    except:
        print("ğŸ’» GPU: Kontrol edilemedi")

    # Token
    token = os.getenv("HUGGINGFACE_TOKEN", "")
    print(f"ğŸ”‘ HF Token: {'âœ… AyarlanmÄ±ÅŸ' if token else 'âŒ Eksik'}")

    print("\n" + "=" * 30)

def quick_start():
    """HÄ±zlÄ± baÅŸlatma"""
    print("""
ğŸš€ HIZLI BAÅLATMA KILAVUZU
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ADIM 1 - Paketleri yÃ¼kleyin:
!pip install transformers torch

ADIM 2 - Token ayarlayÄ±n:
import os
os.environ["HUGGINGFACE_TOKEN"] = "your_token_here"

ADIM 3 - Kodu Ã§alÄ±ÅŸtÄ±rÄ±n:
# Bu cell'i tamamen Ã§alÄ±ÅŸtÄ±rÄ±n

ADIM 4 - Sistemi baÅŸlatÄ±n:
support = run_simple_ai_support()

ADIM 5 - KullanÄ±n:
â€¢ Dil seÃ§in (1-2)
â€¢ Kategori seÃ§in (1-4)
â€¢ Probleminizi yazÄ±n
â€¢ SorularÄ± yanÄ±tlayÄ±n
â€¢ Ã–zeti gÃ¶rÃ¼n

ğŸ’¡ BONUS Ä°PUÃ‡LARI:
â€¢ DetaylÄ± yazÄ±n â†’ Az soru
â€¢ "geÃ§" yazÄ±n â†’ Ä°steÄŸe baÄŸlÄ± sorularÄ± atla
â€¢ Token yoksa mock mode Ã§alÄ±ÅŸÄ±r
â€¢ Her konuÅŸma JSON olarak kaydedilir
""")

def test_mock_mode():
    """Mock mode testini Ã§alÄ±ÅŸtÄ±r"""
    print("ğŸ§ª MOCK MODE TESTÄ°")
    print("=" * 25)

    # Token'Ä± geÃ§ici olarak kaldÄ±r
    original_token = os.environ.get("HUGGINGFACE_TOKEN", "")
    if original_token:
        del os.environ["HUGGINGFACE_TOKEN"]

    try:
        config = SimpleConfigManager()
        llm = SimpleLLMService(config)

        # Test yanÄ±tlarÄ±
        test_prompts = [
            "ATM'de problem yaÅŸadÄ±m",
            "Merhaba",
            "TeÅŸekkÃ¼rler",
            "KartÄ±m Ã§alÄ±ÅŸmÄ±yor"
        ]

        print("Mock yanÄ±tlarÄ±:")
        for prompt in test_prompts:
            response = llm.generate_response(prompt)
            print(f"â€¢ '{prompt}' â†’ '{response}'")

        print("\nâœ… Mock mode Ã§alÄ±ÅŸÄ±yor!")

    except Exception as e:
        print(f"âŒ Mock test hatasÄ±: {e}")

    finally:
        # Token'Ä± geri koy
        if original_token:
            os.environ["HUGGINGFACE_TOKEN"] = original_token

# Export list
__all__ = [
    'SimpleAISupport',
    'run_simple_ai_support',
    'setup_token_guide',
    'demo_usage',
    'check_system',
    'quick_start',
    'test_mock_mode'
]

import os
os.environ["HUGGINGFACE_TOKEN"] = "x"

# 5. Sistemi baÅŸlat
support = run_simple_ai_support()

