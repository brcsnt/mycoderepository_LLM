# -*- coding: utf-8 -*-
"""app_ai.ipynb adlı not defterinin kopyası

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LurhiR35H2fWQRurKZdqLabVc2U1YeO2
"""

# 1. Colab'da gerekli paketleri yükle
!pip install transformers torch ipywidgets accelerate

# AI Müşteri Destek Sistemi - Google Colab Notebook
# Hugging Face LLM entegrasyonlu versiyon

import os
import sys
import logging
import json
from pathlib import Path
from IPython.display import display, HTML, clear_output
import ipywidgets as widgets
from typing import Dict, List, Optional, Any

# Logging setup
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ColabConfigManager:
    """Google Colab için optimize edilmiş config manager"""

    def __init__(self):
        self.load_config()

    def load_config(self):
        """Configuration ayarları"""
        # Hugging Face token - Colab secrets'dan al
        try:
            from google.colab import userdata
            self.hf_token = userdata.get("HUGGINGFACE_TOKEN")
            print("Token Colab secrets'dan alındı")
        except:
            self.hf_token = os.getenv("HUGGINGFACE_TOKEN", "")
            if self.hf_token:
                print("Token environment'dan alındı")
            else:
                print("Hugging Face token bulunamadı")

        # Model ayarları - Colab için optimize edilmiş
        self.model_id = "google/gemma-2-2b-it"
        # Alternatif modeller:
        # "microsoft/DialoGPT-small" - En hafif
        # "facebook/blenderbot-400M-distill" - Konuşma için iyi
        # "google/flan-t5-base" - Instruction following

        self.max_new_tokens = 512
        self.temperature = 0.3
        self.use_real_llm = bool(self.hf_token)  # Token varsa gerçek model kullan

        # Dil ayarları
        self.supported_languages = ["tr", "en"]
        self.default_language = "tr"

        # Colab path yapısı
        self.base_path = "/content"
        self.data_path = "/content/data"

        # Dizinleri oluştur
        os.makedirs(self.data_path, exist_ok=True)

    def get_hf_token(self) -> str:
        return self.hf_token

    def get_model_id(self) -> str:
        return self.model_id

    def get_max_tokens(self) -> int:
        return self.max_new_tokens

    def get_temperature(self) -> float:
        return self.temperature


class ColabLLMService:
    """Google Colab için optimize edilmiş LLM servisi"""

    def __init__(self, config: ColabConfigManager):
        self.config = config
        self.model = None
        self.tokenizer = None
        self.device = None
        self.initialized = False

        if config.use_real_llm:
            self.initialize_model()

    def initialize_model(self):
        """Model ve tokenizer'ı yükle"""
        try:
            import torch
            from transformers import AutoTokenizer, AutoModelForCausalLM

            print("Model yükleniyor...")

            # GPU kontrolü
            if torch.cuda.is_available():
                self.device = "cuda"
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
                print(f"GPU bulundu: {gpu_memory:.1f}GB")

                # Memory'ye göre model seçimi
                if gpu_memory < 8:
                    print("Düşük GPU memory, küçük model kullanılıyor")
                    self.config.model_id = "google/gemma-2-2b-it"
            else:
                self.device = "cpu"
                print("GPU bulunamadı, CPU kullanılıyor")
                self.config.model_id = "google/gemma-2-2b-it"

            # Tokenizer yükle
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.config.model_id,
                token=self.config.hf_token,
                trust_remote_code=True
            )

            # Pad token kontrolü
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

            # Model yükle
            self.model = AutoModelForCausalLM.from_pretrained(
                self.config.model_id,
                token=self.config.hf_token,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                device_map="auto" if self.device == "cuda" else None,
                trust_remote_code=True
            )

            if self.device == "cpu":
                self.model = self.model.to(self.device)

            self.initialized = True
            print(f"Model başarıyla yüklendi: {self.config.model_id}")

        except Exception as e:
            print(f"Model yükleme hatası: {e}")
            print("Mock mode'a geçiliyor...")
            self.initialized = False

    def generate_response(self, prompt: str, max_tokens: int = 256) -> str:
        """Response üret"""
        if not self.initialized or not self.model:
            return self._mock_response(prompt)

        try:
            import torch

            # Input hazırla
            inputs = self.tokenizer.encode(prompt + self.tokenizer.eos_token, return_tensors="pt")
            inputs = inputs.to(self.device)

            # Generate
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_length=inputs.shape[1] + max_tokens,
                    temperature=self.config.temperature,
                    do_sample=True,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    no_repeat_ngram_size=2
                )

            # Decode
            response = self.tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)
            return response.strip()

        except Exception as e:
            print(f"Generation hatası: {e}")
            return self._mock_response(prompt)

    def _mock_response(self, prompt: str) -> str:
        """Mock response (model çalışmazsa)"""
        if "problem" in prompt.lower() or "sorun" in prompt.lower():
            return "Sorununuzu anlıyorum. Size yardımcı olmaya çalışacağım."
        elif "?" in prompt:
            return "Bu konuda daha fazla bilgiye ihtiyacım var."
        else:
            return "Anlayışınız için teşekkürler. Devam edelim."

    def extract_information_with_llm(self, text: str, questions: List[Dict], language: str) -> Dict:
        """LLM ile bilgi çıkarımı"""
        if not self.initialized:
            return self._mock_extraction(text, questions)

        # Prompt oluştur
        questions_text = "\n".join([f"- {q['key']}: {q['question']}" for q in questions])

        if language == "tr":
            prompt = f"""Metinden bilgi çıkar:
Metin: "{text}"

Sorular:
{questions_text}

Sadece metinde açıkça belirtilen bilgileri çıkar. Yoksa "unknown" yaz.
JSON formatında yanıtla.
"""
        else:
            prompt = f"""Extract information from text:
Text: "{text}"

Questions:
{questions_text}

Extract only clearly stated information. Use "unknown" if not found.
Respond in JSON format.
"""

        response = self.generate_response(prompt, 256)

        # JSON parse et
        try:
            import json
            import re

            # JSON bul
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                return json.loads(json_str)
            else:
                return self._mock_extraction(text, questions)

        except:
            return self._mock_extraction(text, questions)

    def _mock_extraction(self, text: str, questions: List[Dict]) -> Dict:
        """Mock bilgi çıkarımı"""
        extracted = {}
        text_lower = text.lower()

        for question in questions:
            key = question['key']

            # Basit pattern matching
            if any(word in text_lower for word in ['atm', 'bankamatik']):
                if key in ['lokasyon', 'location']:
                    extracted[key] = "ATM lokasyonu belirtilmedi"
                elif key in ['problem_tipi', 'problem_type']:
                    extracted[key] = "ATM sorunu"
                else:
                    extracted[key] = "unknown"
            elif any(word in text_lower for word in ['kart', 'card']):
                if key in ['problem_tipi', 'problem_type']:
                    extracted[key] = "Kart sorunu"
                else:
                    extracted[key] = "unknown"
            elif any(word in text_lower for word in ['para', 'money', 'tl', 'lira', 'dollar']):
                if key in ['para_miktari', 'amount', 'tutar']:
                    import re
                    numbers = re.findall(r'\d+', text)
                    if numbers:
                        extracted[key] = f"{numbers[0]} TL"
                    else:
                        extracted[key] = "Miktar belirtilmedi"
                else:
                    extracted[key] = "unknown"
            else:
                extracted[key] = "unknown"

        return extracted


class ColabAISupport:
    """Google Colab için AI Destek Sistemi - Gerçek LLM Entegrasyonlu"""

    def __init__(self):
        print("AI Destek Sistemi başlatılıyor...")

        self.config = ColabConfigManager()
        self.llm_service = ColabLLMService(self.config)

        self.conversation_history = []
        self.current_conversation_id = None
        self.current_language = "tr"
        self.selected_category = None
        self.conversation_state = "idle"
        self.extracted_info = {}
        self.missing_questions = []
        self.current_question_index = 0

        # Kategoriler
        self.categories = {
            "tr": {
                "ATM": [
                    {"key": "problem_tipi", "question": "ATM'de yaşadığınız problem ne?"},
                    {"key": "para_miktari", "question": "Ne kadar paranız sıkıştı?"},
                    {"key": "lokasyon", "question": "Hangi ATM'de problem yaşadınız?"},
                    {"key": "tarih_saat", "question": "Problem hangi tarih ve saatte yaşandı?"}
                ],
                "Kredi Kartı": [
                    {"key": "problem_tipi", "question": "Kredi kartınızla ilgili problem ne?"},
                    {"key": "islem_tarihi", "question": "İşlem tarihi neydi?"},
                    {"key": "islem_miktari", "question": "İşlem tutarı ne kadardı?"},
                    {"key": "merchant", "question": "İşlemi yaptığınız yer neresi?"}
                ],
                "İnternet Bankacılığı": [
                    {"key": "problem_tipi", "question": "İnternet bankacılığı ile ilgili problem ne?"},
                    {"key": "cihaz_bilgisi", "question": "Hangi cihazdan bağlanmaya çalıştınız?"},
                    {"key": "tarayici", "question": "Hangi web tarayıcısını kullanıyorsunuz?"},
                    {"key": "hata_mesaji", "question": "Aldığınız hata mesajı neydi?"}
                ]
            },
            "en": {
                "ATM": [
                    {"key": "problem_type", "question": "What problem did you experience at the ATM?"},
                    {"key": "amount", "question": "How much money got stuck?"},
                    {"key": "location", "question": "Which ATM did you have the problem at?"},
                    {"key": "datetime", "question": "When did the problem occur?"}
                ],
                "Credit Card": [
                    {"key": "problem_type", "question": "What's the problem with your credit card?"},
                    {"key": "transaction_date", "question": "When was the transaction?"},
                    {"key": "amount", "question": "What was the transaction amount?"},
                    {"key": "merchant", "question": "Where was the transaction made?"}
                ]
            }
        }

        self.setup_ui()

        if self.config.use_real_llm:
            print(f"Sistem hazır - Gerçek LLM kullanılıyor: {self.config.model_id}")
        else:
            print("Sistem hazır - Mock mode (HF token gerekli)")

    def setup_ui(self):
        """UI bileşenlerini hazırla"""
        # Dil seçimi
        self.language_dropdown = widgets.Dropdown(
            options=['tr', 'en'],
            value='tr',
            description='Dil:',
            style={'description_width': 'initial'}
        )

        # Kategori seçimi
        self.category_dropdown = widgets.Dropdown(
            options=list(self.categories['tr'].keys()),
            value=None,
            description='Kategori:',
            style={'description_width': 'initial'}
        )

        # Mesaj input
        self.message_input = widgets.Textarea(
            placeholder='Mesajınızı yazın...',
            layout=widgets.Layout(width='100%', height='100px')
        )

        # Gönder butonu
        self.send_button = widgets.Button(
            description='Gönder',
            button_style='primary',
            layout=widgets.Layout(width='100px')
        )

        # Reset butonu
        self.reset_button = widgets.Button(
            description='Yeni Konuşma',
            button_style='warning',
            layout=widgets.Layout(width='150px')
        )

        # Chat output
        self.chat_output = widgets.Output(
            layout=widgets.Layout(height='400px', border='1px solid #ccc', padding='10px')
        )

        # Event handlers
        self.language_dropdown.observe(self.on_language_change, names='value')
        self.category_dropdown.observe(self.on_category_change, names='value')
        self.send_button.on_click(self.on_send_message)
        self.reset_button.on_click(self.on_reset_conversation)

    def on_language_change(self, change):
        """Dil değiştiğinde kategori listesini güncelle"""
        new_lang = change['new']
        self.current_language = new_lang

        # Kategori dropdown'unu güncelle
        available_categories = list(self.categories[new_lang].keys())
        self.category_dropdown.options = available_categories
        self.category_dropdown.value = None
        self.selected_category = None

        self.update_chat_display("info", f"Dil değiştirildi: {new_lang.upper()}")

    def on_category_change(self, change):
        """Kategori değiştiğinde"""
        self.selected_category = change['new']
        if self.selected_category:
            self.update_chat_display("info", f"Kategori seçildi: {self.selected_category}")

    def on_send_message(self, button):
        """Mesaj gönder"""
        message = self.message_input.value.strip()
        if not message:
            return

        if not self.selected_category:
            self.update_chat_display("error", "Lütfen önce bir kategori seçin!")
            return

        # Mesajı temizle
        self.message_input.value = ""

        # Mesajı görüntüle
        self.update_chat_display("user", message)

        # Mesajı işle
        self.process_message(message)

    def on_reset_conversation(self, button):
        """Konuşmayı sıfırla"""
        self.conversation_history = []
        self.current_conversation_id = None
        self.conversation_state = "idle"
        self.extracted_info = {}
        self.missing_questions = []
        self.current_question_index = 0

        self.chat_output.clear_output()
        self.update_chat_display("info", "Yeni konuşma başlatıldı")

    def update_chat_display(self, message_type: str, content: str):
        """Chat ekranını güncelle"""
        colors = {
            "user": "#007bff",
            "bot": "#28a745",
            "info": "#ffc107",
            "error": "#dc3545"
        }

        icons = {
            "user": "Kullanıcı",
            "bot": "Bot",
            "info": "Bilgi",
            "error": "Hata"
        }

        color = colors.get(message_type, "#6c757d")
        icon = icons.get(message_type, "Mesaj")

        with self.chat_output:
            display(HTML(f"""
            <div style="margin: 10px 0; padding: 10px; border-left: 4px solid {color}; background-color: #f8f9fa;">
                <strong style="color: {color};">{icon}</strong><br>
                {content}
            </div>
            """))

    def process_message(self, message: str):
        """Mesajı işle"""
        try:
            if self.conversation_state == "idle":
                # İlk mesaj - bilgi çıkarımı yap
                self.extract_information(message)
            elif self.conversation_state == "collecting":
                # Soru cevabı - cevabı işle
                self.process_answer(message)
            else:
                self.update_chat_display("error", "Beklenmeyen durum")

        except Exception as e:
            logger.error(f"Mesaj işleme hatası: {e}")
            self.update_chat_display("error", f"İşleme hatası: {str(e)}")

    def extract_information(self, user_text: str):
        """Metinden bilgi çıkarımı - Gerçek LLM ile"""
        try:
            category_questions = self.categories[self.current_language][self.selected_category]

            # LLM ile bilgi çıkarımı
            extracted = self.llm_service.extract_information_with_llm(
                user_text, category_questions, self.current_language
            )

            self.extracted_info = extracted

            # Eksik bilgileri tespit et
            self.missing_questions = []
            for question in category_questions:
                key = question['key']
                if extracted.get(key, "unknown") in ["unknown", "", None, "bilinmiyor"]:
                    self.missing_questions.append(question)

            # Sonucu göster
            self.show_extraction_result()

        except Exception as e:
            logger.error(f"Bilgi çıkarım hatası: {e}")
            self.update_chat_display("error", f"Bilgi çıkarımında hata: {str(e)}")

    def show_extraction_result(self):
        """Çıkarım sonucunu göster"""
        # Çıkarılan bilgileri göster
        if self.extracted_info:
            info_text = "<br>".join([f"• {k}: {v}" for k, v in self.extracted_info.items() if v != "unknown"])
            if info_text:
                self.update_chat_display("bot", f"Tespit edilen bilgiler:<br>{info_text}")

        # Eksik bilgi varsa soru sor
        if self.missing_questions:
            self.conversation_state = "collecting"
            self.current_question_index = 0
            self.ask_next_question()
        else:
            self.complete_conversation()

    def generate_smart_question(self, question_data: Dict) -> str:
        """LLM ile akıllı soru üretimi"""
        if not self.llm_service.initialized:
            return question_data['question']  # Fallback

        context = " ".join([msg.get('content', '') for msg in self.conversation_history[-3:]])

        if self.current_language == "tr":
            prompt = f"""Konuşma bağlamı: {context}

Sormak istediğim soru: {question_data['question']}

Bu soruyu konuşma akışına daha uygun ve doğal bir şekilde sor. Samimi ol.
Sadece soruyu döndür, başka açıklama yapma."""
        else:
            prompt = f"""Conversation context: {context}

Question to ask: {question_data['question']}

Make this question more natural and conversational. Be friendly.
Return only the question, no other explanation."""

        smart_question = self.llm_service.generate_response(prompt, 100)

        # Basit temizlik
        if smart_question and len(smart_question) > 10:
            return smart_question.strip()
        else:
            return question_data['question']

    def ask_next_question(self):
        """Sonraki soruyu sor - LLM ile akıllı soru"""
        if self.current_question_index < len(self.missing_questions):
            question_data = self.missing_questions[self.current_question_index]

            # LLM ile akıllı soru oluştur
            smart_question = self.generate_smart_question(question_data)

            progress = f"({self.current_question_index + 1}/{len(self.missing_questions)})"
            self.update_chat_display("bot", f"{progress} {smart_question}")

            # Konuşma geçmişine ekle
            self.conversation_history.append({
                "role": "bot",
                "content": smart_question,
                "question_key": question_data['key']
            })
        else:
            self.complete_conversation()

    def validate_answer_with_llm(self, question_key: str, question_text: str, answer: str) -> Dict:
        """LLM ile cevap validasyonu"""
        if not self.llm_service.initialized:
            return {"valid": True, "extracted_info": answer}  # Fallback

        if self.current_language == "tr":
            prompt = f"""Soru: {question_text}
Cevap: "{answer}"

Bu cevap soruya uygun mu? Değerlendir.

JSON formatında yanıtla:
{{
    "valid": true/false,
    "confidence": 0-100,
    "extracted_info": "çıkarılan bilgi"
}}"""
        else:
            prompt = f"""Question: {question_text}
Answer: "{answer}"

Is this answer appropriate for the question? Evaluate.

Respond in JSON format:
{{
    "valid": true/false,
    "confidence": 0-100,
    "extracted_info": "extracted information"
}}"""

        response = self.llm_service.generate_response(prompt, 150)

        try:
            import json
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                return json.loads(json_match.group(0))
        except:
            pass

        # Fallback validation
        return {"valid": True, "extracted_info": answer}

    def process_answer(self, answer: str):
        """Kullanıcı cevabını işle - LLM ile validasyon"""
        if self.current_question_index < len(self.missing_questions):
            current_question = self.missing_questions[self.current_question_index]
            key = current_question['key']

            # LLM ile cevabı validate et
            validation = self.validate_answer_with_llm(
                key, current_question['question'], answer
            )

            # Cevabı kaydet
            if validation.get('valid', True):
                extracted_info = validation.get('extracted_info', answer)
                self.extracted_info[key] = extracted_info

                # Konuşma geçmişine ekle
                self.conversation_history.append({
                    "role": "user",
                    "content": answer,
                    "question_key": key
                })
            else:
                # Geçersiz cevap - tekrar sor veya atla
                self.update_chat_display("bot", "Bu bilgiyi tam anlayamadım. Devam edelim.")
                self.extracted_info[key] = "belirsiz"

            # Sonraki soruya geç
            self.current_question_index += 1

            if self.current_question_index < len(self.missing_questions):
                self.ask_next_question()
            else:
                self.complete_conversation()

    def complete_conversation(self):
        """Konuşmayı tamamla"""
        self.conversation_state = "completed"

        # Özet oluştur
        summary_text = "<h4>Toplanan Bilgiler:</h4>"
        for key, value in self.extracted_info.items():
            if value and value != "unknown":
                readable_key = key.replace('_', ' ').title()
                summary_text += f"• <strong>{readable_key}:</strong> {value}<br>"

        summary_text += "<br><em>Bilgileriniz kaydedildi ve ilgili birime iletilecektir.</em>"

        self.update_chat_display("bot", summary_text)
        self.update_chat_display("info", "Konuşma tamamlandı. Yeni konuşma başlatabilirsiniz.")

    def display(self):
        """Ana interface'i göster"""
        # Başlık
        display(HTML("<h2>AI Müşteri Destek Sistemi</h2>"))

        # Kontrol paneli
        controls = widgets.HBox([
            self.language_dropdown,
            self.category_dropdown,
            self.reset_button
        ])

        # Mesaj girişi
        message_area = widgets.VBox([
            widgets.Label("Mesajınızı yazın:"),
            self.message_input,
            self.send_button
        ])

        # Ana layout
        main_layout = widgets.VBox([
            controls,
            widgets.HTML("<hr>"),
            widgets.Label("Sohbet:"),
            self.chat_output,
            widgets.HTML("<hr>"),
            message_area
        ])

        display(main_layout)

        # Başlangıç mesajı
        self.update_chat_display("info", "Sistem hazır! Yukarıdan dil ve kategori seçin, sonra mesajınızı yazın.")


# Kullanım fonksiyonları
def setup_colab_environment():
    """Colab ortamını hazırla"""
    print("Gerekli paketler yükleniyor...")

    try:
        import subprocess
        import sys

        # Gerekli paketleri yükle
        packages = [
            "transformers",
            "torch",
            "ipywidgets",
            "accelerate"
        ]

        for package in packages:
            subprocess.check_call([sys.executable, "-m", "pip", "install", package])

        print("Paketler başarıyla yüklendi!")
        return True

    except Exception as e:
        print(f"Paket yükleme hatası: {e}")
        return False

def setup_hf_token():
    """HuggingFace token kurulumu rehberi"""
    print("""
HuggingFace Token Kurulumu:

1. https://huggingface.co/settings/tokens adresine gidin
2. 'New token' ile yeni token oluşturun
3. Token'ı kopyalayın

Colab'da kullanım için iki seçenek:

A) Colab Secrets (Önerilen):
   - Sol panelde 'Secrets' (🔑) bölümüne tıklayın
   - 'Add new secret' butonuna tıklayın
   - Name: HUGGINGFACE_TOKEN
   - Value: [token'ınızı yapıştırın]
   - Save edin

B) Geçici kullanım:
   import os
   os.environ["HUGGINGFACE_TOKEN"] = "your_token_here"
    """)

# Ana çalıştırma fonksiyonu
def run_ai_support():
    """AI destek sistemini çalıştır"""
    print("AI Müşteri Destek Sistemi başlatılıyor...\n")

    # Sistemi başlat
    support = ColabAISupport()
    support.display()

    return support

# Kullanım örneği
if __name__ == "__main__":
    print("AI Müşteri Destek Sistemi - Google Colab")
    print("=" * 50)

    # Ortamı hazırla
    if setup_colab_environment():
        print("\nToken kurulumu için setup_hf_token() fonksiyonunu çalıştırın")
        print("Sistemi başlatmak için run_ai_support() fonksiyonunu çalıştırın")

    # Örnek kullanım:
    # setup_hf_token()  # Token kurulum rehberi
    # support = run_ai_support()  # Sistemi başlat

# 2. HF Token'ı ayarla (iki yoldan biri):
# Yol 1: Colab Secrets (Önerilen)
# - Sol panelde 🔑 Secrets'e tıkla
# - "HUGGINGFACE_TOKEN" ekle
# - Token'ı yapıştır

# Yol B: Geçici
import os
os.environ["HUGGINGFACE_TOKEN"] = "x"

# 5. Sistemi başlat
support = run_ai_support()

# Sistemi başlattıktan sonra:
support.selected_category = "ATM"
support.process_message("ATM'de param sıkıştı, 500 TL kaybettim, Ankara'daki Kızılay ATM'sinde")













"""
AI Müşteri Destek Sistemi - Google Colab Simple Version
Widget kullanmayan, sadece input/print ile çalışan versiyon
"""

import os
import sys
import logging
import json
import time
import re
from datetime import datetime
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
from enum import Enum

# Logging setup
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class ConversationState(Enum):
    """Konuşma durumları"""
    IDLE = "idle"
    COLLECTING = "collecting"
    COMPLETED = "completed"
    ERROR = "error"


@dataclass
class QuestionData:
    """Soru veri yapısı"""
    key: str
    question: str
    validation_pattern: Optional[str] = None
    required: bool = True


class SimpleConfigManager:
    """Basit konfigürasyon yöneticisi"""

    def __init__(self):
        self.load_config()
        self.detect_environment()

    def load_config(self):
        """Konfigürasyon yükle"""
        # Token yönetimi
        self.hf_token = self._get_token()

        # Model ayarları
        self.available_memory = self._get_gpu_memory()
        self.model_id = self._get_optimal_model()

        # Generation parametreleri
        self.generation_config = {
            "max_new_tokens": 256,
            "temperature": 0.7,
            "do_sample": True,
            "top_p": 0.9,
            "repetition_penalty": 1.1,
            "no_repeat_ngram_size": 3
        }

        # Yol ayarları
        self.base_path = "/content"
        self.data_path = "/content/ai_support_data"
        os.makedirs(self.data_path, exist_ok=True)

        # Debug mod
        self.debug_mode = os.getenv("DEBUG", "false").lower() == "true"

    def _get_token(self) -> str:
        """Token'ı al"""
        token = os.getenv("HUGGINGFACE_TOKEN", "")
        if token:
            print("✅ HuggingFace token bulundu")
            return token
        else:
            print("⚠️ HuggingFace token bulunamadı - Mock mode")
            return ""

    def _get_gpu_memory(self) -> float:
        """GPU memory miktarını tespit et"""
        try:
            import torch
            if torch.cuda.is_available():
                memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                print(f"🎮 GPU Memory: {memory_gb:.1f}GB")
                return memory_gb
            else:
                print("💻 CPU kullanılacak")
                return 0.0
        except ImportError:
            print("💻 PyTorch bulunamadı - CPU mode")
            return 0.0

    def _get_optimal_model(self) -> str:
        """Optimal model seç"""
        if self.available_memory >= 6:
            return "microsoft/DialoGPT-medium"
        elif self.available_memory >= 4:
            return "google/gemma-2-2b-it"
        else:
            return "microsoft/DialoGPT-small"

    def detect_environment(self):
        """Çalışma ortamını tespit et"""
        self.environment = {
            "gpu_available": self.available_memory > 0,
            "token_available": bool(self.hf_token),
            "model_selected": self.model_id
        }


class SimpleLLMService:
    """Basit LLM servisi"""

    def __init__(self, config: SimpleConfigManager):
        self.config = config
        self.model = None
        self.tokenizer = None
        self.device = "cpu"
        self.initialized = False

        if config.hf_token:
            self.initialize_model()

    def initialize_model(self):
        """Model yükleme"""
        try:
            print(f"🔄 Model yükleniyor: {self.config.model_id}")
            print("   Bu işlem 1-3 dakika sürebilir...")

            import torch
            from transformers import AutoTokenizer, AutoModelForCausalLM

            # Device seçimi
            if torch.cuda.is_available() and self.config.available_memory > 2:
                self.device = "cuda"
                dtype = torch.float16
            else:
                self.device = "cpu"
                dtype = torch.float32

            # Tokenizer yükle
            print("   📚 Tokenizer yükleniyor...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.config.model_id,
                token=self.config.hf_token,
                trust_remote_code=True
            )

            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

            # Model yükle
            print("   🧠 Model yükleniyor...")
            self.model = AutoModelForCausalLM.from_pretrained(
                self.config.model_id,
                torch_dtype=dtype,
                trust_remote_code=True,
                token=self.config.hf_token,
                device_map="auto" if self.device == "cuda" else None
            )

            if self.device == "cpu":
                self.model = self.model.to(self.device)

            self.initialized = True
            print(f"✅ Model başarıyla yüklendi!")
            print(f"   📱 Device: {self.device}")

        except Exception as e:
            print(f"❌ Model yükleme hatası: {e}")
            print("🔄 Mock mode'a geçiliyor...")
            self.initialized = False

    def generate_response(self, prompt: str, max_tokens: int = None) -> str:
        """Response üretimi"""
        if not self.initialized:
            return self._mock_response(prompt)

        try:
            max_tokens = max_tokens or self.config.generation_config["max_new_tokens"]

            inputs = self.tokenizer.encode(prompt, return_tensors="pt").to(self.device)

            gen_kwargs = self.config.generation_config.copy()
            gen_kwargs["max_new_tokens"] = max_tokens
            gen_kwargs["pad_token_id"] = self.tokenizer.pad_token_id
            gen_kwargs["eos_token_id"] = self.tokenizer.eos_token_id

            with torch.no_grad():
                outputs = self.model.generate(inputs, **gen_kwargs)

            response = self.tokenizer.decode(
                outputs[0][inputs.shape[1]:],
                skip_special_tokens=True
            ).strip()

            return response

        except Exception as e:
            print(f"⚠️ Generation hatası: {e}")
            return self._mock_response(prompt)

    def _mock_response(self, prompt: str) -> str:
        """Mock yanıtlar"""
        prompt_lower = prompt.lower()

        # Türkçe patterns
        if any(word in prompt_lower for word in ["sorun", "problem", "hata"]):
            return "Sorununuzu anlıyorum. Bu konuda size yardımcı olmaya çalışacağım."
        elif any(word in prompt_lower for word in ["teşekkür", "sağol"]):
            return "Rica ederim, yardımcı olabildiysem ne mutlu bana."
        elif any(word in prompt_lower for word in ["merhaba", "selam"]):
            return "Merhaba! Size nasıl yardımcı olabilirim?"
        elif "?" in prompt:
            return "Bu konuda daha fazla bilgiye ihtiyacım var."
        else:
            return "Anlayışınız için teşekkürler. Devam edelim."

    def extract_information(self, text: str, questions: List[QuestionData], language: str) -> Dict[str, str]:
        """Akıllı bilgi çıkarımı"""
        if not self.initialized:
            return self._mock_extraction(text, questions, language)

        # Structured prompt
        questions_text = "\n".join([f"- {q.key}: {q.question}" for q in questions])

        if language == "tr":
            prompt = f"""Metinden bilgi çıkar:

METIN: "{text}"

ÇIKARILACAK BİLGİLER:
{questions_text}

KURALLAR:
- Sadece metinde açıkça belirtilen bilgileri çıkar
- Bilgi yoksa "bilinmiyor" yaz
- JSON formatında yanıtla

JSON:"""
        else:
            prompt = f"""Extract information from text:

TEXT: "{text}"

INFORMATION TO EXTRACT:
{questions_text}

RULES:
- Extract only clearly stated information
- Use "unknown" if not found
- Respond in JSON format

JSON:"""

        response = self.generate_response(prompt, 300)

        # JSON parse
        try:
            json_match = re.search(r'\{[^{}]*\}', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                extracted = json.loads(json_str)

                # Validation
                validated = {}
                for question in questions:
                    key = question.key
                    value = extracted.get(key, "bilinmiyor" if language == "tr" else "unknown")
                    validated[key] = value

                return validated
            else:
                return self._mock_extraction(text, questions, language)

        except Exception as e:
            return self._mock_extraction(text, questions, language)

    def _mock_extraction(self, text: str, questions: List[QuestionData], language: str) -> Dict[str, str]:
        """Mock extraction"""
        extracted = {}
        text_lower = text.lower()
        unknown_word = "bilinmiyor" if language == "tr" else "unknown"

        for question in questions:
            key = question.key
            value = unknown_word

            # Basit pattern matching
            if any(word in text_lower for word in ['atm', 'bankamatik']):
                if "problem" in key or "tip" in key:
                    value = "ATM sorunu"
                elif "lokasyon" in key or "location" in key:
                    value = "ATM lokasyonu belirtilmedi"
            elif any(word in text_lower for word in ['kart', 'card']):
                if "problem" in key or "tip" in key:
                    value = "Kart sorunu"
            elif any(word in text_lower for word in ['para', 'money', 'tl']):
                if "miktar" in key or "amount" in key:
                    import re
                    numbers = re.findall(r'\d+', text)
                    if numbers:
                        value = f"{numbers[0]} TL"

            extracted[key] = value

        return extracted


class SimpleAISupport:
    """Basit AI Destek Sistemi - Console tabanlı"""

    def __init__(self):
        print("🤖 AI Müşteri Destek Sistemi")
        print("=" * 50)

        self.config = SimpleConfigManager()
        self.llm_service = SimpleLLMService(self.config)

        # State management
        self.current_language = "tr"
        self.selected_category = None
        self.conversation_state = ConversationState.IDLE
        self.extracted_info = {}
        self.missing_questions = []
        self.current_question_index = 0
        self.conversation_history = []

        # Categories
        self.categories = {
            "tr": {
                "ATM": [
                    QuestionData("problem_tipi", "ATM'de yaşadığınız problem ne?", required=True),
                    QuestionData("para_miktari", "Ne kadar paranız sıkıştı veya eksik çekildi?", required=False),
                    QuestionData("lokasyon", "Hangi ATM'de problem yaşadınız? (Şube/Adres)", required=True),
                    QuestionData("tarih_saat", "Problem hangi tarih ve saatte yaşandı?", required=True),
                    QuestionData("islem_numarasi", "İşlem numaranız var mı?", required=False)
                ],
                "Kredi Kartı": [
                    QuestionData("problem_tipi", "Kredi kartınızla ilgili problem ne?", required=True),
                    QuestionData("islem_tarihi", "İşlem tarihi neydi?", required=True),
                    QuestionData("islem_miktari", "İşlem tutarı ne kadardı?", required=True),
                    QuestionData("merchant", "İşlemi yaptığınız yer neresi?", required=True),
                    QuestionData("kart_son4", "Kartınızın son 4 hanesi nedir?", required=False)
                ],
                "İnternet Bankacılığı": [
                    QuestionData("problem_tipi", "İnternet bankacılığı ile ilgili problem ne?", required=True),
                    QuestionData("cihaz_bilgisi", "Hangi cihazdan bağlanmaya çalıştınız?", required=True),
                    QuestionData("tarayici", "Hangi web tarayıcısını kullanıyorsunuz?", required=False),
                    QuestionData("hata_mesaji", "Aldığınız hata mesajı neydi?", required=True)
                ],
                "Genel Şikayet": [
                    QuestionData("sikayet_konusu", "Şikayetinizin konusu nedir?", required=True),
                    QuestionData("yaşanan_durum", "Yaşadığınız durumu detaylı anlatır mısınız?", required=True),
                    QuestionData("beklenti", "Bu konuda beklentiniz nedir?", required=True)
                ]
            },
            "en": {
                "ATM": [
                    QuestionData("problem_type", "What problem did you experience at the ATM?", required=True),
                    QuestionData("amount", "How much money got stuck?", required=False),
                    QuestionData("location", "Which ATM did you have the problem at?", required=True),
                    QuestionData("datetime", "When did the problem occur?", required=True)
                ],
                "Credit Card": [
                    QuestionData("problem_type", "What's the problem with your credit card?", required=True),
                    QuestionData("transaction_date", "When was the transaction?", required=True),
                    QuestionData("amount", "What was the transaction amount?", required=True),
                    QuestionData("merchant", "Where was the transaction made?", required=True)
                ]
            }
        }

        self.print_startup_info()

    def print_startup_info(self):
        """Başlangıç bilgileri"""
        status = "✅ GERÇEK AI" if self.llm_service.initialized else "⚠️ MOCK MODE"

        print(f"""
📊 SİSTEM BİLGİLERİ:
   Model: {self.config.model_id}
   Device: {self.llm_service.device}
   GPU Memory: {self.config.available_memory:.1f}GB
   Durum: {status}

🌍 Diller: Türkçe (tr), English (en)
📋 Kategoriler: {', '.join(self.categories['tr'].keys())}
""")

    def run(self):
        """Ana çalıştırma döngüsü"""
        print("\n🎯 AI Müşteri Destek Sistemi'ne Hoş Geldiniz!")
        print("-" * 50)

        while True:
            try:
                # Dil seçimi
                self.select_language()

                # Kategori seçimi
                self.select_category()

                # Ana konuşma
                self.start_conversation()

                # Devam etmek istiyor mu?
                if not self.ask_continue():
                    break

            except KeyboardInterrupt:
                print("\n\n👋 Görüşmek üzere!")
                break
            except Exception as e:
                print(f"\n❌ Hata oluştu: {e}")
                continue

    def select_language(self):
        """Dil seçimi"""
        while True:
            print(f"\n🌍 DİL SEÇİMİ (Mevcut: {self.current_language.upper()})")
            print("1. Türkçe (tr)")
            print("2. English (en)")
            print("3. Devam et")

            choice = input("Seçiminiz (1-3): ").strip()

            if choice == "1":
                self.current_language = "tr"
                print("✅ Türkçe seçildi")
                break
            elif choice == "2":
                self.current_language = "en"
                print("✅ English selected")
                break
            elif choice == "3":
                break
            else:
                print("❌ Geçersiz seçim!")

    def select_category(self):
        """Kategori seçimi"""
        categories = list(self.categories[self.current_language].keys())

        while True:
            print(f"\n📋 KATEGORİ SEÇİMİ")
            for i, category in enumerate(categories, 1):
                print(f"{i}. {category}")

            try:
                choice = input("Kategori numarası: ").strip()
                choice_num = int(choice)

                if 1 <= choice_num <= len(categories):
                    self.selected_category = categories[choice_num - 1]
                    print(f"✅ Seçilen kategori: {self.selected_category}")
                    break
                else:
                    print("❌ Geçersiz numara!")

            except ValueError:
                print("❌ Lütfen bir numara girin!")

    def start_conversation(self):
        """Ana konuşmayı başlat"""
        self.conversation_state = ConversationState.IDLE
        self.extracted_info = {}
        self.missing_questions = []
        self.current_question_index = 0

        print(f"\n💬 KONUŞMA BAŞLADI - {self.selected_category}")
        print("=" * 50)

        if self.current_language == "tr":
            print("Sorununuzu detaylı bir şekilde anlatın:")
            print("(Ne kadar detay verirseniz, o kadar az soru sorarım)")
        else:
            print("Please describe your problem in detail:")
            print("(The more details you provide, the fewer questions I'll ask)")

        print("\nÖrnek: 'ATM'den para çekerken kartım sıkıştı, 500 TL çekmeye çalışıyordum...'")
        print("-" * 50)

        # İlk mesajı al
        user_message = input("Mesajınız: ").strip()

        if not user_message:
            print("❌ Boş mesaj gönderemezsiniz!")
            return

        # Mesajı işle
        self.process_initial_message(user_message)

    def process_initial_message(self, message: str):
        """İlk mesajı işle"""
        print("\n🔍 Bilgiler çıkarılıyor...")

        category_questions = self.categories[self.current_language][self.selected_category]

        # Bilgi çıkarımı
        extracted = self.llm_service.extract_information(
            message, category_questions, self.current_language
        )

        self.extracted_info = extracted

        # Sonuçları göster
        self.show_extraction_results()

        # Eksik bilgileri tespit et
        self.find_missing_information(category_questions)

        # Eksik bilgi varsa sor
        if self.missing_questions:
            self.collect_missing_information()
        else:
            self.complete_conversation()

    def show_extraction_results(self):
        """Çıkarılan bilgileri göster"""
        valid_info = {k: v for k, v in self.extracted_info.items()
                     if v and str(v).lower() not in ["unknown", "bilinmiyor", "belirsiz"]}

        if valid_info:
            print("\n✅ TESPİT EDİLEN BİLGİLER:")
            print("-" * 30)
            for key, value in valid_info.items():
                readable_key = key.replace('_', ' ').title()
                print(f"• {readable_key}: {value}")
        else:
            print("\n⚠️ Henüz yeterli bilgi tespit edilemedi")

    def find_missing_information(self, category_questions):
        """Eksik bilgileri bul"""
        self.missing_questions = []

        for question in category_questions:
            key = question.key
            value = self.extracted_info.get(key, "")
            unknown_values = ["unknown", "bilinmiyor", "", None, "belirsiz"]

            if not value or str(value).lower() in unknown_values:
                if question.required:
                    self.missing_questions.append(question)

        if self.missing_questions:
            if self.current_language == "tr":
                print(f"\n❓ {len(self.missing_questions)} ek bilgiye ihtiyacım var.")
            else:
                print(f"\n❓ I need {len(self.missing_questions)} additional pieces of information.")

    def collect_missing_information(self):
        """Eksik bilgileri topla"""
        self.conversation_state = ConversationState.COLLECTING
        self.current_question_index = 0

        for i, question in enumerate(self.missing_questions):
            progress = f"({i+1}/{len(self.missing_questions)})"

            print(f"\n{progress} {question.question}")

            if not question.required:
                skip_text = "(isteğe bağlı - 'geç' yazabilirsiniz)" if self.current_language == "tr" else "(optional - type 'skip')"
                print(f"   {skip_text}")

            while True:
                answer = input("Yanıtınız: ").strip()

                if not answer:
                    print("❌ Boş yanıt verilemez!")
                    continue

                # Skip kontrolü
                skip_commands = ["geç", "skip", "bilmiyorum", "don't know"]
                if answer.lower() in skip_commands:
                    if not question.required:
                        print("⏭️ Atlandı")
                        self.extracted_info[question.key] = "atlandı" if self.current_language == "tr" else "skipped"
                        break
                    else:
                        required_msg = "Bu bilgi gerekli!" if self.current_language == "tr" else "This information is required!"
                        print(f"❌ {required_msg}")
                        continue

                # Yanıtı kaydet
                self.extracted_info[question.key] = answer
                print("✅ Kaydedildi")
                break

        self.complete_conversation()

    def complete_conversation(self):
        """Konuşmayı tamamla"""
        self.conversation_state = ConversationState.COMPLETED

        print(f"\n🎉 BİLGİ TOPLAMA TAMAMLANDI!")
        print("=" * 50)

        # Özet göster
        print("📋 TOPLANAN BİLGİLER:")
        print("-" * 25)

        for key, value in self.extracted_info.items():
            if value and str(value).lower() not in ["unknown", "bilinmiyor", "atlandı", "skipped"]:
                readable_key = key.replace('_', ' ').title()
                print(f"• {readable_key}: {value}")

        # Referans numarası
        ref_no = f"REF-{int(time.time())}"
        print(f"\n🔢 Referans Numarası: {ref_no}")

        if self.current_language == "tr":
            print("\n✅ Bilgileriniz kaydedildi ve ilgili birime iletilecektir.")
            print("📞 Gerekirse bu referans numarasıyla bize ulaşabilirsiniz.")
        else:
            print("\n✅ Your information has been recorded and will be forwarded.")
            print("📞 You can contact us with this reference number if needed.")

        # Dosyaya kaydet
        self.save_conversation(ref_no)

    def save_conversation(self, ref_no: str):
        """Konuşmayı dosyaya kaydet"""
        try:
            conversation_data = {
                "reference_no": ref_no,
                "timestamp": datetime.now().isoformat(),
                "language": self.current_language,
                "category": self.selected_category,
                "extracted_info": self.extracted_info,
                "status": "completed"
            }

            filename = f"conversation_{ref_no}.json"
            filepath = os.path.join(self.config.data_path, filename)

            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(conversation_data, f, ensure_ascii=False, indent=2)

            print(f"💾 Konuşma kaydedildi: {filename}")

        except Exception as e:
            print(f"⚠️ Kaydetme hatası: {e}")

    def ask_continue(self) -> bool:
        """Devam etmek isteyip istemediğini sor"""
        print("\n" + "="*50)

        if self.current_language == "tr":
            question = "Yeni bir konuşma başlatmak istiyor musunuz? (e/h): "
            yes_answers = ["e", "evet", "yes", "y"]
        else:
            question = "Would you like to start a new conversation? (y/n): "
            yes_answers = ["y", "yes", "e", "evet"]

        while True:
            answer = input(question).strip().lower()

            if answer in yes_answers:
                print("\n🔄 Yeni konuşma başlatılıyor...\n")
                return True
            elif answer in ["h", "hayır", "no", "n"]:
                return False
            else:
                print("❌ Geçersiz cevap! (e/h)")


# Ana fonksiyonlar
def setup_environment():
    """Ortamı hazırla"""
    print("🔧 Ortam kontrol ediliyor...")

    # Gerekli paketler
    required_packages = ["transformers", "torch"]
    missing_packages = []

    for package in required_packages:
        try:
            __import__(package)
            print(f"   ✅ {package}")
        except ImportError:
            missing_packages.append(package)
            print(f"   ❌ {package}")

    if missing_packages:
        print(f"\n⚠️ Eksik paketler: {', '.join(missing_packages)}")
        print("Önce şu komutu çalıştırın:")
        print(f"!pip install {' '.join(missing_packages)}")
        return False

    return True

def setup_token_guide():
    """Token kurulum rehberi"""
    print("""
🔑 HUGGINGFACE TOKEN KURULUMU
════════════════════════════════

1️⃣ Token Alma:
   https://huggingface.co/settings/tokens
   → "New token" → "Read" → Token'ı kopyala

2️⃣ Colab'da Ayarlama:
   import os
   os.environ["HUGGINGFACE_TOKEN"] = "your_token_here"

3️⃣ Sistemi Başlatma:
   support = run_simple_ai_support()
""")

def run_simple_ai_support():
    """Basit AI desteği çalıştır"""
    print("🚀 AI Müşteri Destek Sistemi başlatılıyor...\n")

    if not setup_environment():
        print("❌ Gereksinimler karşılanmadı!")
        return None

    try:
        support = SimpleAISupport()
        support.run()
        return support

    except Exception as e:
        print(f"❌ Başlatma hatası: {e}")
        import traceback
        traceback.print_exc()
        return None

def demo_usage():
    """Demo kullanım"""
    print("""
🎬 DEMO KULLANIM
═══════════════

1. Dil seçin: Türkçe (1)
2. Kategori seçin: ATM (1)
3. Problem yazın: "ATM'den para çekerken kartım sıkıştı, 500 TL çekmeye çalışıyordum"
4. Ek soruları yanıtlayın
5. Özeti görün

💡 Ne kadar detay verirseniz, o kadar az soru sorar!
""")

# Ana çalıştırma
if __name__ == "__main__":
    print("🤖 AI Müşteri Destek Sistemi - Basit Versiyon")
    print("=" * 60)

    print("\n📚 Kullanılabilir Fonksiyonlar:")
    print("• setup_token_guide() - Token kurulum rehberi")
    print("• run_simple_ai_support() - Sistemi başlat")
    print("• demo_usage() - Kullanım örneği")

    print("\n🚀 HIZLI BAŞLATMA:")
    print("support = run_simple_ai_support()")


# ===============================
# GOOGLE COLAB ÇALIŞTIRMA REHBERİ
# ===============================

"""
🎯 ADIM ADIM ÇALIŞTIRMA REHBERİ:

1️⃣ PAKET YÜKLEMESİ:
!pip install transformers torch

2️⃣ TOKEN AYARLAMA:
import os
os.environ["HUGGINGFACE_TOKEN"] = "hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"

3️⃣ KOD ÇALIŞTIRMA:
# Bu dosyayı tamamen çalıştırın (Ctrl+F9)

4️⃣ SİSTEM BAŞLATMA:
support = run_simple_ai_support()

📱 KULLANIM:
• Terminal tabanlı - widget yok!
• Dil seçimi (Türkçe/İngilizce)
• Kategori seçimi (ATM, Kredi Kartı, vs.)
• Problem yazma
• Soru-cevap
• Özet görme

💡 ÖZELLİKLER:
✅ Widget gerektirmez - sadece input/print
✅ Gerçek LLM entegrasyonu
✅ Akıllı bilgi çıkarımı
✅ Çoklu dil desteği
✅ Konuşma kaydı (JSON)
✅ GPU/CPU otomatik algılama
✅ Mock mode (token olmadan da çalışır)
✅ Hata yönetimi

🔧 SORUN GİDERME:
• Token yoksa mock mode'da çalışır
• GPU yoksa CPU kullanır
• Model yüklenemezse mock yanıtlar verir

🎬 ÖRNEK KULLANIM:
1. Sistemi başlatın: run_simple_ai_support()
2. Dil seçin: 1 (Türkçe)
3. Kategori: 1 (ATM)
4. Problem: "ATM'den para çekerken kartım sıkıştı"
5. Soruları yanıtlayın
6. Özeti görün
"""

def check_system():
    """Sistem durumunu kontrol et"""
    print("🔍 SİSTEM KONTROL")
    print("=" * 30)

    # Python version
    print(f"Python: {sys.version.split()[0]}")

    # Packages
    packages = ["transformers", "torch"]
    for package in packages:
        try:
            __import__(package)
            print(f"✅ {package}: Yüklü")
        except ImportError:
            print(f"❌ {package}: Eksik")

    # GPU
    try:
        import torch
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            print(f"🎮 GPU: {gpu_name} ({memory:.1f}GB)")
        else:
            print("💻 GPU: Bulunamadı (CPU kullanılacak)")
    except:
        print("💻 GPU: Kontrol edilemedi")

    # Token
    token = os.getenv("HUGGINGFACE_TOKEN", "")
    print(f"🔑 HF Token: {'✅ Ayarlanmış' if token else '❌ Eksik'}")

    print("\n" + "=" * 30)

def quick_start():
    """Hızlı başlatma"""
    print("""
🚀 HIZLI BAŞLATMA KILAVUZU
═════════════════════════

ADIM 1 - Paketleri yükleyin:
!pip install transformers torch

ADIM 2 - Token ayarlayın:
import os
os.environ["HUGGINGFACE_TOKEN"] = "your_token_here"

ADIM 3 - Kodu çalıştırın:
# Bu cell'i tamamen çalıştırın

ADIM 4 - Sistemi başlatın:
support = run_simple_ai_support()

ADIM 5 - Kullanın:
• Dil seçin (1-2)
• Kategori seçin (1-4)
• Probleminizi yazın
• Soruları yanıtlayın
• Özeti görün

💡 BONUS İPUÇLARI:
• Detaylı yazın → Az soru
• "geç" yazın → İsteğe bağlı soruları atla
• Token yoksa mock mode çalışır
• Her konuşma JSON olarak kaydedilir
""")

def test_mock_mode():
    """Mock mode testini çalıştır"""
    print("🧪 MOCK MODE TESTİ")
    print("=" * 25)

    # Token'ı geçici olarak kaldır
    original_token = os.environ.get("HUGGINGFACE_TOKEN", "")
    if original_token:
        del os.environ["HUGGINGFACE_TOKEN"]

    try:
        config = SimpleConfigManager()
        llm = SimpleLLMService(config)

        # Test yanıtları
        test_prompts = [
            "ATM'de problem yaşadım",
            "Merhaba",
            "Teşekkürler",
            "Kartım çalışmıyor"
        ]

        print("Mock yanıtları:")
        for prompt in test_prompts:
            response = llm.generate_response(prompt)
            print(f"• '{prompt}' → '{response}'")

        print("\n✅ Mock mode çalışıyor!")

    except Exception as e:
        print(f"❌ Mock test hatası: {e}")

    finally:
        # Token'ı geri koy
        if original_token:
            os.environ["HUGGINGFACE_TOKEN"] = original_token

# Export list
__all__ = [
    'SimpleAISupport',
    'run_simple_ai_support',
    'setup_token_guide',
    'demo_usage',
    'check_system',
    'quick_start',
    'test_mock_mode'
]

import os
os.environ["HUGGINGFACE_TOKEN"] = "x"

# 5. Sistemi başlat
support = run_simple_ai_support()

