{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Data Kit - Kendi Local LLM API Demo\n",
    "\n",
    "Bu notebook, Meta'nÄ±n Synthetic Data Kit'ini **kendi local LLM API'niz** ile kullanarak sentetik veri oluÅŸturmayÄ± gÃ¶sterir.\n",
    "\n",
    "## âš¡ Temel Ã–zellikler\n",
    "\n",
    "- âœ… **vLLM gerekmez** - Kendi API endpoint'inizi kullanÄ±n\n",
    "- âœ… **Script deÄŸil** - KÃ¼tÃ¼phane olarak kullanÄ±m\n",
    "- âœ… **DÄ±ÅŸarÄ±dan veri** - Herhangi bir text/PDF/DOCX dosyasÄ±ndan veri oluÅŸturma\n",
    "- âœ… **4 adÄ±mlÄ± pipeline** - Ingest â†’ Create â†’ Curate â†’ Save\n",
    "\n",
    "## ğŸ“‹ Ä°Ã§indekiler\n",
    "\n",
    "1. Kurulum\n",
    "2. Kendi API'nizi YapÄ±landÄ±rma\n",
    "3. Pratik Demo\n",
    "4. Programmatik KullanÄ±m (Python API)\n",
    "5. Batch Ä°ÅŸleme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1ï¸âƒ£ Kurulum\n",
    "\n",
    "Ä°lk olarak kÃ¼tÃ¼phaneyi pip ile kurun:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic Data Kit kurulumu\n",
    "!pip install synthetic-data-kit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerekli kÃ¼tÃ¼phaneler\n",
    "import os\n",
    "import json\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "print(\"âœ… Import'lar baÅŸarÄ±lÄ±!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2ï¸âƒ£ Kendi Local API'nizi YapÄ±landÄ±rma\n",
    "\n",
    "**Ã–NEMLÄ°:** Synthetic Data Kit, OpenAI-uyumlu API'lerle Ã§alÄ±ÅŸÄ±r. \n",
    "\n",
    "Kendi local LLM API'niz (Ollama, LM Studio, Text Generation WebUI, vb.) OpenAI formatÄ±nda endpoint saÄŸlÄ±yorsa direkt kullanabilirsiniz.\n",
    "\n",
    "### ğŸ”§ API Endpoint AyarlarÄ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ BURAYA KENDÄ° API BÄ°LGÄ°LERÄ°NÄ°ZÄ° GÄ°RÄ°N\n",
    "\n",
    "# Ã–rnek: Ollama\n",
    "# API_BASE = \"http://localhost:11434/v1\"\n",
    "# MODEL_NAME = \"llama3.1:8b\"\n",
    "\n",
    "# Ã–rnek: LM Studio\n",
    "# API_BASE = \"http://localhost:1234/v1\"\n",
    "# MODEL_NAME = \"llama-3.1-8b-instruct\"\n",
    "\n",
    "# Ã–rnek: Text Generation WebUI (oobabooga)\n",
    "# API_BASE = \"http://localhost:5000/v1\"\n",
    "# MODEL_NAME = \"your-model-name\"\n",
    "\n",
    "# Kendi API'niz (Ã¶rnek)\n",
    "API_BASE = \"http://localhost:8000/v1\"  # ğŸ‘ˆ Kendi endpoint'iniz\n",
    "MODEL_NAME = \"llama-3.1-8b-instruct\"   # ğŸ‘ˆ Modelinizin adÄ±\n",
    "API_KEY = \"sk-dummy\"  # BazÄ± API'ler key gerektirmez, \"dummy\" bÄ±rakabilirsiniz\n",
    "\n",
    "print(f\"âœ… API AyarlarÄ±:\")\n",
    "print(f\"   Base URL: {API_BASE}\")\n",
    "print(f\"   Model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ§ª API BaÄŸlantÄ± Testi\n",
    "\n",
    "API'nizin Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± test edelim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# OpenAI SDK ile kendi API'nize baÄŸlanma\n",
    "client = OpenAI(\n",
    "    base_url=API_BASE,\n",
    "    api_key=API_KEY\n",
    ")\n",
    "\n",
    "# Test mesajÄ±\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"Merhaba! KÄ±sa bir test mesajÄ±.\"}\n",
    "        ],\n",
    "        max_tokens=50\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… API baÄŸlantÄ±sÄ± baÅŸarÄ±lÄ±!\")\n",
    "    print(f\"\\nğŸ¤– Model yanÄ±tÄ±:\\n{response.choices[0].message.content}\")\n",
    "    \nexcept Exception as e:\n",
    "    print(f\"âŒ API baÄŸlantÄ± hatasÄ±: {e}\")\n",
    "    print(\"\\nğŸ’¡ Kontrol edin:\")\n",
    "    print(\"   1. Local LLM sunucunuz Ã§alÄ±ÅŸÄ±yor mu?\")\n",
    "    print(\"   2. API_BASE URL'i doÄŸru mu?\")\n",
    "    print(\"   3. MODEL_NAME doÄŸru mu?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ Ã‡alÄ±ÅŸma Dizinlerini OluÅŸtur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proje dizin yapÄ±sÄ±\n",
    "BASE_DIR = Path(\"./synthetic_data_demo\")\n",
    "INPUT_DIR = BASE_DIR / \"input\"           # Kaynak dosyalar buraya\n",
    "PARSED_DIR = BASE_DIR / \"parsed\"         # Parse edilmiÅŸ veriler\n",
    "GENERATED_DIR = BASE_DIR / \"generated\"   # OluÅŸturulan QA'lar\n",
    "CURATED_DIR = BASE_DIR / \"curated\"       # FiltrelenmiÅŸ veriler\n",
    "CONFIG_DIR = BASE_DIR / \"config\"         # KonfigÃ¼rasyon dosyalarÄ±\n",
    "\n",
    "# Dizinleri oluÅŸtur\n",
    "for dir_path in [INPUT_DIR, PARSED_DIR, GENERATED_DIR, CURATED_DIR, CONFIG_DIR]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"âœ… Dizin yapÄ±sÄ± oluÅŸturuldu:\")\n",
    "print(f\"   ğŸ“‚ {BASE_DIR}/\")\n",
    "print(f\"      â”œâ”€â”€ input/        (Kaynak dosyalarÄ±nÄ±zÄ± buraya koyun)\")\n",
    "print(f\"      â”œâ”€â”€ parsed/       (Parse edilmiÅŸ chunk'lar)\")\n",
    "print(f\"      â”œâ”€â”€ generated/    (Ãœretilen QA pair'leri)\")\n",
    "print(f\"      â”œâ”€â”€ curated/      (FiltrelenmiÅŸ sonuÃ§lar)\")\n",
    "print(f\"      â””â”€â”€ config/       (YapÄ±landÄ±rma dosyalarÄ±)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âš™ï¸ Config DosyasÄ± OluÅŸtur\n",
    "\n",
    "Synthetic Data Kit iÃ§in custom config dosyasÄ±:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom yapÄ±landÄ±rma\n",
    "config = {\n",
    "    \"llm\": {\n",
    "        \"provider\": \"openai\",  # OpenAI-uyumlu API kullanacaÄŸÄ±z\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_tokens\": 2048\n",
    "    },\n",
    "    \"openai\": {\n",
    "        \"api_base\": API_BASE,\n",
    "        \"api_key\": API_KEY\n",
    "    },\n",
    "    \"ingest\": {\n",
    "        \"chunk_size\": 2000,      # Text'i kaÃ§ karakterlik parÃ§alara bÃ¶lelim\n",
    "        \"chunk_overlap\": 200     # ParÃ§alar arasÄ±nda overlap\n",
    "    },\n",
    "    \"create\": {\n",
    "        \"num_examples_per_chunk\": 3,  # Her chunk'tan kaÃ§ QA Ã¼retilsin\n",
    "        \"format\": \"qa\"                 # qa, cot, summary\n",
    "    },\n",
    "    \"curate\": {\n",
    "        \"quality_threshold\": 7.0,  # 0-10 arasÄ± kalite skoru (7+ iyi)\n",
    "        \"enabled\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "# Config dosyasÄ±nÄ± kaydet\n",
    "config_file = CONFIG_DIR / \"my_config.yaml\"\n",
    "with open(config_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    yaml.dump(config, f, default_flow_style=False, allow_unicode=True)\n",
    "\n",
    "print(f\"âœ… Config dosyasÄ± oluÅŸturuldu: {config_file}\")\n",
    "print(\"\\nğŸ“„ Ä°Ã§erik:\")\n",
    "print(\"=\"*60)\n",
    "print(yaml.dump(config, default_flow_style=False, allow_unicode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3ï¸âƒ£ Ã–rnek Veri HazÄ±rlama\n",
    "\n",
    "DÄ±ÅŸarÄ±dan bir dosya yÃ¼kleyebilir veya Ã¶rnek bir text oluÅŸturabilirsiniz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ã–rnek metin oluÅŸtur (veya kendi dosyanÄ±zÄ± yÃ¼kleyin)\n",
    "sample_text = \"\"\"\n",
    "Yapay Zeka ve Large Language Models\n",
    "\n",
    "Yapay zeka (AI), makinelerin insan benzeri zeka gerektiren gÃ¶revleri yerine getirmesini \n",
    "saÄŸlayan bir teknolojidir. Son yÄ±llarda, Ã¶zellikle Large Language Models (LLM'ler) \n",
    "alanÄ±nda bÃ¼yÃ¼k geliÅŸmeler yaÅŸandÄ±.\n",
    "\n",
    "LLM'ler, milyarlarca parametre iÃ§eren ve Ã§ok bÃ¼yÃ¼k metin veri setleri Ã¼zerinde eÄŸitilen \n",
    "derin Ã¶ÄŸrenme modelleridir. GPT-4, Llama, Claude gibi modeller bu kategoriye girer.\n",
    "\n",
    "Transformer Mimarisi\n",
    "\n",
    "Modern LLM'lerin Ã§oÄŸu, 2017 yÄ±lÄ±nda \"Attention is All You Need\" makalesinde tanÄ±tÄ±lan \n",
    "Transformer mimarisini kullanÄ±r. Bu mimari, self-attention mekanizmasÄ± sayesinde \n",
    "cÃ¼mledeki kelimelerin birbirleriyle iliÅŸkilerini etkili bir ÅŸekilde Ã¶ÄŸrenebilir.\n",
    "\n",
    "Transformerlar, Ã¶nceki RNN ve LSTM tabanlÄ± modellere gÃ¶re birÃ§ok avantaj sunar:\n",
    "- Paralel iÅŸleme yeteneÄŸi\n",
    "- Uzun menzilli baÄŸÄ±mlÄ±lÄ±klarÄ± yakalama\n",
    "- Daha hÄ±zlÄ± eÄŸitim sÃ¼resi\n",
    "\n",
    "Fine-tuning ve Sentetik Veri\n",
    "\n",
    "Fine-tuning, Ã¶nceden eÄŸitilmiÅŸ bir modelin belirli bir gÃ¶reve veya domaine adapte \n",
    "edilmesi sÃ¼recidir. Bu iÅŸlem iÃ§in kaliteli eÄŸitim verisi gerekmektedir.\n",
    "\n",
    "Sentetik veri, gerÃ§ek veri yerine yapay olarak Ã¼retilen veridir. LLM'leri kullanarak \n",
    "yÃ¼ksek kaliteli QA (Soru-Cevap) Ã§iftleri, Chain-of-Thought Ã¶rnekleri veya Ã¶zetler \n",
    "oluÅŸturulabilir. Bu veriler fine-tuning iÃ§in kullanÄ±labilir.\n",
    "\n",
    "Meta'nÄ±n Synthetic Data Kit'i gibi araÃ§lar, bu sÃ¼reci otomatikleÅŸtirerek kaliteli \n",
    "sentetik eÄŸitim verisi oluÅŸturmayÄ± kolaylaÅŸtÄ±rÄ±r.\n",
    "\n",
    "RAG (Retrieval Augmented Generation)\n",
    "\n",
    "RAG, LLM'lerin gÃ¼ncel ve spesifik bilgilere eriÅŸmesini saÄŸlayan bir tekniktir. \n",
    "Model, sorguya yanÄ±t vermeden Ã¶nce bir bilgi tabanÄ±ndan ilgili dokÃ¼manlarÄ± alÄ±r \n",
    "ve bu bilgileri kullanarak yanÄ±t Ã¼retir.\n",
    "\n",
    "RAG sistemleri genellikle ÅŸu bileÅŸenlerden oluÅŸur:\n",
    "1. VektÃ¶r veritabanÄ± (embedding'ler iÃ§in)\n",
    "2. Retriever (ilgili dokÃ¼manlarÄ± bulma)\n",
    "3. Generator (LLM - yanÄ±t oluÅŸturma)\n",
    "\"\"\"\n",
    "\n",
    "# DosyayÄ± kaydet\n",
    "input_file = INPUT_DIR / \"ai_ml_tutorial.txt\"\n",
    "with open(input_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(sample_text)\n",
    "\n",
    "print(f\"âœ… Ã–rnek dosya oluÅŸturuldu: {input_file}\")\n",
    "print(f\"ğŸ“ Dosya boyutu: {len(sample_text)} karakter\")\n",
    "print(f\"\\nğŸ’¡ Ä°pucu: Kendi dosyanÄ±zÄ± kullanmak iÃ§in '{INPUT_DIR}/' dizinine kopyalayÄ±n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4ï¸âƒ£ Synthetic Data Pipeline\n",
    "\n",
    "### Pipeline AdÄ±mlarÄ±:\n",
    "\n",
    "1. **INGEST** â†’ Belgeyi parse et ve chunk'lara bÃ¶l\n",
    "2. **CREATE** â†’ Her chunk'tan QA pair'leri oluÅŸtur\n",
    "3. **CURATE** â†’ Kalite kontrolÃ¼ yap, kÃ¶tÃ¼leri filtrele\n",
    "4. **SAVE-AS** â†’ Ä°stediÄŸin formatta kaydet (Alpaca, ChatML, vb.)\n",
    "\n",
    "### ğŸ”¸ AdÄ±m 1: INGEST - Belgeyi Parse Etme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INGEST komutu\n",
    "!synthetic-data-kit ingest \\\n",
    "    {str(input_file)} \\\n",
    "    --config {str(config_file)}\n",
    "\n",
    "print(\"\\nâœ… Ingest tamamlandÄ±!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse edilmiÅŸ dosyayÄ± inceleyin\n",
    "# Synthetic Data Kit genellikle data/parsed/ dizinine kaydeder\n",
    "parsed_files = list(Path(\"data/parsed\").glob(\"**/*.txt\")) if Path(\"data/parsed\").exists() else []\n",
    "\n",
    "if parsed_files:\n",
    "    print(f\"ğŸ“„ Parse edilmiÅŸ dosya: {parsed_files[0]}\")\n",
    "    with open(parsed_files[0], \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "        print(f\"ğŸ“ Ä°Ã§erik uzunluÄŸu: {len(content)} karakter\")\n",
    "        print(f\"\\nğŸ“ Ä°lk 300 karakter:\\n{content[:300]}...\")\n",
    "else:\n",
    "    print(\"âš ï¸ Parse edilmiÅŸ dosya bulunamadÄ±. LÃ¼tfen ingest komutunu kontrol edin.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”¸ AdÄ±m 2: CREATE - QA Pair'leri OluÅŸturma\n",
    "\n",
    "**Ã–NEMLÄ°:** Bu adÄ±mda local LLM API'niz kullanÄ±lacak!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse edilmiÅŸ dosyayÄ± bul\n",
    "if parsed_files:\n",
    "    parsed_file = parsed_files[0]\n",
    "else:\n",
    "    # Manuel olarak belirt\n",
    "    parsed_file = Path(\"data/parsed/ai_ml_tutorial.txt\")\n",
    "\n",
    "# CREATE komutu - QA pair'leri oluÅŸtur\n",
    "!synthetic-data-kit create \\\n",
    "    {str(parsed_file)} \\\n",
    "    --type qa \\\n",
    "    --num-pairs 10 \\\n",
    "    --config {str(config_file)}\n",
    "\n",
    "print(\"\\nâœ… QA pair'leri oluÅŸturuldu!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OluÅŸturulan QA'larÄ± incele\n",
    "generated_files = list(Path(\"data/generated\").glob(\"**/*qa_pairs.json\")) if Path(\"data/generated\").exists() else []\n",
    "\n",
    "if generated_files:\n",
    "    generated_file = generated_files[0]\n",
    "    print(f\"ğŸ“„ OluÅŸturulan dosya: {generated_file}\")\n",
    "    \n",
    "    with open(generated_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        qa_data = json.load(f)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Toplam {len(qa_data)} QA pair oluÅŸturuldu\\n\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Ä°lk 3 QA'yÄ± gÃ¶ster\n",
    "    for i, item in enumerate(qa_data[:3], 1):\n",
    "        print(f\"\\n#{i}\")\n",
    "        print(f\"â“ Soru: {item.get('question', 'N/A')}\")\n",
    "        print(f\"âœ… Cevap: {item.get('answer', 'N/A')}\")\n",
    "        print(\"-\"*80)\n",
    "else:\n",
    "    print(\"âš ï¸ OluÅŸturulan dosya bulunamadÄ±.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”¸ AdÄ±m 3: CURATE - Kalite KontrolÃ¼\n",
    "\n",
    "LLM-as-a-judge kullanarak dÃ¼ÅŸÃ¼k kaliteli QA'larÄ± filtreleyelim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if generated_files:\n",
    "    # CURATE komutu\n",
    "    !synthetic-data-kit curate \\\n",
    "        {str(generated_file)} \\\n",
    "        --threshold 7.0 \\\n",
    "        --config {str(config_file)}\n",
    "    \n",
    "    print(\"\\nâœ… Curate tamamlandÄ±!\")\n",
    "else:\n",
    "    print(\"âš ï¸ Ã–nce CREATE adÄ±mÄ±nÄ± tamamlayÄ±n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curate edilmiÅŸ dosyayÄ± incele\n",
    "curated_files = list(Path(\"data/curated\").glob(\"**/*_cleaned.json\")) if Path(\"data/curated\").exists() else []\n",
    "\n",
    "if curated_files:\n",
    "    curated_file = curated_files[0]\n",
    "    print(f\"ğŸ“„ FiltrelenmiÅŸ dosya: {curated_file}\")\n",
    "    \n",
    "    with open(curated_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        curated_data = json.load(f)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Kalite kontrolÃ¼ sonrasÄ±: {len(curated_data)} QA kaldÄ±\")\n",
    "    print(f\"ğŸ—‘ï¸ Filtrelenen: {len(qa_data) - len(curated_data)} QA\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    for i, item in enumerate(curated_data[:2], 1):\n",
    "        print(f\"\\n#{i}\")\n",
    "        if 'score' in item:\n",
    "            print(f\"â­ Kalite Skoru: {item['score']}/10\")\n",
    "        print(f\"â“ Soru: {item.get('question', 'N/A')}\")\n",
    "        print(f\"âœ… Cevap: {item.get('answer', 'N/A')[:200]}...\")\n",
    "        print(\"-\"*80)\n",
    "else:\n",
    "    print(\"âš ï¸ FiltrelenmiÅŸ dosya bulunamadÄ±.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”¸ AdÄ±m 4: SAVE-AS - Format DÃ¶nÃ¼ÅŸtÃ¼rme\n",
    "\n",
    "Fine-tuning iÃ§in farklÄ± formatlara dÃ¶nÃ¼ÅŸtÃ¼relim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if curated_files:\n",
    "    output_alpaca = CURATED_DIR / \"final_alpaca.json\"\n",
    "    \n",
    "    # SAVE-AS: Alpaca formatÄ±\n",
    "    !synthetic-data-kit save-as \\\n",
    "        {str(curated_file)} \\\n",
    "        --format alpaca \\\n",
    "        --output {str(output_alpaca)}\n",
    "    \n",
    "    print(f\"\\nâœ… Alpaca formatÄ±: {output_alpaca}\")\n",
    "else:\n",
    "    print(\"âš ï¸ Ã–nce CURATE adÄ±mÄ±nÄ± tamamlayÄ±n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final dataset'i incele\n",
    "if output_alpaca.exists():\n",
    "    with open(output_alpaca, \"r\", encoding=\"utf-8\") as f:\n",
    "        final_data = json.load(f)\n",
    "    \n",
    "    print(f\"ğŸ“¦ Final Dataset (Alpaca Format)\")\n",
    "    print(f\"ğŸ“Š Toplam Ã¶rnek: {len(final_data)}\\n\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Ä°lk Ã¶rneÄŸi gÃ¶ster\n",
    "    print(\"\\nğŸ“ Ã–rnek 1 (Alpaca format):\")\n",
    "    print(json.dumps(final_data[0], indent=2, ensure_ascii=False))\n",
    "else:\n",
    "    print(\"âš ï¸ Final dataset bulunamadÄ±.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5ï¸âƒ£ Programmatik KullanÄ±m (Python API)\n",
    "\n",
    "CLI yerine doÄŸrudan Python kodunda kullanmak isterseniz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI SDK ile kendi QA generator yazma\n",
    "from openai import OpenAI\n",
    "\n",
    "def generate_qa_from_text(text: str, num_pairs: int = 5) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Text'ten QA pair'leri oluÅŸtur (kendi local LLM API ile)\n",
    "    \"\"\"\n",
    "    client = OpenAI(\n",
    "        base_url=API_BASE,\n",
    "        api_key=API_KEY\n",
    "    )\n",
    "    \n",
    "    prompt = f\"\"\"AÅŸaÄŸÄ±daki metinden {num_pairs} adet yÃ¼ksek kaliteli TÃ¼rkÃ§e soru-cevap Ã§ifti oluÅŸtur.\n",
    "Sorular metinde geÃ§en Ã¶nemli kavramlarÄ± test etmeli, cevaplar detaylÄ± ve aÃ§Ä±klayÄ±cÄ± olmalÄ±.\n",
    "\n",
    "Metin:\n",
    "{text}\n",
    "\n",
    "JSON formatÄ±nda dÃ¶ndÃ¼r:\n",
    "{{\n",
    "    \"qa_pairs\": [\n",
    "        {{\n",
    "            \"question\": \"...\",\n",
    "            \"answer\": \"...\"\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Sen bir eÄŸitim verisi oluÅŸturma uzmanÄ±sÄ±n. YÃ¼ksek kaliteli QA Ã§iftleri oluÅŸturursun.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=2048\n",
    "    )\n",
    "    \n",
    "    result_text = response.choices[0].message.content\n",
    "    \n",
    "    # JSON parse et\n",
    "    try:\n",
    "        # Bazen model ``` ile wrap edebilir, temizle\n",
    "        if \"```json\" in result_text:\n",
    "            result_text = result_text.split(\"```json\")[1].split(\"```\")[0]\n",
    "        elif \"```\" in result_text:\n",
    "            result_text = result_text.split(\"```\")[1].split(\"```\")[0]\n",
    "        \n",
    "        data = json.loads(result_text)\n",
    "        return data.get(\"qa_pairs\", [])\n",
    "    \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"âš ï¸ JSON parse hatasÄ±: {e}\")\n",
    "        print(f\"Raw output:\\n{result_text}\")\n",
    "        return []\n",
    "\n",
    "print(\"âœ… QA generator fonksiyonu hazÄ±r!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonksiyonu test et\n",
    "print(\"ğŸ”„ Local LLM ile QA pair'leri oluÅŸturuluyor...\\n\")\n",
    "\n",
    "qa_pairs = generate_qa_from_text(sample_text, num_pairs=3)\n",
    "\n",
    "print(f\"âœ… {len(qa_pairs)} QA pair oluÅŸturuldu!\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, qa in enumerate(qa_pairs, 1):\n",
    "    print(f\"\\n#{i}\")\n",
    "    print(f\"â“ Soru: {qa.get('question', 'N/A')}\")\n",
    "    print(f\"âœ… Cevap: {qa.get('answer', 'N/A')}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”§ Batch Ä°ÅŸleme - Birden Fazla Dosya\n",
    "\n",
    "Bir klasÃ¶rdeki tÃ¼m dosyalarÄ± toplu olarak iÅŸleyelim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BirkaÃ§ Ã¶rnek dosya daha ekleyelim\n",
    "sample_texts = {\n",
    "    \"python_basics.txt\": \"\"\"\n",
    "Python Programlama Dili\n",
    "\n",
    "Python, Guido van Rossum tarafÄ±ndan 1991 yÄ±lÄ±nda geliÅŸtirilmiÅŸ, yÃ¼ksek seviyeli bir programlama dilidir.\n",
    "Basit sÃ¶zdizimi ve geniÅŸ kÃ¼tÃ¼phane desteÄŸi sayesinde veri bilimi, web geliÅŸtirme ve yapay zeka \n",
    "projelerinde yaygÄ±n olarak kullanÄ±lÄ±r.\n",
    "\n",
    "Python'da veri tipleri: int, float, str, list, dict, tuple gibi temel tipler vardÄ±r.\n",
    "List comprehension, generator expression gibi Ã¶zellikler kodu daha okunabilir yapar.\n",
    "\"\"\",\n",
    "    \"rag_systems.txt\": \"\"\"\n",
    "RAG (Retrieval Augmented Generation) Sistemleri\n",
    "\n",
    "RAG, LLM'lerin bilgi tabanlarÄ±na eriÅŸerek daha doÄŸru ve gÃ¼ncel yanÄ±tlar vermesini saÄŸlar.\n",
    "Sistem, kullanÄ±cÄ± sorgusunu embedding'e dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r, vektÃ¶r veritabanÄ±nda benzer dokÃ¼manlarÄ± \n",
    "arar ve bunlarÄ± LLM'e context olarak verir.\n",
    "\n",
    "PopÃ¼ler vektÃ¶r veritabanlarÄ±: Pinecone, Weaviate, ChromaDB, FAISS.\n",
    "Embedding modelleri: OpenAI embeddings, sentence-transformers.\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "# DosyalarÄ± kaydet\n",
    "for filename, content in sample_texts.items():\n",
    "    filepath = INPUT_DIR / filename\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "    print(f\"âœ… OluÅŸturuldu: {filename}\")\n",
    "\n",
    "print(f\"\\nğŸ“‚ Toplam dosya sayÄ±sÄ±: {len(list(INPUT_DIR.glob('*.txt')))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TÃ¼m dosyalarÄ± toplu iÅŸle\n",
    "def process_all_files(input_dir: Path, num_qa_per_file: int = 3):\n",
    "    \"\"\"\n",
    "    Bir dizindeki tÃ¼m text dosyalarÄ±nÄ± iÅŸleyip QA pair'leri oluÅŸtur\n",
    "    \"\"\"\n",
    "    all_qa_pairs = []\n",
    "    \n",
    "    for text_file in input_dir.glob(\"*.txt\"):\n",
    "        print(f\"\\nğŸ”„ Ä°ÅŸleniyor: {text_file.name}\")\n",
    "        \n",
    "        # DosyayÄ± oku\n",
    "        with open(text_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # QA pair'leri oluÅŸtur\n",
    "        qa_pairs = generate_qa_from_text(content, num_pairs=num_qa_per_file)\n",
    "        \n",
    "        # Kaynak dosyayÄ± ekle\n",
    "        for qa in qa_pairs:\n",
    "            qa['source_file'] = text_file.name\n",
    "        \n",
    "        all_qa_pairs.extend(qa_pairs)\n",
    "        print(f\"   âœ… {len(qa_pairs)} QA oluÅŸturuldu\")\n",
    "    \n",
    "    return all_qa_pairs\n",
    "\n",
    "# TÃ¼m dosyalarÄ± iÅŸle\n",
    "print(\"ğŸš€ Batch iÅŸleme baÅŸlÄ±yor...\\n\")\n",
    "all_qa = process_all_files(INPUT_DIR, num_qa_per_file=2)\n",
    "\n",
    "print(f\"\\nâœ… Toplam {len(all_qa)} QA pair oluÅŸturuldu!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TÃ¼m QA'larÄ± kaydet\n",
    "batch_output = GENERATED_DIR / \"batch_qa_pairs.json\"\n",
    "with open(batch_output, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_qa, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"ğŸ’¾ Batch QA'lar kaydedildi: {batch_output}\")\n",
    "\n",
    "# Ã–zet gÃ¶ster\n",
    "print(f\"\\nğŸ“Š Ã–zet:\")\n",
    "for file in INPUT_DIR.glob(\"*.txt\"):\n",
    "    count = sum(1 for qa in all_qa if qa.get('source_file') == file.name)\n",
    "    print(f\"   {file.name}: {count} QA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6ï¸âƒ£ FarklÄ± Format Ã–rnekleri\n",
    "\n",
    "Alpaca dÄ±ÅŸÄ±nda baÅŸka formatlar da kullanabilirsiniz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpaca format\n",
    "def convert_to_alpaca(qa_pairs: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"QA'larÄ± Alpaca formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼r\"\"\"\n",
    "    alpaca_data = []\n",
    "    for qa in qa_pairs:\n",
    "        alpaca_data.append({\n",
    "            \"instruction\": qa.get('question', ''),\n",
    "            \"input\": \"\",\n",
    "            \"output\": qa.get('answer', '')\n",
    "        })\n",
    "    return alpaca_data\n",
    "\n",
    "# ShareGPT format\n",
    "def convert_to_sharegpt(qa_pairs: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"QA'larÄ± ShareGPT formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼r\"\"\"\n",
    "    sharegpt_data = []\n",
    "    for qa in qa_pairs:\n",
    "        sharegpt_data.append({\n",
    "            \"conversations\": [\n",
    "                {\"from\": \"human\", \"value\": qa.get('question', '')},\n",
    "                {\"from\": \"gpt\", \"value\": qa.get('answer', '')}\n",
    "            ]\n",
    "        })\n",
    "    return sharegpt_data\n",
    "\n",
    "# ChatML format\n",
    "def convert_to_chatml(qa_pairs: List[Dict]) -> List[str]:\n",
    "    \"\"\"QA'larÄ± ChatML formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼r\"\"\"\n",
    "    chatml_data = []\n",
    "    for qa in qa_pairs:\n",
    "        chatml = f\"\"\"<|im_start|>user\n",
    "{qa.get('question', '')}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{qa.get('answer', '')}<|im_end|>\"\"\"\n",
    "        chatml_data.append(chatml)\n",
    "    return chatml_data\n",
    "\n",
    "# DÃ¶nÃ¼ÅŸÃ¼mleri yap\n",
    "alpaca_format = convert_to_alpaca(all_qa)\n",
    "sharegpt_format = convert_to_sharegpt(all_qa)\n",
    "chatml_format = convert_to_chatml(all_qa)\n",
    "\n",
    "# Kaydet\n",
    "with open(CURATED_DIR / \"alpaca_format.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(alpaca_format, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(CURATED_DIR / \"sharegpt_format.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(sharegpt_format, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(CURATED_DIR / \"chatml_format.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\\n\".join(chatml_format))\n",
    "\n",
    "print(\"âœ… TÃ¼m formatlar kaydedildi:\")\n",
    "print(f\"   - Alpaca: {CURATED_DIR / 'alpaca_format.json'}\")\n",
    "print(f\"   - ShareGPT: {CURATED_DIR / 'sharegpt_format.json'}\")\n",
    "print(f\"   - ChatML: {CURATED_DIR / 'chatml_format.txt'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format Ã¶rneklerini gÃ¶ster\n",
    "print(\"ğŸ“¦ Format Ã–rnekleri:\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1ï¸âƒ£ Alpaca Format:\")\n",
    "print(json.dumps(alpaca_format[0], indent=2, ensure_ascii=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\n2ï¸âƒ£ ShareGPT Format:\")\n",
    "print(json.dumps(sharegpt_format[0], indent=2, ensure_ascii=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\n3ï¸âƒ£ ChatML Format:\")\n",
    "print(chatml_format[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7ï¸âƒ£ Ä°puÃ§larÄ± ve Best Practices\n",
    "\n",
    "### ğŸ¯ Parametre Optimizasyonu\n",
    "\n",
    "- **chunk_size**: 1500-2500 karakter arasÄ± optimal\n",
    "- **temperature**: 0.7-0.9 arasÄ± Ã§eÅŸitlilik iÃ§in iyi\n",
    "- **quality_threshold**: 7.0-8.0 arasÄ± dengeli\n",
    "- **num_pairs**: Chunk baÅŸÄ±na 3-5 QA optimal\n",
    "\n",
    "### ğŸš€ Performans\n",
    "\n",
    "- BÃ¼yÃ¼k dosyalar iÃ§in Ã¶nce kÃ¼Ã§Ã¼k batch test edin\n",
    "- GPU kullanÄ±n (eÄŸer local API GPU destekliyorsa)\n",
    "- Paralel iÅŸleme iÃ§in birden fazla worker kullanÄ±n\n",
    "\n",
    "### âœ… Kalite KontrolÃ¼\n",
    "\n",
    "- Her adÄ±mdan sonra manuel olarak birkaÃ§ Ã¶rneÄŸi inceleyin\n",
    "- Quality threshold'u kademeli olarak ayarlayÄ±n\n",
    "- FarklÄ± temperature deÄŸerlerini test edin\n",
    "\n",
    "### ğŸ“ Dosya YÃ¶netimi\n",
    "\n",
    "- Kaynak dosyalarÄ±nÄ±zÄ± versiyonlayÄ±n\n",
    "- Her run iÃ§in timestamp ekleyin\n",
    "- BÃ¼yÃ¼k dataset'leri JSONL formatÄ±nda saklayÄ±n (bellek tasarrufu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8ï¸âƒ£ Troubleshooting\n",
    "\n",
    "### âŒ \"Connection Error\" HatasÄ±\n",
    "\n",
    "**Ã‡Ã¶zÃ¼m:**\n",
    "- Local LLM API'nizin Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olun\n",
    "- `API_BASE` URL'ini kontrol edin\n",
    "- `curl {API_BASE}/models` ile test edin\n",
    "\n",
    "### âŒ \"Model Not Found\" HatasÄ±\n",
    "\n",
    "**Ã‡Ã¶zÃ¼m:**\n",
    "- `MODEL_NAME` deÄŸiÅŸkenini kontrol edin\n",
    "- API'nizde hangi modeller var: `curl {API_BASE}/models`\n",
    "\n",
    "### âŒ JSON Parse HatasÄ±\n",
    "\n",
    "**Ã‡Ã¶zÃ¼m:**\n",
    "- LLM'nizin JSON Ã¼retimi zayÄ±f olabilir\n",
    "- Prompt'u daha net yapÄ±n\n",
    "- Temperature'Ã¼ azaltÄ±n (0.5-0.6)\n",
    "- Daha bÃ¼yÃ¼k/iyi model kullanÄ±n\n",
    "\n",
    "### âŒ DÃ¼ÅŸÃ¼k Kaliteli QA'lar\n",
    "\n",
    "**Ã‡Ã¶zÃ¼m:**\n",
    "- Chunk size'Ä± ayarlayÄ±n (Ã§ok kÃ¼Ã§Ã¼k chunk â†’ context eksikliÄŸi)\n",
    "- System prompt'u iyileÅŸtirin\n",
    "- Quality threshold'u yÃ¼kseltin\n",
    "- Daha gÃ¼Ã§lÃ¼ model kullanÄ±n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9ï¸âƒ£ Ã–zet\n",
    "\n",
    "### âœ… Bu Notebook'ta Ã–ÄŸrendikleriniz:\n",
    "\n",
    "1. âœ… Synthetic Data Kit kurulumu\n",
    "2. âœ… Kendi local LLM API'nizi yapÄ±landÄ±rma (vLLM olmadan!)\n",
    "3. âœ… 4 adÄ±mlÄ± pipeline: Ingest â†’ Create â†’ Curate â†’ Save\n",
    "4. âœ… Programmatik kullanÄ±m (Python API)\n",
    "5. âœ… Batch iÅŸleme (birden fazla dosya)\n",
    "6. âœ… FarklÄ± format dÃ¶nÃ¼ÅŸÃ¼mleri (Alpaca, ShareGPT, ChatML)\n",
    "7. âœ… Best practices ve troubleshooting\n",
    "\n",
    "### ğŸ¯ Sonraki AdÄ±mlar:\n",
    "\n",
    "- Kendi dokÃ¼manlarÄ±nÄ±zÄ± `input/` dizinine koyun\n",
    "- FarklÄ± QA tiplerini deneyin (CoT, Summary)\n",
    "- Fine-tuning iÃ§in oluÅŸturduÄŸunuz veriyi kullanÄ±n\n",
    "- Kalite kontrolÃ¼ yapÄ±n ve iyileÅŸtirin\n",
    "\n",
    "### ğŸ“š Kaynaklar:\n",
    "\n",
    "- [Synthetic Data Kit GitHub](https://github.com/meta-llama/synthetic-data-kit)\n",
    "- [Meta Llama Models](https://github.com/meta-llama)\n",
    "- [OpenAI API Docs](https://platform.openai.com/docs/api-reference)\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ BaÅŸarÄ±lar! ArtÄ±k kendi local modelinizle sentetik veri Ã¼retebilirsiniz!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
