{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TÃ¼rkÃ§e BERT Embedding Fine-Tuning - GeliÅŸmiÅŸ Versiyon\n",
    "\n",
    "Bu notebook, **tÃ¼m soru-cevap verilerinizi** kullanarak `dbmdz/bert-base-turkish-cased` modelini fine-tune eder.\n",
    "\n",
    "## âœ¨ Ã–zellikler:\n",
    "- BÃ¼yÃ¼k veri setleri iÃ§in optimize edilmiÅŸ\n",
    "- Negatif Ã¶rnek oluÅŸturma (data augmentation)\n",
    "- Hard negative mining\n",
    "- Checkpoint kaydetme\n",
    "- Memory optimization\n",
    "- Progress tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Kurulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers transformers datasets accelerate torch scikit-learn pandas openpyxl -q -U\n",
    "\n",
    "print(\"âœ… TÃ¼m kÃ¼tÃ¼phaneler yÃ¼klendi!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š KÃ¼tÃ¼phaneleri Ä°Ã§e Aktarma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, evaluation\n",
    "from sentence_transformers.util import cos_sim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# GPU kontrolÃ¼\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ğŸ–¥ï¸  KullanÄ±lan cihaz: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "# Seed ayarla (reproducibility iÃ§in)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š 1. TÃœM VERÄ°LERÄ°NÄ°ZÄ° YÃœKLEYIN\n",
    "\n",
    "### Desteklenen formatlar:\n",
    "- **CSV**: `soru` ve `cevap` sÃ¼tunlarÄ±\n",
    "- **JSON**: `[{\"soru\": \"...\", \"cevap\": \"...\"}, ...]`\n",
    "- **Excel**: `soru` ve `cevap` sÃ¼tunlarÄ±\n",
    "- **Birden fazla dosya**: Otomatik birleÅŸtirme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_file(file_path):\n",
    "    \"\"\"Tek bir dosyayÄ± yÃ¼kler\"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        raise FileNotFoundError(f\"Dosya bulunamadÄ±: {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        if file_path.suffix == '.csv':\n",
    "            df = pd.read_csv(file_path, encoding='utf-8')\n",
    "        elif file_path.suffix == '.json':\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            df = pd.DataFrame(data)\n",
    "        elif file_path.suffix in ['.xlsx', '.xls']:\n",
    "            df = pd.read_excel(file_path)\n",
    "        else:\n",
    "            raise ValueError(f\"Desteklenmeyen format: {file_path.suffix}\")\n",
    "        \n",
    "        print(f\"  âœ“ {file_path.name}: {len(df)} satÄ±r yÃ¼klendi\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— Hata ({file_path.name}): {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def load_all_data(file_paths):\n",
    "    \"\"\"Birden fazla dosyayÄ± yÃ¼kler ve birleÅŸtirir\"\"\"\n",
    "    if isinstance(file_paths, str):\n",
    "        file_paths = [file_paths]\n",
    "    \n",
    "    all_dfs = []\n",
    "    print(\"ğŸ“ Dosyalar yÃ¼kleniyor...\")\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        df = load_data_file(file_path)\n",
    "        if df is not None:\n",
    "            all_dfs.append(df)\n",
    "    \n",
    "    if not all_dfs:\n",
    "        raise ValueError(\"HiÃ§bir dosya yÃ¼klenemedi!\")\n",
    "    \n",
    "    # TÃ¼m dataframe'leri birleÅŸtir\n",
    "    combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    \n",
    "    # SÃ¼tun kontrolÃ¼\n",
    "    if 'soru' not in combined_df.columns or 'cevap' not in combined_df.columns:\n",
    "        print(f\"âŒ Mevcut sÃ¼tunlar: {combined_df.columns.tolist()}\")\n",
    "        raise ValueError(\"Veri setinde 'soru' ve 'cevap' sÃ¼tunlarÄ± olmalÄ±!\")\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# ============================================\n",
    "# BURAYA KENDÄ° DOSYA YOLLARINIZI YAZIN\n",
    "# ============================================\n",
    "\n",
    "# Tek dosya iÃ§in:\n",
    "# DATA_FILES = 'path/to/your/qa_data.csv'\n",
    "\n",
    "# Birden fazla dosya iÃ§in:\n",
    "# DATA_FILES = [\n",
    "#     'path/to/qa_data1.csv',\n",
    "#     'path/to/qa_data2.json',\n",
    "#     'path/to/qa_data3.xlsx'\n",
    "# ]\n",
    "\n",
    "# Ã–rnek veri oluÅŸturma (test iÃ§in - gerÃ§ek verileriniz varsa silin)\n",
    "example_data = {\n",
    "    'soru': [\n",
    "        'Python nedir?',\n",
    "        'Makine Ã¶ÄŸrenmesi ne demektir?',\n",
    "        'Deep learning nedir?',\n",
    "        'BERT modeli nasÄ±l Ã§alÄ±ÅŸÄ±r?',\n",
    "        'Transformer mimarisi nedir?',\n",
    "        'Natural language processing ne iÅŸe yarar?',\n",
    "        'Yapay sinir aÄŸlarÄ± nasÄ±l Ã¶ÄŸrenir?',\n",
    "        'Supervised learning nedir?',\n",
    "        'Unsupervised learning ne demek?',\n",
    "        'Reinforcement learning nasÄ±l Ã§alÄ±ÅŸÄ±r?'\n",
    "    ] * 10,  # 100 Ã¶rnek iÃ§in\n",
    "    'cevap': [\n",
    "        'Python, yÃ¼ksek seviyeli, yorumlamalÄ± bir programlama dilidir.',\n",
    "        'Makine Ã¶ÄŸrenmesi, bilgisayarlarÄ±n verilerden Ã¶ÄŸrenmesini saÄŸlayan yapay zeka dalÄ±dÄ±r.',\n",
    "        'Deep learning, Ã§ok katmanlÄ± yapay sinir aÄŸlarÄ± kullanarak Ã¶ÄŸrenen bir makine Ã¶ÄŸrenmesi yÃ¶ntemidir.',\n",
    "        'BERT, Ã§ift yÃ¶nlÃ¼ transformer mimarisini kullanan bir doÄŸal dil iÅŸleme modelidir.',\n",
    "        'Transformer, dikkat mekanizmasÄ± kullanan modern bir sinir aÄŸÄ± mimarisidir.',\n",
    "        'NLP, bilgisayarlarÄ±n insan dilini anlamasÄ±nÄ± ve iÅŸlemesini saÄŸlar.',\n",
    "        'Yapay sinir aÄŸlarÄ±, geri yayÄ±lÄ±m algoritmasÄ± ile aÄŸÄ±rlÄ±klarÄ±nÄ± gÃ¼ncelleyerek Ã¶ÄŸrenir.',\n",
    "        'Supervised learning, etiketli verilerle model eÄŸitme yÃ¶ntemidir.',\n",
    "        'Unsupervised learning, etiketsiz verilerden Ã¶rÃ¼ntÃ¼ler keÅŸfetme yÃ¶ntemidir.',\n",
    "        'Reinforcement learning, Ã¶dÃ¼l ve ceza sistemiyle Ã¶ÄŸrenen bir makine Ã¶ÄŸrenmesi tÃ¼rÃ¼dÃ¼r.'\n",
    "    ] * 10\n",
    "}\n",
    "\n",
    "pd.DataFrame(example_data).to_csv('example_qa_data.csv', index=False, encoding='utf-8')\n",
    "DATA_FILES = 'example_qa_data.csv'\n",
    "\n",
    "# Verileri yÃ¼kle\n",
    "try:\n",
    "    df = load_all_data(DATA_FILES)\n",
    "    print(f\"\\nâœ… Toplam {len(df):,} soru-cevap Ã§ifti yÃ¼klendi\")\n",
    "    print(f\"\\nğŸ“Š Ä°lk 3 Ã¶rnek:\")\n",
    "    print(df.head(3))\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Hata: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§¹ 2. Veri Temizleme ve Ã–n Ä°ÅŸleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df, verbose=True):\n",
    "    \"\"\"Verileri temizler ve hazÄ±rlar\"\"\"\n",
    "    initial_count = len(df)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"ğŸ§¹ Veri temizleme baÅŸlÄ±yor...\")\n",
    "        print(f\"  BaÅŸlangÄ±Ã§: {initial_count:,} Ã¶rnek\")\n",
    "    \n",
    "    # 1. BoÅŸ deÄŸerleri temizle\n",
    "    df = df.dropna(subset=['soru', 'cevap'])\n",
    "    if verbose:\n",
    "        print(f\"  BoÅŸ deÄŸerler silindi: {initial_count - len(df):,} Ã¶rnek\")\n",
    "    \n",
    "    # 2. Whitespace temizle\n",
    "    df['soru'] = df['soru'].astype(str).str.strip()\n",
    "    df['cevap'] = df['cevap'].astype(str).str.strip()\n",
    "    \n",
    "    # 3. Ã‡ok kÄ±sa Ã¶rnekleri filtrele\n",
    "    min_soru_len = 5\n",
    "    min_cevap_len = 10\n",
    "    df = df[\n",
    "        (df['soru'].str.len() >= min_soru_len) & \n",
    "        (df['cevap'].str.len() >= min_cevap_len)\n",
    "    ]\n",
    "    \n",
    "    # 4. Duplicate'leri kaldÄ±r\n",
    "    before_dedup = len(df)\n",
    "    df = df.drop_duplicates(subset=['soru', 'cevap'], keep='first')\n",
    "    if verbose:\n",
    "        print(f\"  Duplicate silindi: {before_dedup - len(df):,} Ã¶rnek\")\n",
    "    \n",
    "    # 5. Index'i sÄ±fÄ±rla\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  âœ… Temizleme tamamlandÄ±: {len(df):,} Ã¶rnek kaldÄ±\")\n",
    "        print(f\"  ğŸ“‰ Toplam kayÄ±p: {initial_count - len(df):,} Ã¶rnek ({(initial_count - len(df))/initial_count*100:.1f}%)\")\n",
    "        \n",
    "        # Ä°statistikler\n",
    "        print(f\"\\nğŸ“ˆ Veri Ä°statistikleri:\")\n",
    "        print(f\"  Ortalama soru uzunluÄŸu: {df['soru'].str.len().mean():.1f} karakter\")\n",
    "        print(f\"  Ortalama cevap uzunluÄŸu: {df['cevap'].str.len().mean():.1f} karakter\")\n",
    "        print(f\"  En uzun soru: {df['soru'].str.len().max()} karakter\")\n",
    "        print(f\"  En uzun cevap: {df['cevap'].str.len().max()} karakter\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Verileri temizle\n",
    "df = clean_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš¡ 3. Negatif Ã–rnekler OluÅŸturma (Data Augmentation)\n",
    "\n",
    "Model performansÄ±nÄ± artÄ±rmak iÃ§in **hard negative mining** yapÄ±yoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_negative_examples(df, num_negatives_per_positive=2):\n",
    "    \"\"\"\n",
    "    Her pozitif Ã¶rnek iÃ§in negatif Ã¶rnekler oluÅŸturur.\n",
    "    Negatif Ã¶rnekler = ilgisiz cevaplar\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ”„ Negatif Ã¶rnekler oluÅŸturuluyor...\")\n",
    "    print(f\"  Her pozitif Ã¶rnek iÃ§in {num_negatives_per_positive} negatif Ã¶rnek\")\n",
    "    \n",
    "    examples = []\n",
    "    all_answers = df['cevap'].tolist()\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Creating examples\"):\n",
    "        # Pozitif Ã¶rnek\n",
    "        examples.append({\n",
    "            'soru': row['soru'],\n",
    "            'cevap': row['cevap'],\n",
    "            'label': 1.0  # Pozitif\n",
    "        })\n",
    "        \n",
    "        # Negatif Ã¶rnekler (rastgele diÄŸer cevaplar)\n",
    "        negative_indices = random.sample(\n",
    "            [i for i in range(len(all_answers)) if i != idx],\n",
    "            min(num_negatives_per_positive, len(all_answers) - 1)\n",
    "        )\n",
    "        \n",
    "        for neg_idx in negative_indices:\n",
    "            examples.append({\n",
    "                'soru': row['soru'],\n",
    "                'cevap': all_answers[neg_idx],\n",
    "                'label': 0.0  # Negatif\n",
    "            })\n",
    "    \n",
    "    examples_df = pd.DataFrame(examples)\n",
    "    \n",
    "    print(f\"  âœ… Toplam {len(examples_df):,} Ã¶rnek oluÅŸturuldu\")\n",
    "    print(f\"     - Pozitif: {(examples_df['label']==1.0).sum():,}\")\n",
    "    print(f\"     - Negatif: {(examples_df['label']==0.0).sum():,}\")\n",
    "    \n",
    "    return examples_df\n",
    "\n",
    "# Negatif Ã¶rnekler ekle (opsiyonel - performansÄ± artÄ±rÄ±r)\n",
    "USE_NEGATIVE_EXAMPLES = True  # True/False olarak ayarlayÄ±n\n",
    "\n",
    "if USE_NEGATIVE_EXAMPLES:\n",
    "    augmented_df = create_negative_examples(df, num_negatives_per_positive=1)\n",
    "else:\n",
    "    # Sadece pozitif Ã¶rneklerle devam et\n",
    "    augmented_df = df.copy()\n",
    "    augmented_df['label'] = 1.0\n",
    "    print(f\"âœ… Sadece pozitif Ã¶rnekler kullanÄ±lacak: {len(augmented_df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š 4. Train-Validation-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split ratios\n",
    "TRAIN_RATIO = 0.80\n",
    "VAL_RATIO = 0.10\n",
    "TEST_RATIO = 0.10\n",
    "\n",
    "# Ä°lk olarak train ve temp split (train + val&test)\n",
    "train_df, temp_df = train_test_split(\n",
    "    augmented_df, \n",
    "    test_size=(1 - TRAIN_RATIO), \n",
    "    random_state=42,\n",
    "    stratify=augmented_df['label'] if USE_NEGATIVE_EXAMPLES else None\n",
    ")\n",
    "\n",
    "# Val ve test split\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, \n",
    "    test_size=TEST_RATIO/(VAL_RATIO + TEST_RATIO), \n",
    "    random_state=42,\n",
    "    stratify=temp_df['label'] if USE_NEGATIVE_EXAMPLES else None\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“Š Veri Seti DaÄŸÄ±lÄ±mÄ±:\")\n",
    "print(f\"  Training:   {len(train_df):,} Ã¶rnek ({len(train_df)/len(augmented_df)*100:.1f}%)\")\n",
    "print(f\"  Validation: {len(val_df):,} Ã¶rnek ({len(val_df)/len(augmented_df)*100:.1f}%)\")\n",
    "print(f\"  Test:       {len(test_df):,} Ã¶rnek ({len(test_df)/len(augmented_df)*100:.1f}%)\")\n",
    "print(f\"  TOPLAM:     {len(augmented_df):,} Ã¶rnek\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤– 5. Model YÃ¼kleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'dbmdz/bert-base-turkish-cased'\n",
    "\n",
    "print(f\"ğŸ¤– Model yÃ¼kleniyor: {MODEL_NAME}\")\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "# Model bilgileri\n",
    "print(f\"\\nâœ… Model baÅŸarÄ±yla yÃ¼klendi!\")\n",
    "print(f\"  Embedding boyutu: {model.get_sentence_embedding_dimension()}\")\n",
    "print(f\"  Max token uzunluÄŸu: {model.max_seq_length}\")\n",
    "print(f\"  Model device: {model.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ 6. Training Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_examples(df):\n",
    "    \"\"\"DataFrame'i InputExample listesine Ã§evirir\"\"\"\n",
    "    examples = []\n",
    "    for _, row in df.iterrows():\n",
    "        examples.append(\n",
    "            InputExample(texts=[row['soru'], row['cevap']], label=float(row['label']))\n",
    "        )\n",
    "    return examples\n",
    "\n",
    "print(\"ğŸ”„ Training Ã¶rnekleri hazÄ±rlanÄ±yor...\")\n",
    "train_examples = create_input_examples(train_df)\n",
    "print(f\"  âœ… {len(train_examples):,} training Ã¶rneÄŸi hazÄ±r\")\n",
    "\n",
    "# Batch size (GPU belleÄŸinize gÃ¶re ayarlayÄ±n)\n",
    "BATCH_SIZE = 16  # 8, 16, 32, 64 deneyebilirsiniz\n",
    "\n",
    "# DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "    train_examples, \n",
    "    shuffle=True, \n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Batch sayÄ±sÄ±: {len(train_dataloader):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’ª 7. Loss Fonksiyonu ve Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss fonksiyonu seÃ§imi\n",
    "# BÃ¼yÃ¼k veri setleri iÃ§in MultipleNegativesRankingLoss Ã¶nerilir\n",
    "if USE_NEGATIVE_EXAMPLES:\n",
    "    # Negatif Ã¶rnekler varsa CosineSimilarityLoss kullan\n",
    "    train_loss = losses.CosineSimilarityLoss(model)\n",
    "    loss_name = \"CosineSimilarityLoss\"\n",
    "else:\n",
    "    # Sadece pozitif Ã¶rnekler varsa MultipleNegativesRankingLoss kullan\n",
    "    train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "    loss_name = \"MultipleNegativesRankingLoss\"\n",
    "\n",
    "print(f\"ğŸ“‰ Loss fonksiyonu: {loss_name}\")\n",
    "\n",
    "# Validation evaluator\n",
    "val_questions = val_df['soru'].tolist()\n",
    "val_answers = val_df['cevap'].tolist()\n",
    "val_scores = val_df['label'].tolist()\n",
    "\n",
    "evaluator = evaluation.EmbeddingSimilarityEvaluator(\n",
    "    val_questions,\n",
    "    val_answers,\n",
    "    val_scores,\n",
    "    name='qa-validation',\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "print(f\"âœ… Evaluator hazÄ±r ({len(val_questions):,} validation Ã¶rneÄŸi)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ 8. Fine-Tuning\n",
    "\n",
    "### âš™ï¸ Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parametreleri\n",
    "NUM_EPOCHS = 3  # 2-5 epoch genellikle yeterli\n",
    "WARMUP_STEPS = int(len(train_dataloader) * NUM_EPOCHS * 0.1)  # %10 warmup\n",
    "EVALUATION_STEPS = 500  # Her kaÃ§ adÄ±mda bir deÄŸerlendirme\n",
    "SAVE_STEPS = 1000  # Her kaÃ§ adÄ±mda bir checkpoint kaydet\n",
    "\n",
    "# Output path\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "OUTPUT_PATH = f'./turkish-bert-qa-finetuned_{timestamp}'\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸš€ FINE-TUNING BAÅLIYOR\")\n",
    "print(\"=\"*60)\n",
    "print(f\"ğŸ“Š Dataset:\")\n",
    "print(f\"   Training: {len(train_df):,} Ã¶rnekleri\")\n",
    "print(f\"   Validation: {len(val_df):,} Ã¶rnekleri\")\n",
    "print(f\"   Test: {len(test_df):,} Ã¶rnekleri\")\n",
    "print(f\"\\nâš™ï¸  Hyperparameters:\")\n",
    "print(f\"   Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Total steps: {len(train_dataloader) * NUM_EPOCHS:,}\")\n",
    "print(f\"   Warmup steps: {WARMUP_STEPS:,}\")\n",
    "print(f\"   Evaluation steps: {EVALUATION_STEPS}\")\n",
    "print(f\"\\nğŸ’¾ Output:\")\n",
    "print(f\"   Model path: {OUTPUT_PATH}\")\n",
    "print(f\"   Loss: {loss_name}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training baÅŸlat\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    evaluator=evaluator,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    output_path=OUTPUT_PATH,\n",
    "    evaluation_steps=EVALUATION_STEPS,\n",
    "    save_best_model=True,\n",
    "    show_progress_bar=True,\n",
    "    checkpoint_path=OUTPUT_PATH,\n",
    "    checkpoint_save_steps=SAVE_STEPS,\n",
    "    checkpoint_save_total_limit=3  # Son 3 checkpoint'i tut\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… FINE-TUNING TAMAMLANDI!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"ğŸ’¾ Model kaydedildi: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š 9. Model DeÄŸerlendirme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuned modeli yÃ¼kle\n",
    "print(\"ğŸ“¥ Fine-tuned model yÃ¼kleniyor...\")\n",
    "finetuned_model = SentenceTransformer(OUTPUT_PATH)\n",
    "print(\"âœ… Model yÃ¼klendi\")\n",
    "\n",
    "# Test seti Ã¼zerinde deÄŸerlendirme\n",
    "def evaluate_model(model, test_df, model_name=\"Model\"):\n",
    "    \"\"\"Test seti Ã¼zerinde model performansÄ±nÄ± deÄŸerlendirir\"\"\"\n",
    "    print(f\"\\nğŸ“Š {model_name} DeÄŸerlendirmesi:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    similarities = []\n",
    "    \n",
    "    for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Evaluating\"):\n",
    "        soru_emb = model.encode(row['soru'], convert_to_tensor=True)\n",
    "        cevap_emb = model.encode(row['cevap'], convert_to_tensor=True)\n",
    "        similarity = cos_sim(soru_emb, cevap_emb).item()\n",
    "        similarities.append(similarity)\n",
    "    \n",
    "    avg_sim = np.mean(similarities)\n",
    "    std_sim = np.std(similarities)\n",
    "    min_sim = np.min(similarities)\n",
    "    max_sim = np.max(similarities)\n",
    "    \n",
    "    print(f\"  Ortalama benzerlik: {avg_sim:.4f} Â± {std_sim:.4f}\")\n",
    "    print(f\"  Min benzerlik: {min_sim:.4f}\")\n",
    "    print(f\"  Max benzerlik: {max_sim:.4f}\")\n",
    "    print(f\"  Median benzerlik: {np.median(similarities):.4f}\")\n",
    "    \n",
    "    return avg_sim, similarities\n",
    "\n",
    "# Orijinal model\n",
    "print(\"\\nğŸ”„ Orijinal model test ediliyor...\")\n",
    "original_model = SentenceTransformer(MODEL_NAME)\n",
    "original_score, original_sims = evaluate_model(original_model, test_df, \"Orijinal BERT\")\n",
    "\n",
    "# Fine-tuned model\n",
    "print(\"\\nğŸ”„ Fine-tuned model test ediliyor...\")\n",
    "finetuned_score, finetuned_sims = evaluate_model(finetuned_model, test_df, \"Fine-tuned BERT\")\n",
    "\n",
    "# KarÅŸÄ±laÅŸtÄ±rma\n",
    "improvement = ((finetuned_score - original_score) / original_score * 100)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“ˆ PERFORMANS KARÅILAÅTIRMASI\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Orijinal Model:   {original_score:.4f}\")\n",
    "print(f\"Fine-tuned Model: {finetuned_score:.4f}\")\n",
    "print(f\"\\nğŸ¯ Ä°yileÅŸme: {improvement:+.2f}%\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” 10. Ã–rnek Sorgular ile Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_answer(query, knowledge_base_df, model, top_k=5):\n",
    "    \"\"\"\n",
    "    Verilen soru iÃ§in en iyi cevaplarÄ± bulur.\n",
    "    \"\"\"\n",
    "    # Query embedding\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "    \n",
    "    # TÃ¼m cevaplarÄ±n embeddingleri (batch processing)\n",
    "    answers = knowledge_base_df['cevap'].tolist()\n",
    "    answer_embeddings = model.encode(\n",
    "        answers, \n",
    "        convert_to_tensor=True,\n",
    "        batch_size=32,\n",
    "        show_progress_bar=False\n",
    "    )\n",
    "    \n",
    "    # Similarity hesapla\n",
    "    scores = cos_sim(query_embedding, answer_embeddings)[0]\n",
    "    \n",
    "    # Top-k sonuÃ§lar\n",
    "    top_results = torch.topk(scores, k=min(top_k, len(knowledge_base_df)))\n",
    "    \n",
    "    results = []\n",
    "    for score, idx in zip(top_results[0], top_results[1]):\n",
    "        results.append({\n",
    "            'skor': score.item(),\n",
    "            'soru': knowledge_base_df.iloc[idx]['soru'],\n",
    "            'cevap': knowledge_base_df.iloc[idx]['cevap']\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test sorularÄ±\n",
    "test_queries = [\n",
    "    \"Yapay zeka ne iÅŸe yarar?\",\n",
    "    \"Programlama dilleri hakkÄ±nda bilgi ver\",\n",
    "    \"Sinir aÄŸlarÄ± nasÄ±l Ã¶ÄŸrenir?\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ” Ã–RNEK SORGULAR\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nâ“ SORU: {query}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    results = search_answer(query, df, finetuned_model, top_k=3)\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\n{i}. Skor: {result['skor']:.4f}\")\n",
    "        print(f\"   Soru: {result['soru']}\")\n",
    "        print(f\"   Cevap: {result['cevap'][:150]}...\")  # Ä°lk 150 karakter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¾ 11. Model Kaydetme ve Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model zaten OUTPUT_PATH'e kaydedildi\n",
    "print(\"ğŸ’¾ Model Bilgileri:\")\n",
    "print(f\"  Model path: {OUTPUT_PATH}\")\n",
    "print(f\"  Model boyutu: {sum(p.numel() for p in finetuned_model.parameters()):,} parametreler\")\n",
    "\n",
    "# Model yÃ¼kleme kodu\n",
    "print(\"\\nğŸ“ Modeli yÃ¼klemek iÃ§in:\")\n",
    "print(f\"\\nfrom sentence_transformers import SentenceTransformer\")\n",
    "print(f\"model = SentenceTransformer('{OUTPUT_PATH}')\")\n",
    "\n",
    "# Hugging Face Hub'a yÃ¼klemek isterseniz:\n",
    "print(\"\\nâ˜ï¸  Hugging Face Hub'a yÃ¼klemek iÃ§in:\")\n",
    "print(\"\\nfrom huggingface_hub import login\")\n",
    "print(\"login()  # Token girin\")\n",
    "print(f\"finetuned_model.save_to_hub('kullanici-adi/model-adi')\")\n",
    "\n",
    "print(\"\\nâœ… TÃ¼m iÅŸlemler tamamlandÄ±!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ 12. Training Metrikleri GÃ¶rselleÅŸtirme (Opsiyonel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training log dosyasÄ±nÄ± okuma ve gÃ¶rselleÅŸtirme\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "# Log dosyasÄ±nÄ± bul\n",
    "log_files = glob.glob(f\"{OUTPUT_PATH}/**/eval/*.csv\", recursive=True)\n",
    "\n",
    "if log_files:\n",
    "    print(f\"ğŸ“Š Training metrikleri gÃ¶rselleÅŸtiriliyor...\")\n",
    "    \n",
    "    # Ä°lk log dosyasÄ±nÄ± oku\n",
    "    log_df = pd.read_csv(log_files[0])\n",
    "    \n",
    "    if 'spearman_cosine' in log_df.columns:\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # Subplot 1: Spearman Correlation\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(log_df['epoch'], log_df['spearman_cosine'], marker='o')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Spearman Correlation')\n",
    "        plt.title('Validation Performance')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Subplot 2: Steps vs Performance\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(log_df['steps'], log_df['spearman_cosine'], marker='o', color='green')\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('Spearman Correlation')\n",
    "        plt.title('Training Progress')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{OUTPUT_PATH}/training_metrics.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"âœ… Grafik kaydedildi: {OUTPUT_PATH}/training_metrics.png\")\n",
    "else:\n",
    "    print(\"âš ï¸  Log dosyasÄ± bulunamadÄ±\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ KullanÄ±m Ã–rnekleri ve Best Practices\n",
    "\n",
    "### ğŸš€ HÄ±zlÄ± KullanÄ±m:\n",
    "\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "\n",
    "# Modeli yÃ¼kle\n",
    "model = SentenceTransformer('./turkish-bert-qa-finetuned_YYYYMMDD_HHMMSS')\n",
    "\n",
    "# Soru ve cevap encode et\n",
    "soru = \"Python nasÄ±l Ã¶ÄŸrenilir?\"\n",
    "cevap = \"Python Ã¶ÄŸrenmek iÃ§in Ã¶nce temel syntax'Ä± Ã¶ÄŸrenmelisiniz...\"\n",
    "\n",
    "soru_emb = model.encode(soru)\n",
    "cevap_emb = model.encode(cevap)\n",
    "\n",
    "# Benzerlik hesapla\n",
    "similarity = cos_sim(soru_emb, cevap_emb)\n",
    "print(f\"Benzerlik: {similarity.item():.4f}\")\n",
    "```\n",
    "\n",
    "### ğŸ’¡ Ä°puÃ§larÄ±:\n",
    "\n",
    "1. **Veri Kalitesi**: Daha fazla ve kaliteli veri = Daha iyi performans\n",
    "2. **Negatif Ã–rnekler**: Hard negative mining performansÄ± artÄ±rÄ±r\n",
    "3. **Batch Size**: GPU belleÄŸinize gÃ¶re optimize edin\n",
    "4. **Epochs**: Ã‡ok fazla epoch overfitting'e yol aÃ§abilir\n",
    "5. **Evaluation**: DÃ¼zenli olarak validation performansÄ±nÄ± kontrol edin\n",
    "\n",
    "### ğŸ”§ Hyperparameter Tuning:\n",
    "\n",
    "```python\n",
    "# DeneyebileceÄŸiniz kombinasyonlar:\n",
    "BATCH_SIZES = [8, 16, 32]\n",
    "LEARNING_RATES = [2e-5, 3e-5, 5e-5]\n",
    "EPOCHS = [2, 3, 4]\n",
    "```\n",
    "\n",
    "### ğŸ“Š Model KarÅŸÄ±laÅŸtÄ±rma:\n",
    "\n",
    "FarklÄ± loss fonksiyonlarÄ± ve hiperparametreler deneyin:\n",
    "- CosineSimilarityLoss\n",
    "- MultipleNegativesRankingLoss\n",
    "- ContrastiveLoss\n",
    "- TripletLoss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
